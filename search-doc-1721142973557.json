{"searchDocs":[{"title":"Listener panel study update","type":0,"sectionRef":"#","url":"/blog/Listener panel study update","content":"We have two major updates from the Cadenza project. First, we completed the sensory panel study in which our participants with hearing loss developed audio quality scales for use in our listening experiments. We presented our study findings at the International Conference of Music Perception and Cognition (ICMPC) in Tokyo, Japan in August and at the Basic Auditory Science (BAS) conference in London in September. Second, the entrants in the First Cadenza Challenge have now submitted systems they devised to improve the audio quality of various music samples. These samples have been tailored for the hearing profiles of our listener panel participants and are currently being prepared for our online software. We will be releasing the main online listening experiment over the coming days! On 20th September, we ran a webinar for our listener panel. In the first part, we gave a short talk summarising the aims and results of the sensory panel study. You can watch the recording of this event in the next video. Our thanks to the challenge entrants for their submissions!","keywords":"","version":null},{"title":"Sensory evaluation study","type":0,"sectionRef":"#","url":"/blog/Sensory evaluation study","content":"Here at Cadenza our sensory evaluation work to define audio quality for hearing impaired listeners is underway. We want to understand better how hearing-impaired listeners perceive audio quality in music and develop quality metrics that will subsequently be used by our listener panel to rate the systems submitted by challenge entrants. Through careful listening tasks and group discussion, the sensory panel will arrive at a consensus about important sound quality attributes and how these should be measured. You can find out more about this process on the Sensory Evaluation page. The first task was an individual elicitation task in which we asked twelve listeners with hearing impairment to provide single-word perceptual terms to describe various music excerpts provided to them. This resulted in hundreds of unique attributes used to describe sound quality, of which 89 were used more than four times (see Fig. 1) and 87 were used two or three times (see Fig. 2). This provided the starting point for the first Focus group which sought to identify the most important attributes and to identify ways of grouping the attributes into perceptual dimensions that could be captured in the quality metric. The outcome of Focus group one was a spatial mapping of the ways in which the panel grouped attributes together which was used as a starting point for Focus group two (see Fig. 3). The panel then discussed further the meaning and grouping of different terms to reach consensus about important dimensions. In February, we will carry out a third Focus group to arrive at final dimensions, how these can be defined, and how they can be rated or scored in the challenges. We would like to thank our Sensory panel group for their ongoing motivation and commitment with this challenging task! Figure 1, 89 terms that were used four or more times to describe the musical excerpts. Figure 2, 87 terms that were used twice or three times to describe the musical excerpts. Figure 3, starting point for focus group two, discussing the further meaning and grouping of different terms.","keywords":"","version":null},{"title":"Sensory evaluation study update","type":0,"sectionRef":"#","url":"/blog/Sensory evaluation study update","content":"We are now reaching the end of our sensory evaluation study, where a panel of twelve listeners who use hearing aids have worked across online music listening tasks and three focus groups, to reach a consensus on the important perceptual attributes of music audio quality. Starting from 373 unique terms used by participants to describe music audio quality, a discussion process was completed as outlined in the image below: Figure 1, Objectives of each Focus Group session. At this stage in the work, we wanted to share the current state of these perceptual attributes and short definitions: Attribute\tShort DefinitionOverall Audio Quality\tPerceived audio quality results from judgments of the sound of the music, in relation to a person’s expectations of how the music should ideally sound to them. Clarity\tClarity refers to how well you can hear and distinguish between the different instruments and elements within the music. Harshness\tHarshness refers to an uncomfortable overemphasis of certain parts of the sound. It is most often heard in the treble resulting in a piercing, screechy or sharp sounds. Distortion\tDistortion can be caused by artefacts that shouldn’t be present e.g., noise, hiss, pops or crackles. It can also be caused by the pitches sounding wrong. Music with No distortion sounds like an authentic version of what was performed. Spaciousness\tSpaciousness refers to how much you feel the music is ‘coloured’ by the performance space, and how much you can hear the reverberations and sense of space. Treble Strength\tTreble strength refers to the perceived strength or prominence of sound qualities that are characterised by higher frequencies in the treble range, or similarly, sounds, instruments or voices with higher pitches. Middle Strength\tMiddle strength refers to the perceived strength or prominence of sound qualities that are characterised by middle frequencies found between bass and treble ranges, or similarly, sounds, instruments, or voices that pitches perceived as being between lower and higher pitches. Bass Strength\tBass strength refers to the perceived strength or prominence of sound qualities that are characterised by lower frequencies in the bass range, or similarly, sounds, instruments or voices with lower pitches. Frequency Balance\tFrequency balance refers to the perceived balance between treble (or higher pitch) and bass (or lower pitch) sounds. Our next steps in the perceptual research will involve further data collection and testing of these attributes, to understand which are the strongest predictors of overall audio quality. This is an important process as enhanced music signals submitted by challenge entrants will be scored on these attributes by a listening panel, and so the necessity of these attributes requires some initial testing. As always, we would like to express our sincere gratitude and thanks to our sensory panel group for their incredible commitment, motivation, and contribution to this research!","keywords":"","version":null},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Welcome to the new Cadenza webpage. We will be using this page to post the latest news about our forthcoming machine learning challenges and workshops, as well as posts discussing the tools and techniques that we are using in our baseline systems.","keywords":"","version":null},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/cadenza1/cc1_intro","content":"","keywords":"","version":"Next"},{"title":"Task 1: Headphones​","type":1,"pageTitle":"Overview","url":"/docs/cadenza1/cc1_intro#task-1-headphones","content":"   Someone with a hearing loss is listening via headphones, not using their hearing aids. As Figure 1 shows, the machine learning challenge here is to first demix stereo tracks into a VDBO (vocal, drums, bass and other) representation. This then allows a personalised remixing for the listener that has better audio quality. For example, for some music you might amplify the vocals to improve the audibility of the lyrics.  To evaluate the quality of the demixing, the objective measure HAAQI (Hearing aid audio quality index) is used. The evaluation of the remixed version will be via our listening panel.  The block that can be changed by you is labelled Enhancement in Figure 1.  While the main focus is on demixing/remixing, we'll accept entries using alternative signal processing approaches that can improve music for people with a hearing loss. Your entry would replace the whole box labelled enhancement.  Figure 1, The baseline for the headphone listening scenario (Task 1). For simplicity, not all signal paths are shown.    ","version":"Next","tagName":"h2"},{"title":"Task 2: Car​","type":1,"pageTitle":"Overview","url":"/docs/cadenza1/cc1_intro#task-2-car","content":"   The listener is wearing their hearing aids, sitting in a car and listening to recorded music played over the car stereo (see Figure 2). Your task is to process the music played from the stereo to improve the audio quality allowing for the presence of the car noise. You have access to the car speed, which gives an estimation of the power spectrum of the noise but not the noise signal itself, so this is not a noise cancellation task. The block that can be changed is labelled Enhancement.  Figure 2, The arrangement of the listener and speakers for the car listening scenario (Task 2).  Figure 3, The baseline for the car listening scenario (Task 2). For simplicity, not all signal paths are shown.    Watch the UKAN+ Webinar: Machine Learning Challenges to Improve Hearing Devices: Clarity &amp; Cadenza Projects  For more details on the Clarity and Cadenza projects, please see this recording of the webinar given by Trevor Cox, Alinka Greasley, and Rebecca Vos on the topic:   ","version":"Next","tagName":"h2"},{"title":"Summary Task 1","type":0,"sectionRef":"#","url":"/docs/cadenza1/cc1_summary_task1","content":"","keywords":"","version":"Next"},{"title":"1. Leaderboard​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#1-leaderboard","content":" If you have scores using the validation set, send us the score.csv file, and we will include you. The score used for the ranking is the average over all examples.  Ranking\tTeam\tAverage score1\tBaseline Demucs\t0.2592 2\tBaseline OpenUnMix\t0.2273 3\txumx_slicq_v2\t0.2046  ","version":"Next","tagName":"h2"},{"title":"2. Description of the Problem​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#2-description-of-the-problem","content":" A person with a hearing loss is listening to music via headphones. They're not using their hearing aids.  The machine learning task is to decompose a stereo song into a VDBO (vocal, drums, bass and other) representation. This then allows a personalised remixing for the listener that has better audio quality. For example, for some music you might amplify the vocals to improve the audibility of the lyrics.  As shown in Figure [1], the system is split into two stages; the enhancement and evaluation.  Figure 1, The baseline for the headphone listening scenario. For simplicity, not all signal paths are shown.  ","version":"Next","tagName":"h2"},{"title":"2.1 Enhancement Stage​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#21-enhancement-stage","content":" info You can adapt and modify the baseline enhancement script or make your own script.  Your task is to decompose a stereo music signal and produce 8 mono signals or stems corresponding to the right and left vocal, drums, bass and other (VDBO), and produce one stereo signal corresponding to a remix signal optimised for a target listener. For this, you will have access to relevant datasets that will allow you to explore different approaches to separate the music and/or to remix the signals.  2.1.1 Dataset​  In the enhancement stage, you have access to:  Full length songs from MUSDB18-HQ dataset.Music data for augmentation, if needed.Listeners characteristics (audiograms). Listener Data  Please refer to task 1 data page and the baseline readme for details.  To download the datasets, please visit download data and software.  2.1.2 Output​  The output of this stage are:  Eight stems corresponding to the left and right vocal, bass, drums and other stems. Sample rate = 24000 HzPrecision: 16bit integerCompressed using FLAC One stereo remixed signal Sample rate = 32000 HzPrecision: 16bit integerCompressed using FLAC  For more details about the format of the submission, please refer to the submission webpage.  Note The responsibility for the final remixed signal level is yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. Also, there may be clipping in the evaluation block in some tasks if the processed signals are too large.  ","version":"Next","tagName":"h3"},{"title":"2.2 Evaluation Stage​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#22-evaluation-stage","content":" Warning You are not allowed to change the evaluation script provided in the baseline. Your output signals with be scored using this script.  The evaluation stage is a common stage for all submissions. As shown in Figure [1], the evaluation takes the reference stem signals, i.e., the eight reference stems, and the eight processed stems and computes the eight HAAQI scores.  To learn more about HAAQI, please refer to our Learning Resourcesand to our Python HAAQI implementation.  The output of the evaluation stage is a CSV file with all the HAAQI scores.  ","version":"Next","tagName":"h3"},{"title":"3. Software​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#3-software","content":" All the necessary software to run the recipes and make your own submission is available on our Clarity-Cadenza GitHub repository.  The official code for the first challenge was released in version v0.3.4. To avoid any conflict, we highly recommend for you to work using version v0.3.4 and not with the code from the main branch. To install this version:  Download the files of the release v0.3.4 from:https://github.com/claritychallenge/clarity/releases/tag/v0.3.4 Clone the repository and checkout version v0.3.4  git clone https://github.com/claritychallenge/clarity.git git checkout tags/v0.3.4   Install pyclarity from PyPI as:  pip install pyclarity==0.3.4   ","version":"Next","tagName":"h2"},{"title":"4. Baselines​","type":1,"pageTitle":"Summary Task 1","url":"/docs/cadenza1/cc1_summary_task1#4-baselines","content":" In the Clarity/Cadenza GitHub repository, we provide two baselines. Both baseline systems work in a similar way. Using a music source separation model, the systems decompose the music into the target eight stems. Both models were trained exclusively on MUSDB18-HQ training set and no extra data was used for augmentation.  Demucs: This baseline system uses the Hybrid Demucs model. This is a time-domain-based model.Open-UnMix: This baseline system uses the umxhq model from Open-UnMix. This is a spectrogram-based model.  Please, visit the baseline on the GitHub webpageand Baseline links to read more about the baselines and learn how to run them. ","version":"Next","tagName":"h2"},{"title":"Evaluation","type":0,"sectionRef":"#","url":"/docs/cadenza1/cc1_evaluation","content":"","keywords":"","version":"Next"},{"title":"1. Task 1 - Headphone​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#1-task-1---headphone","content":" The evaluation package contains all audio tracks and metadata files necessary for run the evaluation.  cadenza_cad1_task1_evaluation.v1_0.tar.gz [1.9 GB] - audio and metadata evaluation data.  For processing the test data, run:  python3 test.py path.root=/path/to/cadenza_data/dir \\ path.exp=/path/to/experiment/ \\ team_id=&lt;Your_Team_ID&gt;   The script will produce a file we request you to submit called submission_&lt;Your_Team_ID&gt;.zip [23 GB].  ","version":"Next","tagName":"h2"},{"title":"1.1 Evaluation Music Dataset​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#11-evaluation-music-dataset","content":" The evaluation stage uses 49 from the 50 track in the evaluation set from the MUSDB18-HQ dataset. The track Punkdisco - Oral Hygiene was filtered out from the evaluation because it content can be offensive to members of the listener panel.  To keep the entrants' submission size in manageable ranges, the objective evaluation (HAAQI) will be performed only using 30-second segments per song-listener. Therefore, we will only require for entrants to submit the 30-second segment from the eight VDBO. These segments were selected randomly, ensuring that all stems were active at some point in the segment. This because HAAQI cannot be computed over an empty signal. In the case of an instrument was not active in the segment, we rerun the process until find a suitable segment.  In the subjective evaluation (listener panel), listeners will be presented with segments of 15-second of duration. Therefore, for the remixed signal, we just require for entrants to submit the pre-defined segments. These segments were selected randomly. However, as some of the song contain searing words, we ensure that the no swearing words were present in the segment by checking the lyrics. In the case of a swearing word was found in the segment, we rerun the process and select a new segment.  The start and en point of the segments for objective and subjective evaluation were selected separately. Therefore, there is no dependency between them. Also, the start and end point of the segments are fixed and are the same for all entrants.  ","version":"Next","tagName":"h3"},{"title":"1.2 Segments Metadata​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#12-segments-metadata","content":" The start and end point of the segments for objective and subjective evaluation are contained in themetadata/musdb18.segments.test.json file.  For example, below is showing the start and end point for the segments from the song AM Contra - Heart Peripheral. The objective_evaluation item is showing the start and end point for the 30-second segment for objective evaluation (HAAQI). And, subjective_evaluation item is showing the start and end point for the 15-second segment for subjective evaluation (listener panel).  { &quot;AM Contra - Heart Peripheral&quot;: { &quot;subjective_evaluation&quot;: { &quot;start&quot;: 51.77, &quot;end&quot;: 66.77, &quot;duration&quot;: 15.0 }, &quot;objective_evaluation&quot;: { &quot;start&quot;: 129.64, &quot;end&quot;: 159.64, &quot;duration&quot;: 30.0 } }, ... }   ","version":"Next","tagName":"h3"},{"title":"1.3 Test Script​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#13-test-script","content":" Together with the dataset, we released the PyCLarity version 0.3.4. This version includes small changes in the recipes that allow to process the signal and generates the entrant's submission package.  we added four new parameters in the config.yaml:  music_test_file: path to the file with the list of test tracksmusic_segments_test_file: path to the file with the details of the start and end point for each segmentpath.listeners_test_file: path to the test listeners audiogramsteam_id: this is the ID of your team.  path: music_test_file: ${path.metadata_dir}/musdb18.test.json music_segments_test_file: ${path.metadata_dir}/musdb18.segments.test.json listeners_test_file: ${path.metadata_dir}/listeners.test.json team_id: ???   We included the file test.py that process the signal using the test songs and test audiograms. And, generates the segments from the processed signals. After the processing, this script archives the files in a ZIP file with the name submission_&lt;TEAM_ID&gt;.zip. This is the file you need to submit.  ","version":"Next","tagName":"h3"},{"title":"2. Task 2 - Car​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#2-task-2---car","content":" The evaluation package contains all audio tracks and metadata files necessary for run the evaluation.  cadenza_cad1_task2_evaluation.v1_1.tar.gz [717 MB] - audio and metadata evaluation data.  For processing the test data, run:  python3 test.py path.root=/path/to/cadenza_data/dir \\ path.exp=/path/to/experiment/ path.scenes_file=/path/to/scenes.test.json \\ path.scenes_listeners_file=/path/to/scenes_listeners.test.json \\ evaluate.split=test \\ team_id=&lt;Your_Team_ID&gt;   The script will produce a file we request you to submit called submission_&lt;Your_Team_ID&gt;.zip [7.9 GB].  ","version":"Next","tagName":"h2"},{"title":"2.1 Evaluation Music Dataset​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#21-evaluation-music-dataset","content":" Like the training and validation sets, the evaluation set is based on the small split of the FMA dataset and the MTG Jamendo dataset. The tracks were selected following the same procedure as the training and validation sets (see here). However, to keep the entrants' submission in manageable sizes, we reduced the test set to 10 tracks per genre.  ","version":"Next","tagName":"h3"},{"title":"2.2 Test Script​","type":1,"pageTitle":"Evaluation","url":"/docs/cadenza1/cc1_evaluation#22-test-script","content":" Together with the dataset, we released the PyCLarity version 0.3.4. This version includes small changes in the recipes that allow to process the signal and generates the entrant's submission package.  First, we added two new parameters in the config.yaml:  path.listeners_test_file: path to the test listeners audiograms.team_id: this is the ID of your team.  path: listeners_test_file: ${path.metadata_dir}/listeners.test.json team_id: ???   The function load_listeners_and_scenes in the file baseline_utils.py was modified to take into account the test set files.  ... elif config.evaluate.split == &quot;test&quot;: with open(config.path.listeners_test_file, encoding=&quot;utf-8&quot;) as fp: listener_audiograms = json.load(fp) scenes = df_scenes[df_scenes[&quot;split&quot;] == &quot;test&quot;].to_dict(&quot;index&quot;) ...   We included the file test.py that process the signal using the test songs and test audiograms. After the processing, this script archives the files in a ZIP file with the name submission_&lt;TEAM_ID&gt;.zip. This is the file you need to submit. ","version":"Next","tagName":"h3"},{"title":"Listener metadata","type":0,"sectionRef":"#","url":"/docs/cadenza1/Data/cc1_data_listener","content":"","keywords":"","version":"Next"},{"title":"Data file formats and naming conventions​","type":1,"pageTitle":"Listener metadata","url":"/docs/cadenza1/Data/cc1_data_listener#data-file-formats-and-naming-conventions","content":" Audiograms data is stored in a JSON file per dataset with the following format.  { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ ... ], }, &quot;L0002&quot;: { ... }, ... }  ","version":"Next","tagName":"h2"},{"title":"Summary Task 2","type":0,"sectionRef":"#","url":"/docs/cadenza1/cc1_summary_task2","content":"","keywords":"","version":"Next"},{"title":"1. Leaderboard​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#1-leaderboard","content":" If you have scores using the validation set, send us the score.csv file, and we will include you. The score used for the ranking is the total average.  Ranking\tTeam\tAverage score1\tBaseline\t0.1146  ","version":"Next","tagName":"h2"},{"title":"2. Description of the Problem​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#2-description-of-the-problem","content":" A person with hearing loss is wearing their hearing aids and sitting in a car. They're listening to recorded music played over the car stereo (see Figure [1]).  Figure 1, The arrangement of the listener and speakers for the car listening scenario.  Your task is to process the music played from the stereo to improve the audio quality allowing for the presence of the car noise.  As shown in Figure [2], the system is split into two stages; the enhancement and evaluation.  Figure 2, The baseline for the car listening scenario. For simplicity, not all signal paths are shown.  ","version":"Next","tagName":"h2"},{"title":"2.1 Enhancement Stage​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#21-enhancement-stage","content":" info You can adapt and modify the baseline enhancement script or make your own script.  Your task is to process the music in such a way that improves the reproduced quality of the music. For this, you have access to the car speed and other metadata, which gives an estimation of the power spectrum of the noise. You don't have the noise signal itself, so this is not a noise cancellation task.  2.1.1 Dataset​  In the enhancement stage, you have access to:  A music dataset containing 5600 30-second excepts of samples from 8 music genres.Metadata of: Listener Characteristics (audiograms) - see listener DataCar speed and metadata. This is very important as it provides information to approximately estimate the noise spectrum.SNR at the hearing aids. This tells you how loud the car noise relative to the music.  caution The SNR at the hearing aids microphone information is an SNR relative to the music. This means that simply increasing the music level will result in an increment of the noise level.  Please refer to task 2 data page and the baseline readme for details.  To download the datasets, please visit download data and software.  2.1.2 Output​  The output of this stage is one stereo signal:  Sample rate = 32 kHzPrecision: 16-bit integerCompressed using FLAC  For more details about the format of the submission, please refer to submission webpage.  Note The responsibility for the final signal level is yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to someone in the listening panel, they may well turn down the volume. Also, there may be clipping in the evaluation block if the processed signals are too large.  ","version":"Next","tagName":"h3"},{"title":"2.2 Evaluation Stage​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#22-evaluation-stage","content":" Bear in mind You are not allowed to change the evaluation script provided in the baseline. Your output signals with be scored using this script.  The evaluation stage is a common for all submissions. As shown in Figure [2], the evaluation takes the reference music signal. Note that, in this figure, theMusic and the Clean Music are the same signal but are show in separate lines for illustration purposes.  In this stage, both the enhanced and the reference signal are processed before the HAAQI evaluation. See Core Software.  2.2.1 Process on the enhanced signal.​  Generate car noise based on the parameters from the metadata.Apply anechoic HRTFs to the noise.Apply car HRTFs to the enhanced signal.Scale the noise to match the SNR ar hearing aidsAdd both signal  2.2.2 Process on the reference signal.​  Add anechoic room impulses.  To learn more about HAAQI, please refer to our Learning Resourcesand to our Python HAAQI implementation.  The output of the evaluation stage is a CSV file with all the HAAQI scores.  ","version":"Next","tagName":"h3"},{"title":"3. Software​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#3-software","content":" All the necessary software to run the recipes and make your own submission is available on our Clarity-Cadenza GitHub repository.  The official code for the first challenge was released on version v0.3.4. To avoid any conflict, we highly recommend for you to work using version v0.3.4 and not with the code from the main branch. To install this versions you can:  Download the files of the release v0.3.4 from:https://github.com/claritychallenge/clarity/releases/tag/v0.3.4 Clone the repository and checkout version v0.3.4  git clone https://github.com/claritychallenge/clarity.git git checkout tags/v0.3.4   Install pyclarity from PyPI as:  pip install pyclarity==0.3.4   ","version":"Next","tagName":"h2"},{"title":"4. Baseline​","type":1,"pageTitle":"Summary Task 2","url":"/docs/cadenza1/cc1_summary_task2#4-baseline","content":" In the Clarity/Cadenza GitHub repository, we provide a baseline system. Please, visit the baseline on the GitHub webpageand Baseline link to read more about the baseline and learn how to run it. ","version":"Next","tagName":"h2"},{"title":"Task 2 Car","type":0,"sectionRef":"#","url":"/docs/cadenza1/Data/cc1_data_overview_car","content":"","keywords":"","version":"Next"},{"title":"1 Training/development​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#1-trainingdevelopment","content":" ","version":"Next","tagName":"h2"},{"title":"1.1 Music Data​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#11-music-data","content":" The music dataset is based on the small split of the FMA dataset and MTG Jamendo. FMA-Small is a balanced dataset for genre classification. From the eight genres available in FMA-small, we selected five genres:  Hip-HopInstrumentalInternationalPopRock  However, those with hearing loss are more likely to be older people who listen to classical and orchestral music [1]. Therefore, we included samples from these two genres sourced from the MTG-Jamendo dataset:  ClassicalOrchestral  Each genre contains 900 30-seconds samples divided into 800 for training and 100 for development.  ","version":"Next","tagName":"h3"},{"title":"1.2 HRTF Data​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#12-hrtf-data","content":" To allow for the 'room' acoustic in the car, we use Anechoic and Car binaural room impulse responses from the eBrIRD ELOSPHERES dataset.    ","version":"Next","tagName":"h3"},{"title":"2 Evaluation​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#2-evaluation","content":" 700 30-second samplesYou should process all the music.All the music will be used for HAAQI evaluation.We will then select a random 10-second sample from some of this music for listening panel evaluation.    ","version":"Next","tagName":"h2"},{"title":"3. Data file formats and naming conventions​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#3-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"3.1 Enhanced signals​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#31-enhanced-signals","content":" The baseline generates one output per scene:  &lt;Dataset Split&gt;/&lt;Listener ID&gt;/&lt;Scene_ID&gt;_&lt;Listener ID&gt;_&lt;Song ID&gt;.flac  Where:  Dataset Split - Split you are evaluation, train, valid or test.Listener ID - ID of the listener panel member, e.g., L001 to L100 for initial pseudo-listeners, etc.Song ID - ID of the song.  For example:  valid ├───L5000_fma_041020.wav ├───L5000_fma_058333.wav ├───L5007_mtg_00539764.wav ├─── ...   ","version":"Next","tagName":"h3"},{"title":"3.2 Evaluation Signal​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#32-evaluation-signal","content":" In the evaluation stage, several signal can be generated to explore the different intermediate signals. For example, you might want to explore the output of the hearing aid to ensure that samples are not clipped. These additonal signals can be generated by setting the parameter evaluate.save_intermediate_wavs to True in config.yaml.  When evaluate.save_intermediate_wavs is False, the evaluation generates:  ha_processed_signal.wav - Output of the HA to use in HAAQI evaluation.ref_signal_for_eval - Reference signal to use in HAAQI evaluation.  When evaluate.save_intermediate_wavs is True, the evaluation also generates:  car_noise_anechoic.wav - Car noise with anechoic HRTFs at the front HA microphones.car_noise_anechoic_scaled.wav - Car noise with anechoic HRTFs at the front HA microphones scaled to SNR.enh_signal_hrtf.wav - Enhanced musical signal with car HRTFs at the front HA microphone.enh_signal_hrtf_plus_car_noise_anechoic.wav - Enhanced music with car noise added at certain SNR. Signal to pass through the HA.ref_signal_anechoic.wav - Reference signal with anechoic HRTFs at the eardrums.  ","version":"Next","tagName":"h3"},{"title":"3.3 Music Metadata​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#33-music-metadata","content":" Music data is store in a single JSON per file dataset with the following format.   { &quot;fma_000002&quot;: { &quot;track_id&quot;: &quot;000002&quot;, &quot;path&quot;: &quot;training/Hip-Hop/000002.mp3&quot;, &quot;artist&quot;: &quot;AWOL&quot;, &quot;album&quot;: &quot;AWOL - A Way Of Life&quot;, &quot;title&quot;: &quot;Food&quot;, &quot;license&quot;: &quot;Attribution-NonCommercial-ShareAlike 3.0 International&quot;, &quot;genre&quot;: &quot;Hip-Hop&quot;, &quot;bit_rate&quot;: 256000, &quot;duration&quot;: 168, &quot;channels&quot;: 2, &quot;source&quot;: &quot;fma&quot; }, ...   ","version":"Next","tagName":"h3"},{"title":"3.4 HRTFs Metadata​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#34-hrtfs-metadata","content":" HRTFs data is stored in a single JSON with the following format.  { &quot;train&quot;: { &quot;-57.5&quot;: { &quot;car&quot;: { &quot;left_speaker&quot;: { &quot;left_side&quot;: &quot;HR13_E03_CH1_Left&quot;, &quot;right_side&quot;: &quot;HR13_E03_CH1_Right&quot; }, &quot;right_speaker&quot;: { &quot;left_side&quot;: &quot;HR13_E04_CH1_Left&quot;, &quot;right_side&quot;: &quot;HR13_E04_CH1_Right&quot; } }, &quot;anechoic&quot;: { &quot;left_speaker&quot;: { &quot;left_side&quot;: &quot;HR5_E02_CH0_Left&quot;, &quot;right_side&quot;: &quot;HR5_E02_CH0_Right&quot; }, &quot;right_speaker&quot;: { &quot;left_side&quot;: &quot;HR21_E02_CH0_Left&quot;, &quot;right_side&quot;: &quot;HR21_E02_CH0_Right&quot; } } }, ... ... }   ","version":"Next","tagName":"h3"},{"title":"3.5 Scenes Metadata​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#35-scenes-metadata","content":" Scene data is stored in a single JSON with the following format.  { &quot;S100000&quot;: { &quot;scene&quot;: &quot;S100000&quot;, &quot;song&quot;: &quot;fma_081613&quot;, &quot;song_path&quot;: &quot;training/Instrumental/081613.mp3&quot;, &quot;hr&quot;: 25.0, &quot;car_noise_parameters&quot;: { &quot;speed&quot;: 114.0, &quot;gear&quot;: 6, &quot;reference_level_db&quot;: 30.9, &quot;engine_num_harmonics&quot;: 12, &quot;rpm&quot;: 1915.2, &quot;primary_filter&quot;: {&quot;order&quot;: 1, &quot;btype&quot;: &quot;lowpass&quot;, &quot;cutoff_hz&quot;: 20.3632}, &quot;secondary_filter&quot;: {&quot;order&quot;: 2, &quot;btype&quot;: &quot;lowpass&quot;, &quot;cutoff_hz&quot;: 314.2048}, &quot;bump&quot;: {&quot;order&quot;: 2, &quot;btype&quot;: &quot;bandpass&quot;, &quot;cutoff_hz&quot;: [77, 110]}, &quot;dip_low&quot;: {&quot;order&quot;: 1, &quot;btype&quot;: &quot;lowpass&quot;, &quot;cutoff_hz&quot;: 170}, &quot;dip_high&quot;: {&quot;order&quot;: 1, &quot;btype&quot;: &quot;highpass&quot;, &quot;cutoff_hz&quot;: 455} }, &quot;snr&quot;: 7.8386, &quot;split&quot;: &quot;train&quot; }, ...     ","version":"Next","tagName":"h3"},{"title":"4. Reference​","type":1,"pageTitle":"Task 2 Car","url":"/docs/cadenza1/Data/cc1_data_overview_car#4-reference","content":" [1] Bonneville-Roussy, A., Rentfrow, P. J., Xu, M. K., &amp; Potter, J. (2013). Music through the ages: Trends in musical engagement and preferences from adolescence through middle adulthood. Journal of personality and social psychology, 105(4), 703. ","version":"Next","tagName":"h2"},{"title":"Task 1 Headphones","type":0,"sectionRef":"#","url":"/docs/cadenza1/Data/cc1_data_overview_headphone","content":"","keywords":"","version":"Next"},{"title":"1 Training/development​","type":1,"pageTitle":"Task 1 Headphones","url":"/docs/cadenza1/Data/cc1_data_overview_headphone#1-trainingdevelopment","content":" The main training/development database is the MUSDB18-HQ. MUSDB18-HQ has 86 training songs and 14 validation songs.  You can supplement the training and validation data from the following sources:  Bach10FMA-smallMedleydB version 1 and version 2  We leave it to you to decide how to use these as part of the training and validation sets. Note, some songs from MedleydB are already part of the training set in MUSDB18-HQ. For more information on augmenting and supplementing the training data, please see the rules.    ","version":"Next","tagName":"h2"},{"title":"2 Evaluation​","type":1,"pageTitle":"Task 1 Headphones","url":"/docs/cadenza1/Data/cc1_data_overview_headphone#2-evaluation","content":" We will use the MUSDB18-HQ's evaluation set which is made up of 50 songs.You must process all of these for the complete songs.All the music will be used for HAAQI evaluation.We will then select a random 10-second sample from some of the pieces of music for listening panel evaluation.    ","version":"Next","tagName":"h2"},{"title":"3. Data file formats and naming conventions​","type":1,"pageTitle":"Task 1 Headphones","url":"/docs/cadenza1/Data/cc1_data_overview_headphone#3-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"3.1 Enhanced signals​","type":1,"pageTitle":"Task 1 Headphones","url":"/docs/cadenza1/Data/cc1_data_overview_headphone#31-enhanced-signals","content":" There are nine output signals generated by the baseline enhancement algorithm:  Eight enhanced output signal corresponding to the left and right channels of each stem (i.e., as submitted by the challenge entrants)  &lt;Listener ID&gt;/&lt;Song Name&gt;/&lt;Listener ID&gt;_&lt;Song Name&gt;_&lt;Channel&gt;_&lt;Stem&gt;.wav  One enhanced output signal corresponding to the final remix  &lt;Listener ID&gt;/&lt;Song Name&gt;/&lt;Listener ID&gt;_&lt;Song Name&gt;_remix.wav  Where:  Listener ID – ID of the listener panel member, e.g., L001 to L100 for initial pseudo-listeners, etc.Song Name - Track name from MUSDB18, e.g, One Minute Smile.Channel - left or right channelStem - Vocal, Bass, Drums or Others  For example, for development listener ID L5011 and development song name One Minute Smile_left, the enhanced output is:  L5011 └───One Minute Smile ├───L5011_Actions - One Minute Smile_left_bass.wav ├───L5011_Actions - One Minute Smile_right_bass.wav ├───L5011_Actions - One Minute Smile_left_drums.wav ├───L5011_Actions - One Minute Smile_right_drums.wav ├───L5011_Actions - One Minute Smile_left_other.wav ├───L5011_Actions - One Minute Smile_right_other.wav ├───L5011_Actions - One Minute Smile_left_vocals.wav ├───L5011_Actions - One Minute Smile_right_vocals.wav └───L5011_Actions - One Minute Smile_remix.wav   ","version":"Next","tagName":"h3"},{"title":"3.2 Music metadata​","type":1,"pageTitle":"Task 1 Headphones","url":"/docs/cadenza1/Data/cc1_data_overview_headphone#32-music-metadata","content":" Music data is store in a single JSON per file dataset with the following format.  [ { &quot;Track Name&quot;:&quot;A Classic Education - NightOwl&quot;, &quot;Genre&quot;:&quot;Singer/Songwriter&quot;, &quot;Source&quot;:&quot;MedleyDB&quot;, &quot;License&quot;:&quot;CC BY-NC-SA&quot;, &quot;Split&quot;:&quot;train&quot; }, ... ]  ","version":"Next","tagName":"h3"},{"title":"Listening tests","type":0,"sectionRef":"#","url":"/docs/cadenza1/Data/cc1_listening_tests","content":"","keywords":"","version":"Next"},{"title":"1. Overview​","type":1,"pageTitle":"Listening tests","url":"/docs/cadenza1/Data/cc1_listening_tests#1-overview","content":" The listeners will be provided with Sennheiser PC-8 USB headsets to complete the listening experiment. The experiment will be run via a browser-based audio player. We will instruct participants to use a quiet room. The software will play a piece of music, then the participant will respond with their grading of audio quality on a set of scales.    ","version":"Next","tagName":"h2"},{"title":"2. Levels​","type":1,"pageTitle":"Listening tests","url":"/docs/cadenza1/Data/cc1_listening_tests#2-levels","content":" For safety reasons, we allow participants to set the overall audio volume so that the sounds are not so loud as to be uncomfortable. This means that we cannot be sure at what dB SPL the signals will be presented (and as we don’t have any individual measures of loudness recruitment, we don’t know how loud they will seem to the participants either). It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  Our measurements of PC-8 headsets:  A 1-kHz pure tone set to unity gain (RMS 0.707) with soundcard volume at 100% produces a level of 99 dB SPL using an artificial ear with 2 cc coupler (IEC 60318–1).A steady-amplitude “ICRA speech-shaped noise” ([1] track 1), scaled to 0.3 RMS, with soundcard volume at 100% produces an A-weighted level of 90 dB SPL at the same volume level. With this RMS, the noise had 0.1% of its samples clipped.  Accordingly, in the submitted signals, 0 dB FS should correspond to 99 dB SPL (at 1 kHz).    ","version":"Next","tagName":"h2"},{"title":"3. Music samples​","type":1,"pageTitle":"Listening tests","url":"/docs/cadenza1/Data/cc1_listening_tests#3-music-samples","content":" A subset of music tracks will be used from the evaluation dataset.A 15-second sample will be extracted from each music track at a random time point and a short fade-in and fade-out applied.The code for extracting samples will be provided, please see Additional Tools    ","version":"Next","tagName":"h2"},{"title":"4. Listening test design​","type":1,"pageTitle":"Listening tests","url":"/docs/cadenza1/Data/cc1_listening_tests#4-listening-test-design","content":" The listening test will probably have a combinatorial design so that each listener hears entries from each team. Subjects will evaluate the samples on 4-5 perceptual sliders:   One of these scales will be for perceived overall basic audio quality. This will be used to rank the entrants. Perceived audio quality is about well the music meets a person’s expectations of how the music should ideally sound.The other scales will be used to rate aspects of the audio quality. We have been working with a sensory panel to develop these, and the current short-list for the scales as of March 2023 is: Clarity, Harshness, Distortion, Frequency Balance and Spaciousness. This data will be used to produce an improved objective measure for audio quality for future challenges.  There will be approximately 50 members on the panel, and each listener will provide scores for every system, using a subset of the total music samples. Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel.  The number of samples listened to will depend on the total number of systems, but the maximum listening time will be approximately 5 hours per panel member.  Listeners will hear samples that have been processed for their audiogram only, or if we do not have audiograms for all listeners, then they will hear samples processed for an audiogram of the same type.    ","version":"Next","tagName":"h2"},{"title":"5. References​","type":1,"pageTitle":"Listening tests","url":"/docs/cadenza1/Data/cc1_listening_tests#5-references","content":"   ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. ","version":"Next","tagName":"h2"},{"title":"Additional tools","type":0,"sectionRef":"#","url":"/docs/cadenza1/Software/cc1_additional_tools","content":"","keywords":"","version":"Next"},{"title":"1. Car noise generator​","type":1,"pageTitle":"Additional tools","url":"/docs/cadenza1/Software/cc1_additional_tools#1-car-noise-generator","content":" This software generates car noise, using the car speed as input. This is provided so that entrants can generate the car noise for the evaluation stage. You are allow to generate different noises to learn the spectrum characteristics of the noises given the speed. However, entrants cannot use the car noise signal itself to perform noise cancellation, only the metadata (e.g. car speed).  The model is split into two parts.  carnoise_parameters_generator.py - This class generates random parameters given a car speed and return them as a dictionary.carnoise_signal_generator.py - This class takes a dictionary as input and generates the expected noise.  For details of how to use these classes, refer to the docstrings in the classes. ","version":"Next","tagName":"h2"},{"title":"Baseline","type":0,"sectionRef":"#","url":"/docs/cadenza1/Software/cc1_baseline","content":"","keywords":"","version":"Next"},{"title":"1. Task 1: Headphones​","type":1,"pageTitle":"Baseline","url":"/docs/cadenza1/Software/cc1_baseline#1-task-1-headphones","content":" Figure 1, The baseline for the car listening scenario (Task 2), not all connections shown.  The music databases (blue box) are available in object based format, allowing us to create both stereo input to the demixer (grey line) and music in VDBO (vocal, drums, bass, other) format (red lines). These later signals form the reference VDBO signals that are needed for the objective evaluation using HAAQI (Hearing Aid Audio Quality Index) [1]. The demixing part is therefore a variant on a standard demixing challenge, except the quality of the separation is evaluated using HAAQI rather than a measure like SDR (Signal to Distortion Ratio).  The audiogram metadata allows the music enhancement (e.g. demixing/remixing) to be individualised to the hearing ability of the listener (dash grey lines).  The VDBO signals are remixed to give the stereo output from the headphones. It would be possible to use a simple remixer that uses the levels stated in the original music's metadata. But there is freedom here to experiment with changing the remixing to improve the audio quality for the listeners.  ","version":"Next","tagName":"h2"},{"title":"1.1 Baseline system​","type":1,"pageTitle":"Baseline","url":"/docs/cadenza1/Software/cc1_baseline#11-baseline-system","content":" We are presenting two baseline systems using two out-of-the-box systems trained exclusively on the MUSDB18 training data.  The time-domain system Hybrid Demucs music source separation model. This model is publicly available in theTorchAudio library.The spectrogram-based system Open-Unmix model. This model is publicly available through torch.hub.  The output of the demixing is 8 stems corresponding to the left and right channel for each source, i.e., left vocal, right vocal, left bass, right bass, and so on.  Then, a NAL-R amplification and compression is applied to each stem using the personalized audiogram of each listener. NAL-R is a prescription formulation for fitting simple hearing aids. This then creates the output stems.  These output stems are then evaluated by computing the mean HAAQI score of each stem.  For the remixing procedure, the baseline simply does a linear addition of the output stems.  Your challenge is to improve the demixing and remixing blocks in the &quot;enhancement&quot; box. The rest of the baseline is fixed and should not be changed.    ","version":"Next","tagName":"h3"},{"title":"2. Task 2: Car​","type":1,"pageTitle":"Baseline","url":"/docs/cadenza1/Software/cc1_baseline#2-task-2-car","content":" Figure 1, The baseline for the headphone listening scenario (Task 1).  The music database (blue box) provide samples as input to the car stereo and also reference left and right stereo signals for evaluating using HAAQI. Your task is to process the music taking into account the listener audiogram and also the car noise. You have access to the car metadata (e.g. speed), which will determine the power spectrum of the car noise.  The evaluation starts by predicting the signals at the microphones of the hearing aids. The effect of the 'room' acoustics is simulated by applying Binaural Room Impulse Responses (taken from the eBrIRD database). The car noise comes from a simulator we provide.  After the car noise and acoustic simulation, the signals are then processed by a simple hearing aid. This then provides left and right signals that can be used for evaluation either by HAAQI or the listening panel.  Your challenge is to improve the car stereo ('Enhancement'). The 'Evaluation' is fixed and should not be changed.  ","version":"Next","tagName":"h2"},{"title":"2.1 Baseline system​","type":1,"pageTitle":"Baseline","url":"/docs/cadenza1/Software/cc1_baseline#21-baseline-system","content":" In enhancement, the baseline system simply normalises the signal accordingly to the average audiogram of the ear with greater hearing loss. The system attenuates the signal and saves it at 16 bit resolution.  In the evaluation stage, the car noise and car acoustics characteristics (HRTFs) are added to the enhanced signal. This signal is then passed to a simple hearing aid (HA) composed of a NAL-R amplification.  The output of the HA is use for HAAQI and listening panel evaluation.  caution An important issue to be aware of is the possibility of the hearing aid producing signals with an amplitude that causes clipping. When this happens, the system prints warning messages in the log file, including the track path and the number of clipped samples.  More details can be found in the core software page.  2.1.1 Loudness consideration in enhancement​  The baseline enhancement system simply sets an appropriate level for the original music based on the music and audiogram of the listener. It isn't perfect and some samples still clip!  Computes the level in dB LUFS (loudness units relative to full scale) of the original song, i.e., the song from the dataset.Computes the average hearing loss (HL) for each ear. Refer to measuring hearing impairmentresource to understand how this average is computed.Set a temporal target level according maximum average HL. If average is equal or greater than 50, set the temporal target level to -19 dB LUFS. Then, for every 5dB LUFS over 50 dB, reduce the level in an additional 1 dB LUFS.Otherwise, set the temporal target level at -14 dB LUFS. The final target level for normalisation is set as the minimum between the temporal target and the original song levels.  Example: All levels are in dB LUFS  Song\tMax average HL\tOriginal Level\tTemp Target Level\tFinal Level1\t35\t-13\t-14\t-14 2\t42\t-15\t-14\t-15 3\t50\t-15\t-19\t-19 4\t60\t-13\t-19\t-21  The levels of -14 and -19 dB LUFS were set according to Spotify standard.  2.1.1 Loudness consideration in evaluation​  For the signal passing through the hearing aid. The enhanced signal (&quot;processed music in above diagram&quot;) has the car acoustic characteristics and car noise added (See here). This then goes into the hearing aid, which is a simple linear amplifier. If any sample values exceed +1 (or are below -1) on the output of the hearing aid, they are then set to +1 (or -1). Consequently, setting the levels of the enhancement signals to prevent clipping at this point is vital.  For the reference signal (&quot;clean music&quot;) passing to HAAQI, a simple stereo set up in a dead room is simulated:  Anechoic HRTFs are applied to the reference signal to simulate the stereo set-up. This then gives the music at the listener ear canal.The reference signal is then normalise to the lower loudness between the enhanced signal after adding the car acoustics and -14 dB LUFS. This becomes the reference signal for HAAQI evaluation.    ","version":"Next","tagName":"h3"},{"title":"3. References​","type":1,"pageTitle":"Baseline","url":"/docs/cadenza1/Software/cc1_baseline#3-references","content":"   [1] Kates, J.M. and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858. ","version":"Next","tagName":"h2"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/cadenza1/Software/cc1_core_software","content":"","keywords":"","version":"Next"},{"title":"1. Task 1: Headphones​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#1-task-1-headphones","content":" The key elements of the task1 baseline system are the:  Baseline demixerHAAQI music quality model.  ","version":"Next","tagName":"h2"},{"title":"1.1 Baseline Demixer​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#11-baseline-demixer","content":" The demixing stage simply takes the out-of-the-box Hybrid Demucs music source separation model to obtain stereo VDBO stems. This model is publicly available in theTorchAudio library. Then, it applies a NAL-R [1] and compression procedure to each stem to personalize the output.  Inputs: Stereo songs and Listeners characteristics. Output: 8 stems (left and right VDBO stems) for each song-listener. And 1 remixed signal for listener panel.  ","version":"Next","tagName":"h3"},{"title":"2. Task 2: Car stereo​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#2-task-2-car-stereo","content":" The key elements of the task2 baseline system are:  Car acoustics modelHAAQI music quality model  ","version":"Next","tagName":"h2"},{"title":"2.1 Car acoustics model​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#21-car-acoustics-model","content":" Figure 1, The Car Acoustics Model.  The scene metadata contains several parameters that is used by the car scene metadata.  Using the car parameters, a car noise signal is generated. For details of the Car Noise generator, seeadditional tools. This signal is added with anechoic HRTFs at the front microphone of the Hearing Aid (HA).Using the head rotation angle, the model adds car HRTFs at the front microphone of the HA to the enhanced music.These two signal are then added together at a given SNR at the front microphone of the HA.The resulting signal is pass through a fixed HA composed of a simple NAL-R amplification.The HA output is the signal use for evaluation.  ","version":"Next","tagName":"h3"},{"title":"3. HAAQI Audio Qiality model​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#3-haaqi-audio-qiality-model","content":" This is a python implementation of the Hearing Aid Audio Quality Index (HAAQI) model which is used for objective estimation. This will be used in the stage 1 evaluation of entrants (see Rules).  Note that HAAQI is not a binaural metric, instead, each channel must be processed separately. We average the left and right scores to produce a final overall score.  The reference signals for HAAQI are:  Task 1 Headphone. The original left and right channels of the music tracks.Task 2 Car. The references are the left and right signals at the ear canal of a listener auditioning the music over a stereo set-up of two loudspeakers, in an anechoic room.  ","version":"Next","tagName":"h2"},{"title":"4. References​","type":1,"pageTitle":"Core Software","url":"/docs/cadenza1/Software/cc1_core_software#4-references","content":"   [1] Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265. ","version":"Next","tagName":"h2"},{"title":"Key Dates","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_key_dates","content":"Key Dates Draft key dates: March 2023 task launch30th June 2023: Release of evaluation data.14th - 28th July 2023: Competition submission window. All entrants submit: (i) audio for evaluation, and (ii) a draft of their technical report.End of Aug 2023: Entrants informed which systems are going forward to the listening test evaluation stage.Sept-Nov 2023: Listening tests8th December (tentative, to be confirmed): Cadenza-2023 online workshop.","keywords":"","version":"Next"},{"title":"Download data and software","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_download","content":"","keywords":"","version":"Next"},{"title":"1. Software​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza1/Take part/cc1_download#1-software","content":" All the necessary software tools will be available from Clarity-Cadenza GitHub repository  We recommend installing the software first and then following the instructions in the repository's README for downloading and unpacking the data.    ","version":"Next","tagName":"h2"},{"title":"2. Data​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza1/Take part/cc1_download#2-data","content":" To unpack the data for either of the tasks, we recommend you to follow the instructions in the Clarity-Cadenza Challenge GitHub repository.  All participants will require the core data packages. Participants wishing to extend the training set can use either of the augmentation data packages.  ","version":"Next","tagName":"h2"},{"title":"2.1 Task 1 - Headphones​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza1/Take part/cc1_download#21-task-1---headphones","content":" The data is available for download here.  On the download site you will see five data packages are available,  Core Data In case you already have a copy of the MUSDB18-HQ, you won't need to download it from our mirror. However, you still need to request access to the dataset to download the metadata package. This package contains the listeners' audiograms, key information for the HAAQI evaluation.  cadenza_cad1_task1_core_musdb18hq.v1.0.tar.gz [21.1 GB] - core audio data for training, validation and evaluation.cadenza_cad1_task1_core_metadata.v1.0.tar.gz [7 KB] - core metadata for training and validation.cadenza_cad1_task1_augmentation_medleydb.v1.0.tar.gz [38.1 GB] - optional augmentation audio data for training.cadenza_cad1_task1_augmentation_bach10.v1.0.tar.gz [125 MB] - optional augmentation audio data for training.cadenza_cad1_task1_augmentation_fma_small.v1.0.tar.gz [7.1 GB] - optional augmentation audio data for training.  ","version":"Next","tagName":"h3"},{"title":"2.1 Task 2 - Car​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza1/Take part/cc1_download#21-task-2---car","content":" The data is available for download here.  On the download site you will see one data packages available,  cadenza_cad1_task2_core.v1_1.tar.gz [6.1 GB] - core audio and metadata for training and validation. ","version":"Next","tagName":"h3"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_FAQs","content":"","keywords":"","version":"Next"},{"title":"Learning resources​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza1/Take part/cc1_FAQs#learning-resources","content":" We have extensive resources that provide background information on many aspects: hearing loss, hearing aid processing, mixing/remixing, and perceptual testing.  To see current discussions of the challenge, please join the Google group.  If you have more questions that aren't covered, please contact us.  ","version":"Next","tagName":"h3"},{"title":"What data can I use?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza1/Take part/cc1_FAQs#what-data-can-i-use","content":" You should be able to find the information on the Rules or the Data Overview page.  ","version":"Next","tagName":"h3"},{"title":"I want to enter the first task without using the demixing/remixing approach​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza1/Take part/cc1_FAQs#i-want-to-enter-the-first-task-without-using-the-demixingremixing-approach","content":" This is fine (see here)! We are accepting alternative approaches. Please make sure that your approach is clear in your technical report.  ","version":"Next","tagName":"h3"},{"title":"How do I actually enter?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza1/Take part/cc1_FAQs#how-do-i-actually-enter","content":" You should be able to find the information you need about submitting your entry on the Submission page. Please make sure you have signed up to our Google group, as this will ensure you get updates about the challenge.  ","version":"Next","tagName":"h3"},{"title":"I don't want to enter the signal processing challenge, but would like to be involved as a listener​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza1/Take part/cc1_FAQs#i-dont-want-to-enter-the-signal-processing-challenge-but-would-like-to-be-involved-as-a-listener","content":" Please contact us, and we may be able to get you involved with our listening panel and/or sensory evaluation work. ","version":"Next","tagName":"h3"},{"title":"Find a Team","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_find_a_team","content":"Find a Team If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Cadenza Challenge’s Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Cadenza Team","keywords":"","version":"Next"},{"title":"Registration","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_registration","content":"Registration Sign up to our Google group for alerts about the challenges and to help shape the challenges. To enter the challenge, teams are required to register using the form below. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading…","keywords":"","version":"Next"},{"title":"Webinar - 23rd of May 2023","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_webinar2023","content":"","keywords":"","version":"Next"},{"title":"Cadenza Challenge 1 Webinar and Q&A​","type":1,"pageTitle":"Webinar - 23rd of May 2023","url":"/docs/cadenza1/Take part/cc1_webinar2023#cadenza-challenge-1-webinar-and-qa","content":" In May 2023, we held a webinar event to give a short introduction to the project and the tasks of this first round. We also did a tutorial explaining the baseline codes, the relevant functions, how to run each stage and, the output directory structures and formats.  The tutorial is saved in a Jupyter Books format and can be accessed onhttps://www.gerardoroadabike.com/cadenza_webinar_may2023/intro.html  Expositors: Trevor Cox, Alinka Greasley and Gerardo Roa Dabike.  You can watch the recording of this event in:   ","version":"Next","tagName":"h3"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_rules","content":"","keywords":"","version":"Next"},{"title":"1. Teams​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#1-teams","content":" Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win any prizes.  ","version":"Next","tagName":"h2"},{"title":"2. Transparency​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#2-transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged to make their code open source.Anonymous entries are allowed but will not be eligible for any prizes.Teams may reserve the right to be referred to using anonymous code names in the published rank ordering.  ","version":"Next","tagName":"h2"},{"title":"3. What information can I use?​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#3-what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"3.1. Training and development​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#31-training-and-development","content":" There is no limit on the amount of training data that can be generated using our tools and training data sets. Teams can also expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. All the audio or metadata can be used during training and development.  You should not use the evaluation data set for training or tuning the system.  ","version":"Next","tagName":"h3"},{"title":"3.2. Evaluation​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#32-evaluation","content":" The only data that can be used to enhance the evaluation data are:  Task 1: Headphones​  The listener characterisation (e.g. audiogram).The stereo music input signals to the headphones.  Task 2: Car​  The listener characterisation (e.g. audiogram).The audio input signals to the car stereo.The metadata for the car noise generator (e.g. the speed).The SNR of the music and the car noise at the hearing aid's microphones.The angle of the listener's head rotation.  ","version":"Next","tagName":"h3"},{"title":"4. Computational restrictions​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#4-computational-restrictions","content":" There is no need for processing to be causal, as both scenarios are for recorded music.There is no limit on computational cost.Teams must start with the baseline, with the blocks that can be changed labelled Enhancement in Figure 1 and Figure 2 in the first challenge Overview. These are: Task 1: the headphone demixer/remixer.Task 2 the car stereo processer. While HAAQI is being used to complement the listening panel evaluation, other metrics and approaches can be used by the teams.  ","version":"Next","tagName":"h2"},{"title":"5. Submitting multiple entries​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#5-submitting-multiple-entries","content":" You can submit two entries, where one is optimised for HAAQI and the other for listening tests if you wish. In this case:  You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear, which has been optimised for listening tests and the relationship between the two entries.Both systems will be evaluated via HAAQI.  ","version":"Next","tagName":"h2"},{"title":"6. Evaluation of systems​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#6-evaluation-of-systems","content":" ","version":"Next","tagName":"h2"},{"title":"6.1. Stage 1: Objective evaluation​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#61-stage-1-objective-evaluation","content":" Entries will be ranked according to average HAAQI score across all signals in the evaluation dataset. We will use the HAAQI implementation in the baseline system. For Task 1 headphones, the HAAQI evaluation will be on the demixed signals, for Task 2 car, the HAAQI evaluation is on the output of the hearing aid.  For anyone entering Task 1 by a different approach to demixing/remixing, their entries will not be objectively evaluated, because no suitable metric currently exists.  ","version":"Next","tagName":"h3"},{"title":"6.2. Stage 2: Listening test evaluation​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#62-stage-2-listening-test-evaluation","content":" There is a limit on how many systems can be evaluated by the listener panel.The expectation is that only one entry can go through to the listener panel from any entrant.We will choose which ones will go to the listener panel based on: The scores by the objective metric HAAQI.The approach detailed in the technical report.Informal listening by the Cadenza team.  ","version":"Next","tagName":"h3"},{"title":"7. Intellectual property​","type":1,"pageTitle":"Rules","url":"/docs/cadenza1/Take part/cc1_rules#7-intellectual-property","content":" The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to these.  The &quot;submission&quot; constitutes the audio files submitted to the challenge and the accompanying technical report.  The Challenge is organised by the Challenge Organiser.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. ","version":"Next","tagName":"h2"},{"title":"The 2nd Cadenza Challenge","type":0,"sectionRef":"#","url":"/docs/cadenza2/intro","content":"The 2nd Cadenza Challenge The Cadenza Challenges are improving music for people with a hearing loss. Hearing loss causes various problems such as quieter music passages being inaudible, poor and anomalous pitch perception, difficulties identifying and picking out instruments, and problems hearing out lyrics. The 2nd Cadenza Challenge (CAD2) is part of the IEEE SPS Challenge Program. Open from July 2024 it has two tasks: Improving the intelligibility of lyrics for pop/rock music while not harming audio quality. The scenario is listening over headphones. More on Task1: Lyrics Intelligibility. Rebalancing the level of instruments within a small classical music ensemble (e.g. string quartet) to allow personal remixing for hearing aids users. More on Task2: Rebalancing Classical Music. For both tasks, a demix / remix approach could be used. Gains could be applied to the demixed signals before remixing back to stereo to achieve the aims of the challenge. For lyric intelligibility, a simple amplification of the vocals could increase intelligibility, but there are other ways to achieve this that might cause less harm to audio quality. It would also be possible to use other machine learning approaches such as end-to-end transformation for either task. We provide music signals, software tools, objective metrics and baselines. The two tasks are evaluated using objective metrics. For lyric intelligibility, there will also be perceptual tests with listeners who have hearing loss. Since listeners may be experiencing live or recorded music, we welcome both causal and non-causal approaches. To stay up to date please sign up for the Cadenza Challenge's Google group. 📝 CAD2 now has cash prizes.","keywords":"","version":"Next"},{"title":"Submission","type":0,"sectionRef":"#","url":"/docs/cadenza1/Take part/cc1_submission","content":"","keywords":"","version":"Next"},{"title":"1. What audio do I need to submit?​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#1-what-audio-do-i-need-to-submit","content":" Signals should be submitted as 16-bit FLAC files with a 24 kHz or 32 kHz sampling rate depending on the signal (details below), and 0 dB FS corresponds to 100 dB SPL, given the capabilities of the headsets used by the listening panel. See the page on listening tests for more information about reproduction levels from the headset. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It’s worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves. Also, there may be clipping in the evaluation block in some tasks if the processed signals are too large.  ","version":"Next","tagName":"h2"},{"title":"1.1. Task 1: Headphones​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#11-task-1-headphones","content":" You must submit the following audio for all the signals in the evaluation set:  The VDBO (vocal, drums, bass, other) demixed signals for both left and right. Predefined 30-second segment.16-bit24 kHz sampling rateCompressed using the lossless FLAC compressor The remixed stereo signal. Predefined 15-second segment.16-bit32 kHz sampling rateCompressed using the lossless FLAC compressor  For those replacing the whole enhancement block with another approach, just the left and right output signals from your enhancement processer are required.  ","version":"Next","tagName":"h3"},{"title":"1.2. Task 2: Car​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#12-task-2-car","content":" You must submit the following audio for all the signals in the evaluation set:  The output processed music enhanced by the car stereo. 16-bit32 kHz sampling rateCompressed using the lossless FLAC compressor  ","version":"Next","tagName":"h3"},{"title":"2. Code​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#2-code","content":" We encourage you to make your code open source.  ","version":"Next","tagName":"h2"},{"title":"3. Technical report​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#3-technical-report","content":" Draft: A draft of the report needs to be uploaded along with your processed signals.The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules. Technical Report: A two page technical report must be submitted as a paper to the Cadenza-2023 Workshop (date to be confirmed).Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used.You can use the Interspeech 2023 template.  ","version":"Next","tagName":"h2"},{"title":"4. Where do I submit the signals?​","type":1,"pageTitle":"Submission","url":"/docs/cadenza1/Take part/cc1_submission#4-where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a OneDrive to which you will be able to securely upload your signals.  Materials uploaded will be visible to the Cadenza Team but not to other entrants.  Your processed signals should be named using the conventions used by the baseline system:  Task 1: &lt;Listener ID&gt;/&lt;Song Name&gt;/&lt;Listener ID&gt;_&lt;Song Name&gt;_&lt;Channel&gt;_&lt;Stem&gt;.flac and&lt;Listener ID&gt;/&lt;Song Name&gt;/&lt;Listener ID&gt;_&lt;Song Name&gt;_remix.wav as explained inTask1 data pageTask 2: &lt;Dataset Split&gt;/&lt;Listener ID&gt;/&lt;Scene_ID&gt;_&lt;Listener ID&gt;_&lt;Song ID&gt;.flac as explained in Task2 data page. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., submission_E001 and then packaged using zip or tar or any standard packaging tool.  The resulting files should be about 21 GB for Task 1 and 7.3 GB for Task 2. ","version":"Next","tagName":"h2"},{"title":"Listener metadata","type":0,"sectionRef":"#","url":"/docs/cadenza2/data_listener","content":"","keywords":"","version":"Next"},{"title":"Data file formats and naming conventions​","type":1,"pageTitle":"Listener metadata","url":"/docs/cadenza2/data_listener#data-file-formats-and-naming-conventions","content":" Audiogram data is stored in a JSON file per dataset with the following format.  { &quot;L0001&quot;: { &quot;name&quot; : &quot;L0001&quot;, &quot;audiogram_cfs&quot; : [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot; : [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot; : [ ... ], }, ... }   Compressor params are provided for the final signal amplification. This file contains individualised compression ratios and gains per band.   &quot;L0001&quot;: { &quot;frequencies&quot;: [250, 500, 1000, 2000, 4000, 8000], &quot;cr_l&quot;: [2.1, 1.5, 1.3, 1.5, 2.4, 3.7], &quot;gain_l&quot;: [9.6, 11.9, 12.9, 16.4, 19.0, 28.0], &quot;cr_r&quot;: [...] &quot;gain_r&quot;: [...] },  ","version":"Next","tagName":"h2"},{"title":"Amplification","type":0,"sectionRef":"#","url":"/docs/cadenza2/amplification","content":"","keywords":"","version":"Next"},{"title":"Hearing aid amplification​","type":1,"pageTitle":"Amplification","url":"/docs/cadenza2/amplification#hearing-aid-amplification","content":" The baselines for both tasks use a common amplification process. This compensates for frequency-dependent hearing loss our listeners have using a set of dynamic range compressors. The same amplification is also applied to the reference used in the objective metric HAAQI.  The amplification is a simulation of hearing-aid non-linear amplification. It is applied separately to the left and right signals. Each extract is split into octave-spaced bands with centre frequencies of 0.25-8 kHz using cross-over filters. Frequency-dependent gains are applied to the filtered bands. Gains are based on an A-weighted 65 dB presentation level using [an implementation of] the CAMFIT nonlinear prescription [1]. Compression ratios were determined by the difference in prescribed outputs at 55 and 85 dB input (cf. IEC 60118, 2022) with an upper ratio limit of 6:1 imposed to avoid extreme values. Compression attack and release times (11-14 and 80-100 ms, respectively, across centre frequencies) were based on average values measured using IEC 60118 procedures in hearing aid devices [2]. The filtered, amplified and compressed stimuli were then summed.  The amplification in the baselines is part of the enhancement block, so you are free to replace it with a different approach. This will decrease HAAQI scores, however, because the reference signal for HAAQI uses this default amplification. A better amplification stage might score higher in listening tests, however. Note, we are allowing teams to submit a system optimised for listening tests and another optimised for objective evaluation.  ","version":"Next","tagName":"h2"},{"title":"Metadata​","type":1,"pageTitle":"Amplification","url":"/docs/cadenza2/amplification#metadata","content":" For each listener (e.g. L001) the compressor parameter json gives the compression ratios (cr) and gains (gain) for each frequency range and left and right ears. Other parameters in the compressor are fixed. see Listener Metadata for details.  ","version":"Next","tagName":"h3"},{"title":"Levels and clipping​","type":1,"pageTitle":"Amplification","url":"/docs/cadenza2/amplification#levels-and-clipping","content":" The original mixes from the scene generator are set to a level of -40 LUFS. This is the level designed to work well with our non-linear amplification. Consequently, when inputting signals to the compressor part of the music enhancer make sure they're roughly at this level. If you use very different levels you will find the HAAQI score will be much lower.  It is also good practice to monitor whether clipping is happening due to the amplification. A small amount of clipping might be acceptable to allow sufficient amplification for those with hearing loss, too much and there will be audible distortion.  Note, if you are making a causal system, be careful your normalisation process does not use information from the future i.e. you can't calculate the rms for the whole extract of music because that involves samples in the future.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Amplification","url":"/docs/cadenza2/amplification#references","content":" [1] Moore, B.C.J., Glasberg, B.R. and Stone, M.A., 1999. Use of a loudness model for hearing aid fitting: III. A general method for deriving initial fittings for hearing aids with multi-channel compression. British Journal of Audiology, 33(4), pp.241-258. [2] Whitmer, W.M. and Akeroyd, M.A., 2011. Level discrimination of speech sounds by hearing-impaired individuals with and without hearing amplification. Ear and hearing, 32(3), pp.391-398. ","version":"Next","tagName":"h2"},{"title":"Task 1: Lyric Intelligibility","type":0,"sectionRef":"#","url":"/docs/cadenza2/Lyric Intelligibility/lyrics","content":"","keywords":"","version":"Next"},{"title":"A. Introduction​","type":1,"pageTitle":"Task 1: Lyric Intelligibility","url":"/docs/cadenza2/Lyric Intelligibility/lyrics#a-introduction","content":" Studies show that not being able to hear the lyrics in music is an important problem to tackle for those with hearing loss [1]. Consequently, this task is about improving the intelligibility of lyrics when listening to pop/rock over headphones. But this needs to be done without losing too much audio quality - you can't improve intelligibility just by turning off the rest of the band! For this reason, we will be evaluating both intelligibility and audio quality, and giving you different targets to explore the balance between these attributes.  This task could be tackled in many different ways using machine learning. A few examples:  Within speech technology, there are many different approaches to improving speech intelligibility that have been developed. Can these methods be adapted to improve listening to vocals?Within demixing, technologies allow the separation of music into different components including a vocal track. This then allow processing of the vocals and remixing to improve intelligibility.End-to-end approaches allow the transformation of audio from one style to another. How can this be adapted for this task?  But we'd welcome other approaches as well.  ","version":"Next","tagName":"h2"},{"title":"A.1 What is lyric intelligibility?​","type":1,"pageTitle":"Task 1: Lyric Intelligibility","url":"/docs/cadenza2/Lyric Intelligibility/lyrics#a1-what-is-lyric-intelligibility","content":" Lyric intelligibility, as defined by Cadenza's sensory panel of hearing aid users, refers to &quot;how clearly and effortlessly the words in the music can be heard&quot;. Across this sensory panel and work by Fine et al. [2], there are four groups of factors that can affect lyric intelligibility:  Performer: includes articulation, voice quality and diction.Music-to-singer balance: includes balance in dynamics or pitch, music genre, song speed and composition style.Listener [1]: includes listener attention, familiarity, expectation and hearing ability.Environmental: includes room acoustics, proximity to performer and use or abuse of amplification.  As listeners are using headphones in our scenario, environmental factors are not included.  From the first three factors, (1) and (2) are addressed in the task by including samples with different singing styles and background accompaniment. For example, in the challenge datasets, one can find samples of music tracks where the background is not prominent and the sung words are more easily heard. This is illustrated by the following example extracted from the training set from MUSDB18-HQ dataset:    Did you pick up the lyrics? Track Name: Actions - South Of The Water Lyrics: my skin's falling off i'm breaking at the seeps he's holding me under and i can't breath Transcriptions made by [Schulze-Forster et al.]  The datasets also include tracks where the singing can be more difficult to hear, either because the background level is higher than the singing level and/or the singing style makes the lyrics difficult to hear. The next example, also drawn from MUSDB18-HQ dataset, illustrates how the background accompaniment can mask the singing line, affecting the lyric intelligibility.    Did you pick up the lyrics? Track Name: Dark Ride - Burning Bridges Lyrics: burning bridges fire in my soul burning bridges forget about control burn those witches i am the only one burn the bridges i relied upon Transcriptions made by [Schulze-Forster et al.]  Listener issues (factor 3) will be covered by us providing listeners' hearing characteristics as Audiograms.  Challenge entrants will be provided with appropriate music datasets and sets of audiograms for training, development and evaluation.  ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Task 1: Lyric Intelligibility","url":"/docs/cadenza2/Lyric Intelligibility/lyrics#references","content":"   [1] Fine, P. A., &amp; Ginsborg, J. (2014). &quot;Making myself understood: perceived factors affecting the intelligibility of sung text,&quot; Frontiers in psychology, 5, 809. [2] A. Greasley, H. Crook, and R. Fulford, &quot;Music listening and hearing aids: perspectives from audiologists and their patients,&quot; International Journal of Audiology, vol. 59, no. 9, pp. 694–706, 2020. ","version":"Next","tagName":"h2"},{"title":"Lyric Intelligibility Data","type":0,"sectionRef":"#","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, validation and evaluation data​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#a-training-validation-and-evaluation-data","content":" The training and validation data are provided at challenge launch. The evaluation data is provided closer to the submission deadline.  ","version":"Next","tagName":"h2"},{"title":"A.1 Training and validation data​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#a1-training-and-validation-data","content":" The dataset uses the transcription extension [1] of the training split of MUSDB18-HQ [2]. This extension comprises 96 manual transcriptions of English songs by non-native English speakers, totalling 366 minutes of audio.  We permit the use of the following additional datasets in training: FMA, MedleydB version 1 and version 2, and MousesDB. We also permit the use of pre-trained models that might have been developed using these databases.  You should not use pre-trained models that were trained on our evaluation data.  ","version":"Next","tagName":"h3"},{"title":"A.2 Evaluation (test) set​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#a2-evaluation-test-set","content":" The evaluation dataset combines the English subset of the JamendoLyrics dataset (20 songs) [3] with the 46 transcribed songs from the evaluation split of the MUSDB18-HQ dataset. We will tell you what part of the songs are required, the required value of α\\alphaα and the audiograms of the listeners.  danger The evaluation set should not be used for refining the system.  ","version":"Next","tagName":"h3"},{"title":"B. Metadata Information​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b-metadata-information","content":" ","version":"Next","tagName":"h2"},{"title":"B.1 Listener characteristics​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b1-listener-characteristics","content":" We provide metadata characterising the hearing abilities of listeners so the audio signals can be personalised. This is common for both tasks, so please see Listener Metadata for more details.  { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [45, 45, 35, 45, 60, 65, 70, 65], &quot;audiogram_levels_r&quot;: [40, 40, 45, 45, 60, 65, 80, 80] }, &quot;L0002&quot;: { &quot;name&quot;: &quot;L0002&quot;, ... }   ","version":"Next","tagName":"h3"},{"title":"B.2 Alpha​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b2-alpha","content":" This gives the balanced between intelligibility and quality. It will range from 0 to 1 in 0.1 steps.  { &quot;alpha_0&quot;: 0.0, &quot;alpha_1&quot;: 0.1, ... }   ","version":"Next","tagName":"h3"},{"title":"B.3 Music​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b3-music","content":" This provides the information of the audio segments with its transcriptions.  { &quot;A_Classic_Education_-_NightOwl_1&quot;: { &quot;track_name&quot;: &quot;A Classic Education - NightOwl&quot;, &quot;path&quot;: &quot;musdb18_hq/train/audios/A Classic Education - NightOwl&quot;, &quot;segment_id&quot;: 1, &quot;start_time&quot;: 0, &quot;end_time&quot;: 8.2, &quot;confidence&quot;: &quot;a&quot;, &quot;text&quot;: &quot;i think you're right i do&quot; }, &quot;A_Classic_Education_-_NightOwl_2&quot;: { ...   ","version":"Next","tagName":"h3"},{"title":"B.4. Scenes​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b4-scenes","content":" This files provide the combination of segment ids and alpha to use for that segment. This is a randomly generated combinations.  { &quot;S10001&quot;: { &quot;segment_id&quot;: &quot;A_Classic_Education_-_NightOwl_1&quot;, &quot;alpha&quot;: &quot;alpha_10&quot; }, &quot;S10002&quot;: { &quot;segment_id&quot;: &quot;A_Classic_Education_-_NightOwl_2&quot;, &quot;alpha&quot;: &quot;alpha_5&quot; }, &quot;S10003&quot;: { ...   ","version":"Next","tagName":"h3"},{"title":"B.5 Scene-Listeners​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#b5-scene-listeners","content":" This provides the list of listeners for each scene.  { &quot;S10001&quot;: [&quot;L0067&quot;, &quot;L0044&quot;], &quot;S10002&quot;: [&quot;L0073&quot;, &quot;L0054&quot;], ...   ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Lyric Intelligibility Data","url":"/docs/cadenza2/Lyric Intelligibility/lyric_data#references","content":"   [1] Schulze-Forster, K., Doire, C.S., Richard, G. and Badeau, R., 2021. Phoneme level lyrics alignment and text-informed singing voice separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, pp.2382-2395. [2] Rafii, Z., Liutkus, A., Stöter, F.-R., Mimilakis, S. I., and Bittner, R. (2019). MUSDB18-HQ - an Uncompressed Version of MUSDB18. [Dataset]. doi:10.5281/zenodo.3338373 [3] Durand, S., Stoller, D. and Ewert, S., 2023, June. Contrastive learning-based audio to lyrics alignment for multiple languages. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE. ","version":"Next","tagName":"h2"},{"title":"Lyric Intelligibility Task Overview","type":0,"sectionRef":"#","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview","content":"","keywords":"","version":"Next"},{"title":"A. Task overview​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#a-task-overview","content":" Entrants will process pop/rock music to increase the intelligibility with least loss of audio quality. Audio will be evaluated for their intelligibility and audio quality.  Figure 1. A simplified schematic of the baseline system.  A scene generator (blue box): Selects the stereo music signal.Gives a value of α\\alphaα (metadata) that sets the balance between intelligibility and audio quality (see evaluation below). The music enhancement stage (pink box) takes the music as inputs and attempts to improve the intelligibility.Listener characteristics (green oval) are audiograms and compressor settings to allow personalised processing in the enhancement stage and are also used in objective evaluation.The enhancement outputs are evaluated using objective metrics (orange boxes): For intelligibility using a metric based on Whisper.For audio quality via the Hearing-Aid Audio Quality Index (HAAQI) [1].  Systems will also be evaluated by our listening panel.  Your challenge is to improve what happens in the pink, music enhancement box. The rest of the baseline is fixed and should not be changed.  ","version":"Next","tagName":"h2"},{"title":"B. Causality​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#b-causality","content":" We will accept causal and non-causal systems. Non-causal systems could be used for recorded music, whereas causal systems would also work for live listening. A baseline will be provided for each case. The allowed latency for causal systems will be 5 milliseconds, that is, systems cannot look beyond 5 ms into the future.    ","version":"Next","tagName":"h2"},{"title":"C. Evaluation​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#c-evaluation","content":" There will be two rank lists, one based on listening tests and the other on objective metrics. It is fine to submit two systems, one optimised for the listening panel and one for the objective metrics.  ","version":"Next","tagName":"h2"},{"title":"C.1 Listening tests​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#c1-listening-tests","content":" The listener panel will score the music for both audio quality and intelligibility. Listeners will be asked to rate music extracts using the follow definitions:  Lyric Intelligibility: “Lyric intelligibility refers to how clearly and effortlessly the words in the music can be heard.”Basic Audio Quality: “Results from judgments of the sound of the music, in relation to a person’s expectations of how the music should ideally sound to them.”  To rank the teams, the intelligibility LI and quality Q ratings from the listening tests will be combined in a weighted average to get an overall score:  score=α z(𝑄)+(1−α) z(𝐿𝐼)\\begin{align} score = \\alpha ~ z(𝑄) + (1 − \\alpha) ~ z (𝐿_𝐼) \\tag{1} \\end{align}score=α z(Q)+(1−α) z(LI​)​(1)​  Where the weighting α\\alphaα will allow the balance between intelligibility and quality to be varied, and z()z()z() indicates a zzz-normalisation to make the two metrics compatible for the weighted average.  We are likely to also ask listeners to rate the samples for Clarity, Distortion and Preference to better understand the results from the perceptual evaluation. But these scales will not be used for rank ordering the systems.  ","version":"Next","tagName":"h3"},{"title":"C.2 Objective metrics​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#c2-objective-metrics","content":" Our quality and intelligibility metrics will be combined using Equation (1) as for the listening tests. Audio quality is evaluated using HAAQI [1]. Intelligibility is scored using correct transcribed words ratio (CTW) using a lyric transcription algorithm based on Whisper. HAAQI is an intrusive metrics and the reference will be the original signal with a 1 dB amplification applied to the vocal signal and -1dB to the accompaniment, because that has been shown to be preferred by wearers of hearing aids [3].  Objective metrics are always an approximation and you may want to use other approaches and metrics to optimise a system for the listening panel (for example, you could use singing-adapted STOI [2]).  ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Lyric Intelligibility Task Overview","url":"/docs/cadenza2/Lyric Intelligibility/lyric_overview#references","content":"   [1] Kates, J. M., &amp; Arehart, K. H. (2015). The hearing-aid audio quality index (HAAQI). IEEE/ACM transactions on audio, speech, and language processing, 24(2), 354-365. [2] Sharma, B., &amp; Wang, Y. (2019). Automatic evaluation of song intelligibility using singing adapted STOI and vocal-specific features. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28, 319-331. [3] Benjamin, A.J. and Siedenburg, K., 2023. Exploring level-and spectrum-based music mixing transforms for hearing-impaired listeners. The Journal of the Acoustical Society of America, 154(2), pp.1048-1061. ","version":"Next","tagName":"h2"},{"title":"Rebalancing Classical Baseline","type":0,"sectionRef":"#","url":"/docs/cadenza2/Rebalancing Classical/rebalance_baseline","content":"","keywords":"","version":"Next"},{"title":"A. Task Overview​","type":1,"pageTitle":"Rebalancing Classical Baseline","url":"/docs/cadenza2/Rebalancing Classical/rebalance_baseline#a-task-overview","content":" Challenge entrants are supplied with a fully functioning baseline system, which is illustrated in Figure 1  Figure 1. A simplified schematic of the baseline system.  Click here for overview of Figure 1 A scene generator (blue box): Selects the stereo music signal.Gives the target gains (metadata) for the different instruments in the ensemble. The music enhancement stage (pink box) takes the music as inputs and attempts to make a new mix with the target gains.Listener characteristics (green oval) are audiograms and compressor settings to allow personalised processing in the enhancement stage and are also used in objective evaluation.The enhancement outputs are evaluated (orange box) for audio quality using the Hearing-Aid Audio Quality Index (HAAQI) Your challenge is to improve what happens in the pink music enhancement box. The rest of the baseline is fixed and should not be changed.  ","version":"Next","tagName":"h2"},{"title":"B. Music Enhancer​","type":1,"pageTitle":"Rebalancing Classical Baseline","url":"/docs/cadenza2/Rebalancing Classical/rebalance_baseline#b-music-enhancer","content":" Figure 2 shows the music enhancer supplied in the baseline. Your task is to improve this.  Figure 2. A simplified schematic of the baseline system.  The baseline approach is to demix the stereo music into the number of instruments in the ensemble. For this, the baseline uses eight audio source separation model, each trained for a specific target instrument. During the separation, a percentage of the residual is added back to the estimated target to reduce any distortion resulting from the separation. Next, the specified gains are applied to each instrument before recombining the signals. Then, an amplification is applied to ensure the downmix stereo is roughly the same level as the original. The final stage is to apply a frequency-dependent amplification to correct for the hearing loss - see amplification for more details.  The output is FLAC format 16-bit, 32 kHz.  ","version":"Next","tagName":"h2"},{"title":"C. Objective Evaluation​","type":1,"pageTitle":"Rebalancing Classical Baseline","url":"/docs/cadenza2/Rebalancing Classical/rebalance_baseline#c-objective-evaluation","content":" The enhanced audio is evaluating using the HAAQI implementation in pyClarity. This is an intrusive metric and requires a reference. The reference signal is constructed by applying the specified gains to the isolated instrument stems and summing the result. A gain is applied to ensure the reference mix is not amplified. The reference is also amplified using a simulation of a simple hearing-aid - see amplification for more details.  ","version":"Next","tagName":"h2"},{"title":"D. Baseline Results​","type":1,"pageTitle":"Rebalancing Classical Baseline","url":"/docs/cadenza2/Rebalancing Classical/rebalance_baseline#d-baseline-results","content":" Baseline\tHAAQICausal\t- NonCausal\t0.4594 ","version":"Next","tagName":"h2"},{"title":"Task 2: Rebalancing Classical Music","type":0,"sectionRef":"#","url":"/docs/cadenza2/Rebalancing Classical/rebalancing","content":"Task 2: Rebalancing Classical Music Image by Port(u*o)s oder Phil Ortenau from Wikimedia Commons In a pilot study, we found listeners with hearing loss liked the ability to rebalance the different instruments in an ensemble. Consequently, your task is to develop a system that can take a stereo recording of a small classical ensemble and change the levels of the instruments in the mix by applying specified gains. Ensembles will have between two and five instruments. Signals will be scored objectively by HAAQI (Hearing Aid Audio Quality Index) [1]. This task could be tackled by demixing the stereo into the separate instruments, applying the requested gains and downmixing back to stereo. Compared to previous demix challenges a novelty is doing this for classical music instead of pop/rock. Additional novelty could also come from a causal algorithm, if you decide to make something that works for live music. Of course there are other approaches such as end-to-end transformation that might be adapted for the task. The following example illustrates how changing the level of one instrument can alter the listening experience. The piece consists of an ensemble of five instruments: Bass, Cello, Viola, Violin 1, and Violin 2, drawn from the EnsembleSet dataset. The first sample corresponds to the original mixture. The second sample is the original track with the Violin 1 levels increased by 6 dB while keeping the rest unchanged. The last sample demonstrates the original track with the Viola levels increased by 10 dB while keeping the rest unchanged. Original Violin 1 Viola\t","keywords":"","version":"Next"},{"title":"Lyric Intelligibility Baseline","type":0,"sectionRef":"#","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline","content":"","keywords":"","version":"Next"},{"title":"A. Overview​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#a-overview","content":" Challenge entrants are supplied with a fully functioning baseline system, which is illustrated in Figure 1.  Figure 1. The Lyric Intelligibility Baseline  Click here for overview of Figure 1 A scene generator (blue box): Selects the stereo music signal.Gives a value of α\\alphaα (metadata) that sets the balance between intelligibility and audio quality (see evaluation below). The music enhancement stage (pink box) takes the music as inputs and attempts to improve the intelligibility.Listener characteristics (green oval) are audiograms and compressor settings to allow personalised processing in the enhancement stage and are also used in objective evaluation.The enhancement outputs are evaluated using objective metrics (orange boxes): For intelligibility using a metric based on Whisper (correct transcribed words ratio).For audio quality via the Hearing-Aid Audio Quality Index (HAAQI) [1]. Your challenge is to improve what happens in the pink, music enhancement box. The rest of the baseline is fixed and should not be changed.  ","version":"Next","tagName":"h2"},{"title":"B. Music Enhancer​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#b-music-enhancer","content":" Figure 2 shows the music enhancer supplied in the baseline. Your task is to improve this.    Figure 2. The Baseline Music Enhancer  The baseline approach is to demix the stereo music into vocals and instrumentation. To change intelligibility we apply amplification to the vocals and accompaniment before recombining the signals. The final stage is to apply a frequency-dependent amplification to correct for the hearing loss.  Demixing uses Conv-TasNet [1] as this can be used in either a causal and non-causal form. The overall architecture is Fig 1A in reference [1], encoder-separator-decorer. The implementation is a fully convolutional system.  Linear gains are applied to the vocals (VampV_{amp}Vamp​) and accompaniment (AampA_{amp}Aamp​) using the following formulations:  Vamp=β2+1Aamp=2−Vamp\\begin{align} V_{amp} &amp;= \\beta^2 + 1 \\tag{1} \\\\ A_{amp} &amp;= 2 - V_{amp} \\tag{2} \\end{align}Vamp​Aamp​​=β2+1=2−Vamp​​(1)(2)​  where β\\betaβ = α\\alphaα, the balance parameter given in the metadata. Eqns (1) and (2) were chosen so for α\\alphaα=0 the gains are 0 dB for both the vocals and the accompaniment so the original mix is achieved, and for α\\alphaα=1 only the vocals remain, which ought to be maximum intelligibility. A quadratic function was empirically chosen to reduce the gain difference between vocals and accompaniment for low α\\alphaα values.  Remixing to stereo This is a sum of the amplified vocals and accompaniment.  Amplification is a simulation of hearing-aid non-linear amplification - see amplification for more details.  Output is FLAC format 16-bit, 44.1 kHz.  ","version":"Next","tagName":"h2"},{"title":"C. Objective Evaluation​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#c-objective-evaluation","content":" ","version":"Next","tagName":"h2"},{"title":"C.1 Intelligibility​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#c1-intelligibility","content":" The enhanced audio is first passed through the MSBG hearing loss simulator in the pyClarity codebase [2]. It is then passed through the Whisper ASR algorithm (size: base) [3] to gain a text transcription. The objective score will be the transcription correctness computed as the ratio of correct transcribed words compared to the correct transcript of the lyrics.  LI=∑Correct words∑Total words\\begin{align} L_I &amp;= \\frac{\\sum{\\text{Correct words}}}{\\sum{\\text{Total words}}} \\tag{3} \\end{align}LI​​=∑Total words∑Correct words​​(3)​  ","version":"Next","tagName":"h3"},{"title":"C.2 Quality​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#c2-quality","content":" The enhanced audio is evaluating using the HAAQI implementation in pyClarity. This is an intrusive metric and requires a reference. The reference signal is the original stereo track but with the vocals amplified by 1 dB and the accompaniment attenuated by 1 dB. This is done because research shows a small amplification of lyrics is preferred by people with hearing loss [5]. The reference is also amplified using a simulation of a simple hearing-aid - see amplification for more details.  Q=HAAQI(reference,processed)\\begin{align} Q &amp;= HAAQI(\\text{reference}, \\text{processed}) \\tag{4} \\end{align}Q​=HAAQI(reference,processed)​(4)​  ","version":"Next","tagName":"h3"},{"title":"C.3 Overall score​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#c3-overall-score","content":" To rank the teams, the intelligibility LI and quality Q ratings are combined as follows:  score=α 𝑄+(1−α) 𝐿𝐼\\begin{align} score = \\alpha ~ 𝑄 + (1 − \\alpha) ~ 𝐿_𝐼 \\tag{5} \\end{align}score=α Q+(1−α) LI​​(5)​  ","version":"Next","tagName":"h3"},{"title":"D. Baseline Results​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#d-baseline-results","content":" Baseline\tHAAQI\tCorrectness\tOverall\tZ_NormalizedCausal\t0.7755\t0.3732\t0.6514\t0.2486 NonCausal\t0.7841\t0.3857\t0.6649\t0.2597  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Lyric Intelligibility Baseline","url":"/docs/cadenza2/Lyric Intelligibility/lyric_baseline#references","content":"   [1] Luo, Y. and Mesgarani, N., 2019. Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8), pp.1256-1266. [2] Tu, Z., Ma, N. and Barker, J., 2021. Optimising hearing aid fittings for speech in noise with a differentiable hearing loss model. arXiv preprint arXiv:2106.04639. [3] Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C. and Sutskever, I., 2023, July. Robust speech recognition via large-scale weak supervision. In International conference on machine learning (pp. 28492-28518). PMLR. [4] Kates, J. M., &amp; Arehart, K. H. (2015). The hearing-aid audio quality index (HAAQI). IEEE/ACM transactions on audio, speech, and language processing, 24(2), 354-365. [5] Benjamin, A.J. and Siedenburg, K., 2023. Exploring level-and spectrum-based music mixing transforms for hearing-impaired listeners. The Journal of the Acoustical Society of America, 154(2), pp.1048-1061. ","version":"Next","tagName":"h2"},{"title":"Rebalancing Classical Music Task Overview","type":0,"sectionRef":"#","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_overview","content":"","keywords":"","version":"Next"},{"title":"A. Task Overview​","type":1,"pageTitle":"Rebalancing Classical Music Task Overview","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_overview#a-task-overview","content":" Figure 1. A simplified schematic of the baseline system.  A scene generator (blue box): Selects the stereo music signal.Gives the target gains (metadata) for the different instruments in the ensemble. The music enhancement stage (pink box) takes the music as inputs and attempts to make a new mix with the target gains.Listener characteristics (green oval) are audiograms and compressor settings to allow personalised processing in the enhancement stage and are also used in objective evaluation.The enhancement outputs are evaluated (orange box) for audio quality using the Hearing-Aid Audio Quality Index (HAAQI)  Your challenge is to improve what happens in the pink music enhancement box. The rest of the baseline is fixed and should not be changed.  ","version":"Next","tagName":"h2"},{"title":"B. Causality​","type":1,"pageTitle":"Rebalancing Classical Music Task Overview","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_overview#b-causality","content":" We are interested in both causal and non-causal systems. Non-causal systems could be used for recorded music, whereas causal systems would also work for live listening. The allowed latency for causal systems will be 5 milliseconds, that is, systems cannot look beyond 5 ms into the future.    ","version":"Next","tagName":"h2"},{"title":"C. Evaluation​","type":1,"pageTitle":"Rebalancing Classical Music Task Overview","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_overview#c-evaluation","content":" Systems will be evaluated using HAAQI [1] objective metric. HAAQI is an intrusive metrics and the reference will be the mixture of the original isolated sources rebalanced using the target gains.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Rebalancing Classical Music Task Overview","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_overview#references","content":"   [1] Kates, J. M., &amp; Arehart, K. H. (2015). The hearing-aid audio quality index (HAAQI). IEEE/ACM transactions on audio, speech, and language processing, 24(2), 354-365. ","version":"Next","tagName":"h2"},{"title":"Download data and software","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/download","content":"","keywords":"","version":"Next"},{"title":"A. Software​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza2/Take Part/download#a-software","content":" All the necessary software to run the recipes and make your own submission is available on our Clarity-Cadenza GitHub repository.  The official code for the Second Cadenza Challenge was released in version v0.6.0. To avoid any conflict, we highly recommend for you to work using this version and not with the code from the main branch. To install this version you have three options  1. Download the files of the release v0.6.0 download from https://github.com/claritychallenge/clarity/releases/tag/v0.6.0unpacked the package From inside the directory, run: pip install -e .   2. Clone the repository and checkout version v0.6.0 git clone https://github.com/claritychallenge/clarity.git git checkout tags/v0.6.0 cd clarity pip install -e .   3. Install pyclarity version 0.6.0 from PyPI pip install pyclarity==0.6.0   ","version":"Next","tagName":"h2"},{"title":"B. Data​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza2/Take Part/download#b-data","content":" ","version":"Next","tagName":"h2"},{"title":"B.1 Download the data for Task 1​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza2/Take Part/download#b1-download-the-data-for-task-1","content":" The data is available in one packages, please complete this form to request access.  cadenza_cad2_task1_train.v1_0.tar.gz [15 GB] - audio data and metadata.  Unpack the packages  After downloading the packages Unpack the packages under the same root directory as:  tar -xvzf &lt;PACKAGE_NAME&gt;   ","version":"Next","tagName":"h3"},{"title":"B.2 Download the data for Task 2​","type":1,"pageTitle":"Download data and software","url":"/docs/cadenza2/Take Part/download#b2-download-the-data-for-task-2","content":" The data is available in several packages, please complete this form to request access.  CadenzaWoodwind.zip [7.5 GB] - CadenzaWoodwind audio dataset.EnsembleSet_Mix_1.zip [5.2 GB] - Small subset of EnsembleSet.metadat.zip [209 kB] - metadata for the system.Stereo_Reverb_Real_Data_For_Tuning.zip [182 MB] - Small subset of real recordings for tuning.  Unpack the packages  After downloading the packages, save them in the same directory and run the script process_zenodo_download.py found with the baseline on GitHub. ","version":"Next","tagName":"h3"},{"title":"Registration","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/cc2_registration","content":"Registration Sign up to our Google group for alerts about the challenges and to help shape the challenges. To enter the challenge, teams are required to register using the form below. Please submit one form per team, providing a single contact email address. When the submission date approaches, you will be sent a team ID and an individualised link for submitting materials. Loading…","keywords":"","version":"Next"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/FAQs","content":"","keywords":"","version":"Next"},{"title":"Learning resources​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza2/Take Part/FAQs#learning-resources","content":" We have extensive resources that provide background information on many aspects: hearing loss, hearing aid processing, mixing/remixing, and perceptual testing. To see current discussions of the challenge, please join the Google group.  ","version":"Next","tagName":"h3"},{"title":"I'm unclear if my approach is within the rules​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza2/Take Part/FAQs#im-unclear-if-my-approach-is-within-the-rules","content":" Please contact the Cadenza Team via cadenzachallengecontact@gmail.com with details of what you want to do so we can advise.  ","version":"Next","tagName":"h3"},{"title":"What data can I use?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza2/Take Part/FAQs#what-data-can-i-use","content":" You should be able to find the information on the Rules page.  ","version":"Next","tagName":"h3"},{"title":"I want to enter the challenge without using the demixing/remixing approach​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza2/Take Part/FAQs#i-want-to-enter-the-challenge-without-using-the-demixingremixing-approach","content":" This is fine and we welcome alternative approaches. Please make sure that your method is clear in your technical report.  ","version":"Next","tagName":"h3"},{"title":"How do I actually enter?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/cadenza2/Take Part/FAQs#how-do-i-actually-enter","content":" See the Submission page. Please make sure you have signed up to our Google group, as this will ensure you get updates about the challenge. ","version":"Next","tagName":"h3"},{"title":"Important Dates","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/key_dates","content":"Important Dates July 2024: Challenge launch with training/development datasets; baseline; rules and documentation.Dec 2024: Release evaluation datasets.17th Jan 2025: Submission deadline for evaluation by objective metrics. (23:59 AoE)Feb-May 2025: Listening test evaluation period.May-June 2025: Workshop. We're exploring a special session at ICA/ASA 2025 otherwise will be online.","keywords":"","version":"Next"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/rules","content":"","keywords":"","version":"Next"},{"title":"1. Teams​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#1-teams","content":" Teams must pre-register and nominate a contact person.Teams can be from one or more institution.Anonymous entries are allowed.You must provide a technical document of up to 2 pages describing the system/model, what data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).You are encouraged to make your code open source so others can build on your work.  ","version":"Next","tagName":"h2"},{"title":"2. What information can I use?​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#2-what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"2.1. Training and development​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#21-training-and-development","content":" There is no limit on the amount of training data that can be generated using our tools.Each task lists what data can be used for developing the systems: Lyric intelligibility data.Rebalancing classical data. You can use standard augmentation techniques such as: randomizing the stems, flipping right and left channels, applying SpecAugmentation [1], pitch shifting, etc. If you are unclear whether an augmentation technique is acceptable, please contact the Cadenza team. You may not use other datasets without prior approval by the Cadenza team. If you think there is a public dataset that would be a good addition to the challenge then please propose it to the Cadenza team. If we agree, we will then make it available to all teams. Pretrained models These can only have been developed with public databases.You must not use pretrained models developed on private datasets.You must not use pretrained models that included any of the CAD2 evaluation data in their training. All the audio or metadata can be used during training and development.You must not use the evaluation data set for training or tuning the system.  ","version":"Next","tagName":"h3"},{"title":"2.2. Evaluation​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#22-evaluation","content":" The only data that can be used during evaluation are:  The audiograms giving the listener characterisation for personalisation.The stereo music input signals.For rebalancing classical music the target gains.For lyric intelligibility the 𝛼 value that gives the balance between intelligibility and quality during evaluation.  ","version":"Next","tagName":"h3"},{"title":"3. Ranking​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#3-ranking","content":" If a system has broken the rules they will not appear in the official rankings.  Objective: entries will be ranked according to average score across all signals in the evaluation dataset.Listening panel: entries will be ranked according to average score across all evaluation signals auditioned in the listening tests.  We will also report whether the systems are causal or non-causal in the rank order table and model size.  ","version":"Next","tagName":"h2"},{"title":"4. Prizes​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#4-prizes","content":" We will be awarding prizes for the best systems in our rankings:  Objective score for lyric intelligibilityListening panel score for lyric intelligibilityObjective score for rebalancing classical music  A prize pot of $1000 has been made available by the generosity of the IEEE SPS.  Anonymous entries and those with direct links to the Cadenza project team are ineligible for cash prizes, sorry.  ","version":"Next","tagName":"h2"},{"title":"5. Computational restrictions​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#5-computational-restrictions","content":" Systems must either be: causal and low latency to allow them to work with live music, ornon-causal, for use with recorded music. The latency restrictions for causal entries are that the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future i.e., no information from input samples &gt;t+5 ms. See this blog post from our sister Clarity project for more.There is no limit on computational cost, but entrants must report model size.Teams must start with the baseline, with the blocks that can be changed labelled EnhancementWhile we have provided metrics for evaluation, other metrics and approaches can be used by the teams during training.  ","version":"Next","tagName":"h2"},{"title":"6. Submitting multiple entries for a task​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#6-submitting-multiple-entries-for-a-task","content":" This will be allowed in two circumstances:  If very different approaches are used.For the lyric intelligibility, you could submit a system tuned for objective evaluation and another tuned for the listening panel.  ","version":"Next","tagName":"h2"},{"title":"7. Intellectual property​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#7-intellectual-property","content":" The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to these.  The submission constitutes the audio files submitted to the challenge and the accompanying technical report.  The Challenge Organiser is the Cadenza Project team.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the submission.  Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.  ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Rules","url":"/docs/cadenza2/Take Part/rules#references","content":" [1] Park, D.S., Zhang, Y., Chiu, C.C., Chen, Y., Li, B., Chan, W., Le, Q.V. and Wu, Y., 2020, May. Specaugment on large scale datasets. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6879-6883). IEEE. ","version":"Next","tagName":"h2"},{"title":"Submission","type":0,"sectionRef":"#","url":"/docs/cadenza2/Take Part/submission","content":"","keywords":"","version":"Next"},{"title":"1. What audio do I need to submit?​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#1-what-audio-do-i-need-to-submit","content":" You must submit the following audio for all the signals in the evaluation set:  ","version":"Next","tagName":"h2"},{"title":"1.1 Task 1 - Lyrics Intelligibility​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#11-task-1---lyrics-intelligibility","content":" The enhanced stereo signal.16-bit44.1 kHz sampling rateCompressed using the lossless FLAC compressor0 dB FS corresponds to 100 dB SPL  ","version":"Next","tagName":"h3"},{"title":"1.2 Task 2 - Rebalancing Classical Music​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#12-task-2---rebalancing-classical-music","content":" The rebalanced stereo signal.16-bit32 kHz sampling rateCompressed using the lossless FLAC compressor0 dB FS corresponds to 100 dB SPL  ","version":"Next","tagName":"h3"},{"title":"2. Code​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#2-code","content":" We encourage you to make your code and models open source.  ","version":"Next","tagName":"h2"},{"title":"3. Technical report​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#3-technical-report","content":" Draft: A draft of the technical report needs to be uploaded along with your evaluation signals.The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules. Technical Report: A two-page technical report must be submitted as a paper alongside your data.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any pre-existing tools, software and models used.We suggest you can use the ICASSP 2024 template.  ","version":"Next","tagName":"h2"},{"title":"4. How will intellectual property be handled?​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#4-how-will-intellectual-property-be-handled","content":" See here under Intellectual Property.  ","version":"Next","tagName":"h2"},{"title":"5. Where do I submit the signals?​","type":1,"pageTitle":"Submission","url":"/docs/cadenza2/Take Part/submission#5-where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a OneDrive to which you will be able to securely upload your signals. All materials uploaded will be visible to the Cadenza Team but not to other entrants.  Your processed signals should be named using the conventions used by the baseline system:  &lt;Scene ID&gt;_&lt;Listener ID&gt;_remix.flac  Any changes to the naming convention may result in your signals not being able to be scored. ","version":"Next","tagName":"h2"},{"title":"Rebalancing Classical Music Data","type":0,"sectionRef":"#","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, validation and evaluation data​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#a-training-validation-and-evaluation-data","content":" The training and validation data are provided at challenge launch. The evaluation data is provided closer to the submission deadline.  ","version":"Next","tagName":"h2"},{"title":"A.1 Training and validation data​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#a1-training-and-validation-data","content":" Training requires a large amount of audio data and there is not enough recordings of classical music ensembles with isolated instruments. Consequently, for training we are providing music ensembles synthesised from scores.  EnsembleSet [1] This contains 80 pieces from classical 17 composers. EnsembleSet has renders for 11 different instruments. We're using the string parts. CadenzaWoodwind A new data set we've created in a similar way to EnsembleSet but for five woodwind instruments (flute, clarinet, oboe, alto saxophone and bassoon). There are two quartet orchestrations: (a) flute, clarinet, oboe, and bassoon and (b) flute, alto saxophone, oboe and bassoon. The stems for each solo instrument are presented along with the two mixtures. See our Zenodo archive for the dataset for more details.  We permit the use of the following non-classical music datasets in training: MUSDB18-HQ, FMA, MedleydB version 1 and version 2, MTG-Jamendo and MousesDB. We also permit the use of pre-trained models that might have been developed using these databases.  You should not use pre-trained models that were trained on the evaluation data (BACH10 and URMP datasets).  Validation We provide a split of the training set for validation. The audio tracks are divided into consecutive 15-second segments to match how the evaluation set will be provided. The validation samples are a collection of correct mixtures (strings quartets from EnsembleSet and woodwind quartets from CadenzaWoodwind) and random mixtures to account for mixtures that contains strings and woodwind instruments.  ","version":"Next","tagName":"h3"},{"title":"A.2 Real data tuning set​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#a2-real-data-tuning-set","content":" This is a small dataset of real recordings created in the same way as the evaluation set - see below. It is intended to help you cope with any mismatch between the synthesised training+validation data and the real-recording evaluation set. It is not statistically balanced in terms of what instruments it includes. Consequently, caution is needed when using it to fine tune a machine learning model.  ","version":"Next","tagName":"h3"},{"title":"A.3 Evaluation (test) set​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#a3-evaluation-test-set","content":" The evaluation dataset are created using:  BACH10 [2], andUniversity of Rochester multi-modal music performance (URMP) [3].  BACH10 has 10 four-part J.S. Bach chorales performed on bassoon, clarinet, alto saxophone and violin. The URMP dataset a total of 44 duets, trios, quartets and quintets. The pieces are from 19 composers, including: Mozart, Tchaikovsky and Beethoven. The pieces are performed by a combination of 14 different instruments. Due to their low representation in the evaluation datasets we excluded pieces featuring double bass, horn, trombone, trumpet and tuba.  Both databases have mono recordings of isolated instruments in anechoic conditions. We have taken these and created stereo versions in small halls using convolution reverb based on ambisonic impulse responses from the Openair database [4]. See our Zenodo archive for the dataset for more details.  In the evaluation, each scene will be processed for a number of random test listeners.  danger The evaluation set should not be used for refining the system.  ","version":"Next","tagName":"h3"},{"title":"B. Metadata Information​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b-metadata-information","content":" ","version":"Next","tagName":"h2"},{"title":"B.1 Listener characteristics​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b1-listener-characteristics","content":" We provide metadata characterising the hearing abilities of listeners so the audio signals can be personalised. This is common for both tasks, so please see Listener Metadata for more details.  { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [45, 45, 35, 45, 60, 65, 70, 65], &quot;audiogram_levels_r&quot;: [40, 40, 45, 45, 60, 65, 80, 80] }, &quot;L0002&quot;: { &quot;name&quot;: &quot;L0002&quot;, ... }   ","version":"Next","tagName":"h3"},{"title":"B.2 Gains​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b2-gains","content":" We provide metadata giving the gains to use for rebalancing the mixture. There are 4 gains per music sample, but we also provide code to create more. The gains applied to each instrument are chosen as follows:  Choosing how many sources instruments have their gain altered: 1 ... N-1 (when N is the number of instrument). So for quartet 1, 2, or 3.Choosing the gain for those tracks: [-10, -6, -3, 3, 6, 10] dB.The other sources instruments have gains of 0 dB.  We also apply an additional gain to all tracks to ensure the above process does not result in large amplification or attenuation of the mix. This gain is:  G=−10log10(1N∗(10dbi10))\\begin{align} G = -10 log_{10} (\\frac{1}{N} * (10^{\\frac{dbi}{10}})) \\tag{1} \\end{align}G=−10log10​(N1​∗(1010dbi​))​(1)​    Where there are N tracks and the gain for the ith track is dBi.  In the metadata, this is then reported as the gain for each source instrument. An example for a duet would be:  { &quot;gain_0001&quot;: { &quot;source_1&quot;: -10, &quot;source_2&quot;: 0 }, &quot;gain_0002&quot;: { &quot;source_1&quot;: -6, &quot;source_2&quot;: 0 }, &quot;gain_0003&quot;: { &quot;source_1&quot;: -3, &quot;source_2&quot;: 0 }, }   ","version":"Next","tagName":"h3"},{"title":"B.3 Music​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b3-music","content":" This provide information of details of the composition  { &quot;anitrasdance_001&quot;: { &quot;source_1&quot;: { &quot;source_dataset&quot;: &quot;EnsembleSet&quot;, &quot;instrument&quot;: &quot;Cello&quot;, &quot;track&quot;: &quot;anitrasdance/Mix_1/Cello.flac&quot;, &quot;start&quot;: 46, &quot;duration&quot;: 15 }, &quot;source_2&quot;: { &quot;source_dataset&quot;: &quot;EnsembleSet&quot;, &quot;instrument&quot;: &quot;Viola&quot;, &quot;track&quot;: &quot;anitrasdance/Mix_1/Viola.flac&quot;, &quot;start&quot;: 46, &quot;duration&quot;: 15 }, &quot;source_3&quot;: { &quot;source_dataset&quot;: &quot;EnsembleSet&quot;, &quot;instrument&quot;: &quot;Violin_1&quot;, &quot;track&quot;: &quot;anitrasdance/Mix_1/Violin_1.flac&quot;, &quot;start&quot;: 46, &quot;duration&quot;: 15 }, &quot;source_4&quot;: { &quot;source_dataset&quot;: &quot;EnsembleSet&quot;, &quot;instrument&quot;: &quot;Violin_2&quot;, &quot;track&quot;: &quot;anitrasdance/Mix_1/Violin_2.flac&quot;, &quot;start&quot;: 46, &quot;duration&quot;: 15 }, &quot;mixture&quot;: { &quot;source_dataset&quot;: &quot;EnsembleSet&quot;, &quot;instrument&quot;: &quot;Mixture&quot;, &quot;track&quot;: &quot;anitrasdance/Mix_1/mix_anitrasdance.flac&quot;, &quot;start&quot;: 46, &quot;duration&quot;: 15 } }   ","version":"Next","tagName":"h3"},{"title":"B.4 Scene​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b4-scene","content":" The scenes are then:  { &quot;S50001&quot;: { &quot;music&quot;: &quot;anitrasdance_000&quot;, &quot;gain&quot;: &quot;gain_0645&quot; }, &quot;S50002&quot;: { &quot;music&quot;: &quot;anitrasdance_000&quot;, &quot;gain&quot;: &quot;gain_0713&quot; }, }   ","version":"Next","tagName":"h3"},{"title":"B.5 Scene-listener​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#b5-scene-listener","content":" { &quot;S10001&quot;: [ &quot;L0051&quot;, &quot;L0001&quot; ], &quot;S10002&quot;: [ &quot;L0028&quot;, &quot;L0012&quot; ] }   ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Rebalancing Classical Music Data","url":"/docs/cadenza2/Rebalancing Classical/rebalancing_data#references","content":"   [1] Sarkar, S., Benetos, E. and Sandler, M., 2022. Ensembleset: A new high-quality synthesised dataset for chamber ensemble separation. [2] Duan, Z. and Pardo, B., 2011. Soundprism: An online system for score-informed source separation of music audio. IEEE Journal of Selected Topics in Signal Processing, 5(6), pp.1205-1215. [3] Li, B., Liu, X., Dinesh, K., Duan, Z. and Sharma, G., 2018. Creating a multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications. IEEE Transactions on Multimedia, 21(2), pp.522-535. [4] Murphy, D.T. and Shelley, S., 2010, November. Openair: An interactive auralization web resource and database. In Audio Engineering Society Convention 129. Audio Engineering Society. ","version":"Next","tagName":"h2"},{"title":"Contact Us","type":0,"sectionRef":"#","url":"/docs/contact","content":"","keywords":"","version":"Next"},{"title":"Send us an email​","type":1,"pageTitle":"Contact Us","url":"/docs/contact#send-us-an-email","content":" You can contact the Clarity Team by email at claritychallengecontact@gmail.com  ","version":"Next","tagName":"h2"},{"title":"Join the Google group​","type":1,"pageTitle":"Contact Us","url":"/docs/contact#join-the-google-group","content":" If you wish to stay updated with Clarity Challenges please sign up the Clarity Challenge’s Google group ","version":"Next","tagName":"h2"},{"title":"Listener metadata","type":0,"sectionRef":"#","url":"/docs/icassp_2024/data/data_listener","content":"","keywords":"","version":"Next"},{"title":"Data file formats and naming conventions​","type":1,"pageTitle":"Listener metadata","url":"/docs/icassp_2024/data/data_listener#data-file-formats-and-naming-conventions","content":" Audiogram data is stored in a JSON file per dataset with the following format.  { &quot;L0001&quot;: { &quot;name&quot; : &quot;L0001&quot;, &quot;audiogram_cfs&quot; : [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot; : [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot; : [ ... ], }, ... }  ","version":"Next","tagName":"h2"},{"title":"Additional tools","type":0,"sectionRef":"#","url":"/docs/icassp_2024/software/additional_tools","content":"Additional tools The Clarity-Cadenza git include a variety of additional tools. These include hearing loss simulation, a range of objective measures and standard hearing aid processors. For more details see the readme on the git.","keywords":"","version":"Next"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/icassp_2024/icassp_2024_results","content":"Results The 2024 ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids is now complete. Results are shown below along with links to the system description papers. The table below reports the HAAQI score for all the submitted systems. Where a system ID ends with '_aug', data augmentations techniques has been used. Those that end with '_sup' has used supplemented data. Links to the system reports are also provided when available. Rank\tTeam\tID\tPaper\tHAAQI10\tBaseline\tHDemucs 0.5697 16\tBaseline\tOpenUnMix 0.5113 4\tT003\tE003 0.5923 3\tT003\tE003_sup 0.5929 18\tT009\tE009 0.4784 17\tT009\tE009-B 0.4794 7\tT011\tE011 0.5798 5\tT011\tE011_aug 0.5857 8\tT012\tE012 0.5731 19\tT016\tE016 0.1436 6\tT018\tE018 0.5849 2\tT022\tE022 0.6309 1\tT022\tE047 0.6317 11\tT025\tE025 0.561 15\tT031\tE031 0.5304 12\tT031\tE031_aug 0.5434 13\tT042\tE042 0.5426 14\tT042\tE042_aug 0.5343 9\tT046\tE046 0.5704","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/docs/icassp_2024/intro","content":"","keywords":"","version":"Next"},{"title":"What makes the demix different to previous demix challenges?​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#what-makes-the-demix-different-to-previous-demix-challenges","content":" The left and right signals you are working with are those picked up by a microphone at each ear when the person is listening to a pair of stereo loudspeakers. This means the signals at the ear that you have for demix is a combination of both the right and left stereo signals because of cross-talk (see Figure 1). This cross-talk will be strongest at low frequency when the wavelength is largest. This means that the spatial distribution of an instrument will be different in the microphone signals at the ear compared to the original left-right music signals. Stereo demix algorithms will need to be revised to allow for this frequency-dependent change. We will model the cross-talk using HRTFs (Head Related Transfer Functions), assuming the music comes from a pair of stereo loudspeakers in a dead room.  Figure 1, The scenario.  Although in the long term demixing on hearing aids would need to be causal and low latency, for ICASSP 2024 we are allowing causal and non-causal approaches. If you produce a low latency solution that will be great and very novel. There are increasing number of DNN approaches for causal signal processing from other areas such as speech that could be adapted for this.  ","version":"Next","tagName":"h2"},{"title":"Do I have to demix and then downmix to stereo?​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#do-i-have-to-demix-and-then-downmix-to-stereo","content":" Our baseline does demixing, but you don't have to. You could create an end-to-end system without an explicit demixing stage if you want.  ","version":"Next","tagName":"h2"},{"title":"Do I need to know about hearing loss and hearing aids?​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#do-i-need-to-know-about-hearing-loss-and-hearing-aids","content":" Not really. We provide code for a standard amplification that is done by simple hearing aids. The challenge is mostly about rebalancing the music. We use a metric developed for hearing aids, but you could use another quality metric like Signal to Distortion Ratio (SDR) to develop your systems if you prefer. If you want to learn more about hearing loss and aids, however, there is lots of information in our learning resources.  ","version":"Next","tagName":"h2"},{"title":"The task​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#the-task","content":" Figure 2 shows a simplified schematic of the baseline system:  A scene generator (blue box) creates the scene characteristics: Music signal at the hearing aids' microphones.The required gains for the output signal for the vocals, drums, bass and other (VDBO).The reference rebalanced signal for scoring. The music enhancement stage (pink box) takes the music at the microphones as inputs and attempts to generate the rebalanced output: It estimates the VDBO components from the mixture.Then, it remixes the signal using the gains.Lastly, it applies listener-specific amplification following a standard hearing aid fitting. Listener characteristics (green oval) are audiograms and are supplied to both the enhancement and evaluation.The enhancement outputs are evaluated for audio quality via the Hearing-Aid Audio Quality Index (HAAQI) [1] (orange box). Note, HAAQI is an intrusive measure that requires a reference signal.  Figure 2, The simplified baseline schematics. For simplicity, not all signal paths are shown.  Your challenge is to improve what happens in the pink music enhancement box. The rest of the baseline is fixed and should not be changed.  We are interested in systems that are either: (i) causal and low latency for live music, and (ii) non-causal for recorded music.  ","version":"Next","tagName":"h2"},{"title":"What is being provided​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#what-is-being-provided","content":" You will have access to:  Full length songs from MUSDB18-HQ dataset.HRTFs to model the propagation of sound from the loudspeakers to the hearing aid microphones.Scripts to pre-process the music and construct the music signals at the hearing aid microphones with HRTF applied.Music data for augmentation, if needed.Listeners characteristics (audiograms) - see Listener DataTarget gains for the VDBO stems used to mix the target stereo  Please refer to data page and the baseline readme in GitHub for details. To download the datasets, please visit download data and software  ","version":"Next","tagName":"h2"},{"title":"The systems' output​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#the-systems-output","content":" Stereo downmixed signals  Sample rate: 32,000 HzPrecision: 16-bit integerCompression: FLAC  For more details about the format of the submission, please refer to the submission webpage.  :::caution Note The responsibility for the final remixed signal level is yours. It’s worth bearing in mind there may be clipping in the evaluation block in some tasks if the processed signals are too large. :::  ","version":"Next","tagName":"h2"},{"title":"Evaluation Stage​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#evaluation-stage","content":" Warning You are not allowed to change the evaluation script provided in the baseline. Your output signals with be scored using this script.  The evaluation stage computes HAAQIscores for the remixed stereo - see Python HAAQI implementation.  The output of the evaluation stage is a CSV file with all the HAAQI scores.  ","version":"Next","tagName":"h2"},{"title":"Cite as​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#cite-as","content":" @misc{roa2023cadenza, title={The Cadenza ICASSP 2024 Grand Challenge}, author={Gerardo Roa Dabike and Michael A. Akeroyd and Scott Bannister and Jon Barker and Trevor J. Cox and Bruno Fazenda and Jennifer Firth and Simone Graetzer and Alinka Greasley and Rebecca Vos and William Whitmer}, year={2023}, eprint={2310.03480}, archivePrefix={arXiv}, primaryClass={eess.AS}, note={to be submitted in ICASSP 2024} }   ","version":"Next","tagName":"h2"},{"title":"References​","type":1,"pageTitle":"Overview","url":"/docs/icassp_2024/intro#references","content":"   [1] Kates, J.M. and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858. ","version":"Next","tagName":"h2"},{"title":"Baseline","type":0,"sectionRef":"#","url":"/docs/icassp_2024/software/baseline","content":"","keywords":"","version":"Next"},{"title":"The Pre-Process blocks​","type":1,"pageTitle":"Baseline","url":"/docs/icassp_2024/software/baseline#the-pre-process-blocks","content":" The system starts by applying HRTFs to the music of MUSDB18-HQ dataset, simulating the music as it is picked up by the hearing aids microphones. This stage is illustrated by the &quot;pre-process enhancement&quot; and &quot;pre-process evaluation&quot; boxes. However, in practice both boxes correspond to the output of the generate_at_mic_musdb18.py script.  First, it takes the Scene details: MUSDB18-HQ music (mixture, vocal, drums, bass, other).Subject head and loudspeaker-position (HRTFs). It applies the HRTFs to the left and right side of all signals (mixture and VDBO components) [Figure 2]The mixture with HRTF applied corresponds to the output of the &quot;pre-process enhancement&quot; block.The VDBO signals with HRTF applied correspond to the output of the &quot;pre-process evaluation&quot; block.  Figure 3, The scenario.  ","version":"Next","tagName":"h3"},{"title":"The Enhancement block​","type":1,"pageTitle":"Baseline","url":"/docs/icassp_2024/software/baseline#the-enhancement-block","content":" The enhancement takes a mixture signal as it is picked up by the hearing aids microphones and attempts to output a personalized rebalanced stereo rendition of the music.  First, it takes stereo tracks (&quot;mixture at the hearing aid mics&quot;) and demixes them into their VDBO (vocal, drums, bass and other) representation. This is done by using an out-of-the-box audio source separation system.Then, using the gains provided, the music is downmixed to stereo after changing the level of the different elements of the music.Next, the downmixed signal is normalised to match the LUFS level of the input mixture.NAL-R amplification is applied to the normalised downmixed signal, allowing for a personalised amplification for the listener using a standard hearing aid algorithm.This amplified signal is the output of the system: Processed signal  ","version":"Next","tagName":"h3"},{"title":"The Evaluation block​","type":1,"pageTitle":"Baseline","url":"/docs/icassp_2024/software/baseline#the-evaluation-block","content":" The evaluation generates the reference and processed signals and computes the HAAQI score.  First, it takes the VDBO signals at the hearing aid microphones (these are the VDBO components provided by MUSDB18-HQ with the HRTF applied to them) and remixes the signals using the same gains as applied in the enhancement.Then, it normalises the remix to the same LUFS level as the &quot;mixture at the hearing aid mics&quot;.Next, it applies the NAL-R amplification.This process results in the Reference signal for HAAQI, which simulates a &quot;listener preferred mixture&quot;. The reference signal corresponds to an ideal rebalanced signal when we have access to the clean VDBO components.As HAAQI is an intrusive metric, the score is computed by comparing the Processed signal (downmixed music) with the Reference signal, focussing on changes to time-frequency envelope modulation, temporal fine structure and long-term spectrum.  Note In the Enhancement and Evaluation blocks, we apply a loudness normalisation (in LUFS) after applying the gains. This is to keep the loudness of the remix at the same levels as the mixture at the hearing aid mics.As required by HAAQI, we resample both the reference and processed signal before computing the score.  ","version":"Next","tagName":"h3"},{"title":"Baseline Scores​","type":1,"pageTitle":"Baseline","url":"/docs/icassp_2024/software/baseline#baseline-scores","content":" Two baseline systems are proposed by employing two out-of-the-box audio source separation systems in the enhancement block.  Hybrid Demucs [2] distributed on TorchAudioOpen-Unmix [3] distributed through Pytorch hub.  The average HAAQI scores are:  System\tLeft HAAQI\tRight HAAQI\tOverallDemucs\t0.6690\t0.6665\t0.6677 OpenUnmix\t0.5986\t0.5940\t0.5963  ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Baseline","url":"/docs/icassp_2024/software/baseline#references","content":"   [1] Kates, J.M. and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858  [2] Défossez, A. &quot;Hybrid Spectrogram and Waveform Source Separation&quot;. Proceedings of the ISMIR 2021 Workshop on Music Source Separation. doi:10.48550/arXiv.2111.03600  [3] Stöter, F. R., Liutkus, A., Ito, N., Nakashika, T., Ono, N., &amp; Mitsufuji, Y. (2019). &quot;Open-Unmix: A Reference Implementation for Music Source Separation&quot;. Journal of Open Source Software, 4(41), 1667. doi:10.21105/joss.01667 ","version":"Next","tagName":"h2"},{"title":"Download data and software","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/download","content":"","keywords":"","version":"Next"},{"title":"A. Software​","type":1,"pageTitle":"Download data and software","url":"/docs/icassp_2024/take_part/download#a-software","content":" All the necessary software to run the recipes and make your own submission is available on our Clarity-Cadenza GitHub repository.  The official code for the ICASSP 2024 Cadenza Challenge was released in version v0.4.1. To avoid any conflict, we highly recommend for you to work using this version and not with the code from the main branch. To install this version you have three options  1. Download the files of the release v0.4.1 download from https://github.com/claritychallenge/clarity/releases/tag/v0.4.1unpacked the package From inside the directory, run: pip install -e .   2. Clone the repository and checkout version v0.4.1 git clone https://github.com/claritychallenge/clarity.git git checkout tags/v0.4.1 cd clarity pip install -e .   3. Install pyclarity version 0.4.1 from PyPI pip install pyclarity==0.4.1   ","version":"Next","tagName":"h2"},{"title":"B. Data​","type":1,"pageTitle":"Download data and software","url":"/docs/icassp_2024/take_part/download#b-data","content":" ","version":"Next","tagName":"h2"},{"title":"B.1 Download the packages​","type":1,"pageTitle":"Download data and software","url":"/docs/icassp_2024/take_part/download#b1-download-the-packages","content":" The data is available in five packages, please complete this form to request access.  All participants will require the core data packages.  Participants wanting to extend the training data can use either of the augmentation data packages.  cad_icassp_2024_core.v1.1.tgz [230 KB] - metadata and hrtf signals.cad_icassp_2024_train.v1.0.tgz [14.8 GB] - audio data for training systems.cad_icassp_2024_validation.v1.0.tgz [5.9 GB] - audio and metadata data for validation.cad_icassp_2024_evaluation.v1.0.tgz [1.4 GB] - audio and metadata data for evaluation (only mixtures).cad_icassp_2024_medleydb.v1.0.tgz [38.1 GB] - optional audio data for training augmentation.bach10.zip [125 MB] - optional audio data for training augmentation.fma_small.zip [7.2 GB] - optional audio data for training augmentation.  ","version":"Next","tagName":"h3"},{"title":"B.2 Unpack the packages​","type":1,"pageTitle":"Download data and software","url":"/docs/icassp_2024/take_part/download#b2-unpack-the-packages","content":" After downloading the packages Unpack the packages under the same root directory as:  tar -xvzf &lt;PACKAGE_NAME&gt;  ","version":"Next","tagName":"h3"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/icassp_2024/software/core_software","content":"","keywords":"","version":"Next"},{"title":"HAAQI Audio Quality model​","type":1,"pageTitle":"Core Software","url":"/docs/icassp_2024/software/core_software#haaqi-audio-quality-model","content":" This is a python implementation of the Hearing Aid Audio Quality Index (HAAQI) model which is used for objective estimation. This will be used in the stage 1 evaluation of entrants (see Rules).  Note that HAAQI is not a binaural metric, instead, each channel must be processed separately. We average the left and right scores to produce a final overall score.  Instructions and recommendations on the use of HAAQI You can call the HAAQI metric using the clarity.evaluator.haaqi.compute_haaqi function as: compute_haaqi( processed_signal, reference_signal, processed_sample_rate, reference_sample_rate, audiogram, equalisation, level1 ) The processed_signal corresponds to the output signal from enhancement block in the baseline.The reference_signal corresponds to the amplified reference signal. This is generated by the evaluation block in the baseline.It is recommended to resample the processed_signal and reference_signal signals to 24,000 Hz before calling the metric.Set the parameters processed_sample_rate and reference_sample_rate equal to 24000.The audiogram corresponds to the left or right audiogram object.Set equalisation to 2, which indicates to HAAQI that the reference_signal has NAL-R applied to it.Set the level1 parameter to the level of the reference signal before applying the hearing aid amplification (NAL-R). Recommended to set it as 65 - 20 * log10(RMS(reference signal before NAL-R)) Note Please note that the level1 parameter uses the reference signal without NAL-R amplification. And, the reference_signal expects the amplified version of the same signal used in level1.  ","version":"Next","tagName":"h2"},{"title":"4. References​","type":1,"pageTitle":"Core Software","url":"/docs/icassp_2024/software/core_software#4-references","content":"   [1] Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265. ","version":"Next","tagName":"h2"},{"title":"Registration","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/registration","content":"Registration Sign up to our Google group for alerts about the challenges and to help shape the challenges. To enter the challenge, teams are required to register using the form below. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a OneDrive for submitting materials.Loading…","keywords":"","version":"Next"},{"title":"Important Dates","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/key_dates","content":"Important Dates 3rd September 2023: Outline website launch.15th September 2023: Full challenge launch: Release training/dev data; baseline; rules and documentation.30th November 2023: Release of evaluation data.11th December 2023: Teams submit processed signals and technical reports. (23:59 AoE)19th December 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-2024.9nd January 2024: Submission deadline for invited 2-page papers.23rd January: Notification of acceptance and reviews returned.30th January: Camera-ready copy submission.14-19th April 2024: Invited papers presented at ICASSP-2024 in Seoul, Korea.","keywords":"","version":"Next"},{"title":"ICASSP 2024 Data","type":0,"sectionRef":"#","url":"/docs/icassp_2024/data/data_overview","content":"","keywords":"","version":"Next"},{"title":"A. Training, validation and evaluation data​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#a-training-validation-and-evaluation-data","content":" The music dataset is based on MUSDB18-HQ [1] and a subset of the OlHeaD-HRTF [2].  ","version":"Next","tagName":"h2"},{"title":"A.1 Training data​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#a1-training-data","content":" The training data needs to be generated by the entrants (to reduce download size). We provide:  A mirror of the MUSDB18-HQ training split. (22 GB) 100 songs.44,100 Hz sample rate.16-bit. A set of 96 HRTFs. (300 KB) 16 subjects heads6 different loudspeaker angle positions (±\\pm± 22.5°\\degree°, ±\\pm± 30.0°\\degree°, and ±\\pm± 37.5°\\degree°) 400 scenes descriptions: The MUSDB18-HQ track for each scene.4 different left/right loudspeaker angle position A script (part of the software) to generate the training set.  To generate the training set please do:  Download and install PyClarity as described in Download data and software.Download and extract the core and train data packages as described in Download data and software.Go to recipes/cad_icassp_2024/generate_dataset directoryRun generate_at_mic_musdb18.py setting the path.root parameter to the directory where you saved the core data.  python3 generate_at_mic_musdb18.py \\ path.root=/Volumes/cadenza_data/icassp_2024/   The script will iterate through the scenes, generating the at the microphone (at-mic) music samples. This process will create:  The directory .../icassp_2024/audio/at_mic_music, where the at-the-microphone signals will be saved (88 GB). This location can be changed by setting the path.output_music_dir parameter.The file .../icassp_2024/metadata/at_mic_music.train.json with the scenes metadata necessary to run the baseline. This location can be changed by setting the path.output_music_file parameter.  ","version":"Next","tagName":"h3"},{"title":"A.2 Validation data​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#a2-validation-data","content":" An independent validation set was constructed based on the new MoisesDB dataset [2]. Songs from MoisesDB were randomly selected to match the number of tracks per genre in the MUSDB18-HQ test set.  Genres Note that MUSDB18-HQ and MoisesDB don't share the same genres for all classes. While MUSDB18-HQ includes a compound genre, Pop/Rock, MoisesDB distinguishes between Pop and Rock genres more explicitly. Additionally, unlike MUSDB18-HQ, MoiseDB does not feature a Heavy Metal class.  Genre\tValidation Set\tMUSDB18 TestReggae\t2\t2 Rap\t3\t3 Pop\t20\t- Rock\t21\t- Pop/Rock\t-\t37 Heavy Metal\t-\t4 Electronic\t4\t4 Total\t50\t50  The validation dataset is available as a downloadable package in the download data and software section. No data generation is required.  Each track was divided into several consecutive 10-second excerpts, ensuring that no silent segments were selected. Then, a random Head-Related Transfer Function (HRTF) was applied to each excerpt. This means that two excerpts from the same track will have different pairs of HRTFs applied, thus requiring separation models to be robust under varying HRTF conditions and for different songs  967 samples10-second duration44,100 Hz16-bit  Note that for the validation dataset, only two listeners were assigned per scene in the scene_listeners.json file. For more details about the scene_listener metadata file, please refer to section [C.5 Scene-listeners](#C.5 Scene-listeners) below.  ","version":"Next","tagName":"h3"},{"title":"A.3 Evaluation (test) set​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#a3-evaluation-test-set","content":" The evaluation set is based in the MUSDB18-HQ test set (50 tracks).  The MUSDB18-HQ has the following genre distribution:  Genre\tTracksReggae\t2 Rap\t3 Pop/Rock\t37 Heavy Metal\t4 Electronic\t4 Total\t50  Following the same procedure as the validation set, each track is divided into several consecutive 10-second excerpts, ensuring that no silent segments are selected. Then, a randomly selected HRTF is applied to each excerpt.  960 samples10-second duration44,100 Hz16-bit  In the evaluation, each scene will be processed for 20 random test listeners.  note The evaluation set only contains the mixture signal. This is to ensure it is not use for refining the systems.  ","version":"Next","tagName":"h3"},{"title":"B. Training augmentation data​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#b-training-augmentation-data","content":" ","version":"Next","tagName":"h2"},{"title":"B.1 New Scenes​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#b1-new-scenes","content":" You can augment the training data by generating more random training scenes. For this, we provide the generate_dataset/generate_train_scenes.py script. This script shares the config.yaml withgenerate_dataset/generate_at_mic_musdb18.py script.  To generate more scenes, you need:  Set the parameter scene.number_scenes_per_song to the number of the different scenes to generate for each track.The default value of scenes per song is 4.This script uses the track name to seed the random generation. This ensures: a reproducible scenes' generation.increasing the number of scenes per song will always result in the 4 scenes provided in the core package plus new scenes. Run the script as:  python3 generate_train_scenes.py \\ path.root=/Volumes/cadenza_data/icassp_2024/ \\ scene.number_scenes_per_song=&lt;number greater than 4&gt;   Additionally, you can change the scene_listener.number_listeners_per_scene parameter to set how many listeners pair with the same scene. By default, this parameter is set to 2.  python3 generate_train_scenes.py \\ path.root=/Volumes/cadenza_data/icassp_2024/ \\ scene_listener.number_listeners_per_scene=&lt;number greater than 2&gt;   Consider that ... Generating new scenes will not preserve the scene_id from the core package. This is because the id is assigned incrementally. However, this does not have any significance in the process as these scenes are used exclusively for training separation models.  ","version":"Next","tagName":"h3"},{"title":"B.2 Augmenting tracks​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#b2-augmenting-tracks","content":" Teams can supplement the training data using the following resources:  Bach10FMA-smallMedleydB version 1 and version 2  We leave the teams to decide how to use these as part of the training of their systems. The supplemental data will need HRTFs applied to them as we did for MUSDB18-HQ. Note, 46 songs from MedleydB version 1 are already part of the training set in MUSDB18-HQ.  MUSDB18-HQ already contains 46 tracks from the MedleyDB version 1 A Classic Education - NightOwlAimee Norwich - ChildAlexander Ross - Goodbye BoleroAlexander Ross - Velvet CurtainAuctioneer - Our Future FacesAvaLuna - WaterductBigTroubles - PhantomCelestial Shore - Die For UsClara Berry And Wooldog - Air TrafficClara Berry And Wooldog - StellaClara Berry And Wooldog - Waltz For My VictimsCreepoid - OldTreeDreamers Of The Ghetto - Heavy LoveFaces On Film - Waiting For GaGrants - PunchDrunkHelado Negro - Mitad Del MundoHezekiah Jones - Borrowed HeartHop Along - Sister CitiesInvisible Familiars - Disturbing WildlifeLushlife - Toynbee SuiteMatthew Entwistle - Dont You EverMeaxic - Take A StepMeaxic - You ListenMusic Delta - 80s RockMusic Delta - BeatlesMusic Delta - BritpopMusic Delta - Country1Music Delta - Country2Music Delta - DiscoMusic Delta - GospelMusic Delta - GrungeMusic Delta - HendrixMusic Delta - PunkMusic Delta - ReggaeMusic Delta - RockMusic Delta - RockabillyNight Panther - FirePort St Willow - Stay EvenSecret Mountains - High HorseSnowmine - CurfewsSteven Clark - BountyStrand Of Oaks - SpacestationSweet Lights - You Let Me DownThe Districts - VermontThe Scarlet Brand - Les Fleurs Du MalThe So So Glos - Emergency  For more information on augmenting and supplementing the music training data, please see the rules.  ","version":"Next","tagName":"h3"},{"title":"C. Metadata Information​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c-metadata-information","content":" ","version":"Next","tagName":"h2"},{"title":"C.1 Listener characteristics​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c1-listener-characteristics","content":" We provide metadata characterising the hearing abilities of listeners so the audio signals can be personalised. The quantification of the listeners' hearing is done with left and right audiograms. These measure the threshold at which people can hear a pure-tone sound.More information on what audiograms are and how they're measured.  For training, we provide 83 listener audiograms that were collected for the Clarity project.For validation, we constructed a new set of 50 listener audiograms from the von Gablenz and Holube (2019)dataset.For evaluation, we will use a set of 52 listener audiograms use in first Cadenza Challenges  Visit Listener Metadata for more details.  ","version":"Next","tagName":"h3"},{"title":"C.2 Gains​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c2-gains","content":" We provide metadata giving the gains to use for rebalancing the mixture. There are 1105 possible combinations created by:  Choosing how many VDBO tracks have their gain altered: 1, 2, or 3.Choosing the gain for those tracks: [-10, -6, -3, 3, 6, 10] dB.  In the metadata, this is then reported as the gain for each of the VDBO track. The same set of gains are used for training, validation and evaluation.  { ... &quot;gain_0007&quot;: { &quot;vocals&quot;: 10, # Amplify vocals in 10 dB &quot;drums&quot; : 0, &quot;bass&quot; : 0, &quot;other&quot; : 0 }, ... &quot;gain_0138&quot;: { &quot;vocals&quot;: 0, &quot;drums&quot; : -10, # Attenuate drums in 10 dB &quot;bass&quot; : 6, # Amplify bass in 6 dB &quot;other&quot; : 0 }, ... &quot;gain_0381&quot;: { &quot;vocals&quot;: 3, # Amplify vocals in 3 dB &quot;drums&quot; : 10, # Amplify drums in 10 dB &quot;bass&quot; : -6, # Attenuate bass in 6 dB &quot;other&quot; : 0 }, ... }   ","version":"Next","tagName":"h3"},{"title":"C.3 Head and loudspeaker positions​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c3-head-and-loudspeaker-positions","content":" We provide metadata indicating the subject head and loudspeaker positions. This set corresponds to a subset of the OlHeaD-HRTF [2] dataset. There are 16 subjects and 9 possible angles combinations.  Left and right HRTFs angles combination Angle Left\tAngle Right-22.5\t22.5 -22.5\t30.0 -22.5\t37.5 -30.0\t22.5 -30.0\t30.0 -30.0\t37.5 -37.5\t22.5 -37.5\t30.0 -37.5\t37.5  The data is provided in a JSON file:  { &quot;hlp_0001&quot;: { &quot;subject&quot; : &quot;VP_E1&quot;, &quot;left_angle&quot; : -37.5, &quot;right_angle&quot;: 22.5, &quot;mic&quot; : &quot;BTE_fr&quot; }, ... }   ","version":"Next","tagName":"h3"},{"title":"C.4 Scenes​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c4-scenes","content":" The scene metadata is provided in a JSON file with the following structure:  { &quot;scene_10001&quot;: { &quot;music&quot; : &quot;A Classic Education - NightOwl&quot;, &quot;gain&quot; : &quot;gain_0639&quot;, &quot;head_loudspeaker_positions&quot;: &quot;hlp_0056&quot; }, ... }   ","version":"Next","tagName":"h3"},{"title":"C.5 Scene-listeners​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c5-scene-listeners","content":" The scene-listener metadata is provided in a JSON file with the following structure:  { &quot;scene_10001&quot;: [ &quot;L0066&quot;, &quot;L0057&quot; ], ... }   ","version":"Next","tagName":"h3"},{"title":"C.6 MUSDB18-HQ​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c6-musdb18-hq","content":" The MUSDB18-HQ metadata is provided in a single JSON file per dataset.  [ { &quot;Track Name&quot; : &quot;A Classic Education - NightOwl&quot;, &quot;Genre&quot; : &quot;Singer/Songwriter&quot;, &quot;Source&quot; : &quot;MedleyDB&quot;, &quot;License&quot; : &quot;CC BY-NC-SA&quot;, &quot;Split&quot; : &quot;train&quot; }, ... ]   ","version":"Next","tagName":"h3"},{"title":"C.7 Signals at hearing aid microphones​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#c7-signals-at-hearing-aid-microphones","content":" The at_the_mic metadata is provided in a JSON file with the following structure.:  { &quot;A Classic Education - NightOwl-hlp_0056&quot;: { &quot;Track Name&quot; : &quot;A Classic Education - NightOwl-hlp_0056&quot;, &quot;Split&quot; : &quot;valid&quot;, &quot;Path&quot; : &quot;valid/A Classic Education - NightOwl-hlp_0056&quot; }, ... }   ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"ICASSP 2024 Data","url":"/docs/icassp_2024/data/data_overview#references","content":"   [1] Rafii, Z., Liutkus, A., Stöter, F.-R., Mimilakis, S. I., and Bittner, R. (2019). MUSDB18-HQ - an Uncompressed Version of MUSDB18. [Dataset]. doi:10.5281/zenodo.3338373 [2] F. Denk, S.M.A. Ernst, S.D. Ewert and B. Kollmeier, (2018): Adapting hearing devices to the individual ear acoustics: Database and target response correction functions for various device styles. Trends in Hearing, vol 22, p. 1-19. DOI:10.1177/2331216518779313 [3] Pereira, I., Araújo, F., Korzeniowski, F., &amp; Vogl, R. (2023). Moisesdb: A dataset for source separation beyond 4-stems. arXiv preprint arXiv:2307.15913 ","version":"Next","tagName":"h2"},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/FAQs","content":"","keywords":"","version":"Next"},{"title":"Learning resources​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/icassp_2024/take_part/FAQs#learning-resources","content":" We have extensive resources that provide background information on many aspects: hearing loss, hearing aid processing, mixing/remixing, and perceptual testing. To see current discussions of the challenge, please join the Google group. If you have more questions that aren't covered, please contact us.  ","version":"Next","tagName":"h3"},{"title":"What data can I use?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/icassp_2024/take_part/FAQs#what-data-can-i-use","content":" You should be able to find the information on the Rules or the Data Overview page.  ","version":"Next","tagName":"h3"},{"title":"I want to enter the challenge without using the demixing/remixing approach​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/icassp_2024/take_part/FAQs#i-want-to-enter-the-challenge-without-using-the-demixingremixing-approach","content":" This is fine and we welcome alternative approaches. Please make sure that your method is clear in your technical report.  ","version":"Next","tagName":"h3"},{"title":"How do I actually enter?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/docs/icassp_2024/take_part/FAQs#how-do-i-actually-enter","content":" You should be able to find the information you need about submitting your entry on the Submission page. Please make sure you have signed up to our Google group, as this will ensure you get updates about the challenge. ","version":"Next","tagName":"h3"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/rules","content":"","keywords":"","version":"Next"},{"title":"1. Teams​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#1-teams","content":" Teams must pre-register and nominate a contact person.Teams can be from one or more institution.  ","version":"Next","tagName":"h2"},{"title":"2. Transparency​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#2-transparency","content":" Anonymous entries are allowed.Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged to make their code open source.  ","version":"Next","tagName":"h2"},{"title":"3. What information can I use?​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#3-what-information-can-i-use","content":" Official data: refers to the training dataset generated by using the provided scenes.tran.json file.Data augmentation: refers to the use of techniques like, such as randomizing the stems, flipping right and left channels, applying SpecAugmentation, pitch shifting, etc.Supplememented data: refers to the use of additional music samples. This can be by generating extra training data using MUSDB18; or the use of MedleyDB, BACH10, or FMA.  ","version":"Next","tagName":"h2"},{"title":"3.1. Training and development​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#31-training-and-development","content":" There is no limit on the amount of training data that can be generated using our tools and the provided training data sets.You may not use other datasets.You can only use pretrained models that have been developed with public databases such as the training split of MUSDB18-HQ. You must not start with models pretrained on private datasets.All the audio or metadata can be used during training and development.You must not use the evaluation data set for training or tuning the system.Teams that decide to use data augmentation and/or supplemented data must: Using data augmentation: submit 2 systems, one without the augmentations and one with the augmentations.Using supplemented data: submit 2 systems, one without the supplemented data and one with the supplemented data.Using both augmentation and supplemented data: submit four systems, one only with the official data, one with the supplemented data, one with the data augmentation and one with both supplemented and data augmentation. Systems using the data augmentation and/or the data supplementation will be scored and ranked in the challenge.Systems trained using any other source for data supplementation not explicitly mentioned here will not enter the ranking.  \tOfficial data\tOfficial data + supplementationOfficial data\tAll teams must submit\tOptional submission Official data + augmentation\tOptional submission\tOptional submission  ","version":"Next","tagName":"h3"},{"title":"3.2. Evaluation​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#32-evaluation","content":" The only data that can be used during evaluation are:  The audiograms giving the listener characterisation for personalisation.The target gains.The stereo music input signals to the hearing aid.  ","version":"Next","tagName":"h3"},{"title":"4. Computational restrictions​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#4-computational-restrictions","content":" Systems must either be: causal and low latency to allow them to work with live music, ornon-causal, for use with recorded music. The latency restrictions for causal entries are that the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future i.e., no information from input samples &gt;t+5 ms. See this blog post from our sister Clarity project for more.There is no limit on computational cost, but entrants must report model size.Teams must start with the baseline, with the blocks that can be changed labelled EnhancementWhile HAAQI is being used for evaluation, other metrics and approaches can be used by the teams during training.  ","version":"Next","tagName":"h2"},{"title":"5. Submitting multiple entries​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#5-submitting-multiple-entries","content":" This will be allowed if different approaches are used.  ","version":"Next","tagName":"h2"},{"title":"6. Evaluation of systems​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#6-evaluation-of-systems","content":" Music: we will use the MUSDB18-HQ's evaluation set which is made up of 50 songs. We will ask teams to submit defined 10 second segments from the remixed stereo for each track.Gains: We will provide metadata giving the target mix for the VDBO tracks for the evaluation.Listener audiograms: we will use 50 real measured audiograms that we have been collected.  Entries will be ranked according to average HAAQI score across all signals in the evaluation dataset. We will also report whether the systems are causal or non-causal in the rank order table and model size.  ","version":"Next","tagName":"h2"},{"title":"7. Intellectual property​","type":1,"pageTitle":"Rules","url":"/docs/icassp_2024/take_part/rules#7-intellectual-property","content":" The following terms apply to participation in this machine learning challenge (“Challenge”). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to these.  The &quot;submission&quot; constitutes the audio files submitted to the challenge and the accompanying technical report.  The Challenge is organised by the Challenge Organiser.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. ","version":"Next","tagName":"h2"},{"title":"Submission","type":0,"sectionRef":"#","url":"/docs/icassp_2024/take_part/submission","content":"","keywords":"","version":"Next"},{"title":"1. What audio do I need to submit?​","type":1,"pageTitle":"Submission","url":"/docs/icassp_2024/take_part/submission#1-what-audio-do-i-need-to-submit","content":" You must submit the following audio for all the signals in the evaluation set:  The rebalanced stereo signal.Predefined 10-second segments.16-bit32 kHz sampling rateCompressed using the lossless FLAC compressor0 dB FS corresponds to 100 dB SPL  ","version":"Next","tagName":"h2"},{"title":"2. Code​","type":1,"pageTitle":"Submission","url":"/docs/icassp_2024/take_part/submission#2-code","content":" We encourage you to make your code open source.  ","version":"Next","tagName":"h2"},{"title":"3. Technical report​","type":1,"pageTitle":"Submission","url":"/docs/icassp_2024/take_part/submission#3-technical-report","content":" Draft: A draft of the technical report needs to be uploaded along with your evaluation signals.The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules. Technical Report: A two page technical report must be submitted as a paper alongside your data.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any pre-existing tools, software and models used.We suggest you can use the ICASSP 2024 template.The top five systems will be invited to submit a paper to the ICASSP 2024 special session.  ","version":"Next","tagName":"h2"},{"title":"4. How will intellectual property be handled?​","type":1,"pageTitle":"Submission","url":"/docs/icassp_2024/take_part/submission#4-how-will-intellectual-property-be-handled","content":" See here under Intellectual Property.  ","version":"Next","tagName":"h2"},{"title":"5. Where do I submit the signals?​","type":1,"pageTitle":"Submission","url":"/docs/icassp_2024/take_part/submission#5-where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a OneDrive to which you will be able to securely upload your signals. All materials uploaded will be visible to the Cadenza Team but not to other entrants.  Your processed signals should be named using the conventions used by the baseline system:  &lt;Scene ID&gt;_&lt;Listener ID&gt;_remix.flac  Any changes to the naming convention may result in your signals not being able to be scored. ","version":"Next","tagName":"h2"},{"title":"Demixing and Remixing","type":0,"sectionRef":"#","url":"/docs/learning_resources/Audio_mixing/edu_AM_demixing_remixing","content":"Demixing and Remixing The MUSDB18 dataset is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems. The website also provides information on the&quot;Compressed STEMS&quot; format used in the dataset. The Open Source Tools &amp; Data for Music Source Separation site, by Ethan Manilow, Prem Seetharaman, and Justin Salamon, provides an in-depth tutorial on source separation, and covers the main steps in source separation, including importing datasets and training models. The Deep learning for automatic mixing book, by Christian J. Steinmetz, Soumya Sai Vanka, Marco Martínez and Gary Bromham, provides a comprehensive tutorial that serves as a starting point for researchers interested on applying the last deep learning techniques on automatic music mixing. The video of the tutorial is available here.","keywords":"","version":"Next"},{"title":"Objective measures for hearing-aid audio","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech","content":"","keywords":"","version":"Next"},{"title":"HASPI and HASQI​","type":1,"pageTitle":"Objective measures for hearing-aid audio","url":"/docs/learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haspi-and-hasqi","content":" Below is a by James Kates on two objective measures used for evaluating the intelligibility and quality of hearing-aid processed speech. These metrics have much in common with the metric we're using in the first challenges for music, HAAQI [1].    Click here for synopsis Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyze the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h2"},{"title":"HAAQI (Hearing Aid Audio Quality Index)​","type":1,"pageTitle":"Objective measures for hearing-aid audio","url":"/docs/learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index","content":" HAAQI was developed by James Kates and Kathryn Arehart and is an intrusive (double-ended) metric, i.e., it requires both a processed and reference signal [1]. In the Cadenza evaluation, the HAAQI function is configured so the reference signal has a amplification applied, so all frequency bands contribute equally to its loudness. This amplification uses the NAL-R prescriptive (gain) formula [2]. This prescribes the amount of gain across frequency bands to apply to the incoming signal based on the individual pure-tone audiogram thresholds at those frequencies (in dB HL) to improve audibility. This is linear amplification and no compression.  A schematic for the rest of HAAQI is shown Figure 1. HAAQI involves an auditory model including hearing thresholds to allow for hearing loss via the audiogram and gain. HAAQI then compares the temporal fine-structure (BM Vib in Fig. 1) and the envelope of the two signals (Env) using correlation (Corr) and quantifying spectal differences. (The processes are similar to HASQI; see video above). The main difference to HASQI is the way the separate linear and non-linear terms are combined to predict perceived quality (&quot;combine terms&quot;). The metric has been shown to predict well the effects of additive noise (e.g., background noise and/or artefacts) as well as noise reduction, nonlinearities (e.g., compression) and spectral shifts. But it has not been used before to explore demixed signals, as is being done in Task 1 of the 2023 challenge.  Figure 1, Schematic for HAAQI [1].  ","version":"Next","tagName":"h2"},{"title":"User's Guide​","type":1,"pageTitle":"Objective measures for hearing-aid audio","url":"/docs/learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#users-guide","content":" For more information about the design and use of HASPI, HASQI and HAAQI, please refer to the official user's guide.  Note to users In the HAAQI Python implementation, the parameter equalisation indicates if the reference signal already includes an amplification or not. If the reference signal does not include an amplification, one have to set the parameterequalisation = 1. This will make to HAAQI to apply the NAL-R amplification. It is important to note that the processed signal must include the hearing aid amplification before is presented to HAAQI. No hearing aid amplification is applied to this signal within the HAAQI score.  ","version":"Next","tagName":"h2"},{"title":"Reference​","type":1,"pageTitle":"Objective measures for hearing-aid audio","url":"/docs/learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#reference","content":" [1] Kates, J.M. and Arehart, K.H., 2015. The hearing-aid audio quality index (HAAQI). IEEE/ACM transactions on audio, speech, and language processing, 24, pp.354-365.  [2] Byrne, D. and Dillon, H., 1986. The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid. Ear and hearing, 7(4), pp.257-265. ","version":"Next","tagName":"h2"},{"title":"Hearing Impairment","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_impairment/edu_HI_general","content":"Hearing Impairment Hearing impairment is becoming increasingly prevalent in society, which is impacted by the growing number of elderly people in the population. Latest figures from the World Health Organisation estimate that around 5% of the population – or 430 million people – have a disabling hearing impairment (defined as a hearing loss of at least 40 dB in the better hearing ear) which could benefit from rehabilitation. This figure is projected to rise to 700 million people by 2050. There are many different types and degrees of hearing impairment, and a recent infographic developed by the Aural Diversity project highlights the wide range of hearing dysfunctionthat people can experience. At a broad level, hearing impairment is classified medically by the location of impairment, as follows: Conductive – impairments in the conductive path of the outer ear and middle ear Sensorineural – impairments in the inner ear and auditory nerve Central – impairments in the central auditory nervous system). These impairments cause different types of hearing deficits, One of the most common effects of hearing disorders is a hearing threshold shift or a worsening of people’s ability to hear quiet sounds, including the subtle details of speech and music. Impairments can also cause distortions such as smearing which can increase difficulties understanding speech and appreciating music.","keywords":"","version":"Next"},{"title":"Intelligent Sound Mixing","type":0,"sectionRef":"#","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing","content":"","keywords":"","version":"Next"},{"title":"What is Intelligent Sound Mixing?​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#what-is-intelligent-sound-mixing","content":" Intelligent Sound Mixing is any automatic audio signal processing where you apply techniques towards a given goal. A crude summary of what happens in commercial music production, is to create a mix where all instruments are balanced in terms of level, their frequency ranges are sculpted (with EQ) so that each instrument occupies a portion of the audio range, and their locations in the stereo image create a pleasant and discernible audio scene for the listener. Vocals usually “ride” on top of this mix, with slightly louder levels so the lyrics are intelligible. There is often the use of artificial reverberation to create intimacy and spaciousness. A sound engineer will carry out these tasks manually using her own experience. Automatic and “intelligent” systems need to implement these principles based solely on signal parameters and some “mixing” targets set a priori [1].  ","version":"Next","tagName":"h3"},{"title":"What have you and others done on this in the past?​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#what-have-you-and-others-done-on-this-in-the-past","content":" My work with colleagues has been mainly on trying to understand what constitutes a “good” mix. We try to identify the underlying perceptual dimensions that characterise high quality audio mixes and how these correlate to signal parameters. Through experimentation with listening tests and signal feature analysis, the same 3 or 4 dimensions kept coming up as important – these are Loudness, Frequency Balance, Bass and Stereo Width [2]. We used databases containing 100s of songs and, in some cases, multiple mixes of the same song. That allowed us to understand the “normal” distribution of signal parameters [3] which may be used to guide intelligent systems in the task of mixing multitrack audio[4].  There’s been numerous efforts in creating machines that can automatically mix audio tracks [5]. For this to work, a link between signal parameters and audio mixing processes needs to be established, and that’s not trivial.  ","version":"Next","tagName":"h3"},{"title":"In task 1 headphones, the stereo track is being split into vocals, drums, bass and other. How might you approach doing an intelligent remix for someone with a hearing loss?​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#in-task-1-headphones-the-stereo-track-is-being-split-into-vocals-drums-bass-and-other-how-might-you-approach-doing-an-intelligent-remix-for-someone-with-a-hearing-loss","content":" Given that the underlying stems can be separated, the obvious approach would be to edit and remix these in some way that makes sense for the listener allowing for the hearing loss. We know that, in music productions, vocals are usually mixed about 4 to 6dB louder than the average level of the mix, with guitars as the next louder instruments. A mix targeting for someone with hearing loss might attempt to make the melody and lyrics more salient in the mix. The levels of rhythm back tracks such as drums and bass may be reduced relative to vocals and guitars, possibly with relative level differences going beyond the “standard” 6dB. Another approach might be to remix in a way to reduce perceptual masking from bass-heavy stems on other stems. This can be achieved by reducing the level of the former or applying some form of clever dynamic equalisation such as a multiband compressor. The position of each stem on the stereo image is also a factor affecting the listening experience. Positioning “important” stems away from other “masking” stems using panning in the stereo image will allow perceptual unmasking which will, potentially, help distinguish between the stems and create a more “spacious” stereo experience. This can be achieved if the underlying stems can be easily accessed through demixing.  One of the problems is that, at the moment, we don’t yet know whether this will be acceptable to someone with a hearing loss or whether the nature of the mix, as intended by a producer, will be too severely altered.  ","version":"Next","tagName":"h3"},{"title":"Does the mix has to be personalised to the hearing ability of the listener or can \"one-size-fit-all\"?​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#does-the-mix-has-to-be-personalised-to-the-hearing-ability-of-the-listener-or-can-one-size-fit-all","content":" This is where the “intelligent” part of the remixing can lead to great gains. If the system has access to the hearing loss profile, it can target a personalised mix which will be optimised for the individual. Level and frequency modifications can take into account the frequency regions where the listener has better preserved hearing. For example, if someone has severe impairment in the high frequency range, it makes sense to reshape the audio so that most relevant information (eg: vocals and melody) is presented in the mid-frequencies. For listeners with mild to moderate hearing loss, a simple balancing of levels of instruments to appear more prominent in the best hearing frequencies would make sense. For someone with severe hearing loss a complete remix where some of the stems are reduced more drastically will remove competing audio streams and present audio which is “easier” to listen to, whilst maintaining the fundamental aspects of rhythm, melody and vocals.  ","version":"Next","tagName":"h3"},{"title":"In task 2, the stereo track has to be processed so it works best in the presence of car noise. What simple mixing rules could producers apply to improve music in this case?​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#in-task-2-the-stereo-track-has-to-be-processed-so-it-works-best-in-the-presence-of-car-noise-what-simple-mixing-rules-could-producers-apply-to-improve-music-in-this-case","content":" You are making the processing done by the car stereo, so the strategy should be to adapt that signal to the car noise, as well as the profile of the hearing loss. Here, an adaptive increase of those frequencies potentially masked by the car noise, as its profile changes through speed, will ensure musical components are less masked by the noise. Moreover, as the available source signal is in stereo, another solution would be to use a “mid-side” technique (obtained by summing both left and right channels to obtain a mid channel and subtracting them to obtain a side channel). Mid and side channels can then be edited separately with various tools such as EQ, multiband compression or even just simple level adjustment, before converting them back to the stereo left and right channels. A simple increase of the mid-channel allows all the instruments which have been panned centrally in the stereo mix, e.g.: vocals, bass, snare, to be raised in terms of their relative level. A mix that may sound too spacious and lacking detail for a hearing impaired user can be made more defined and focused on vocals by reducing the level of the side channel before converting back to a left/right mix.  ","version":"Next","tagName":"h3"},{"title":"References​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#references","content":" P. Pestana and J. D Reiss, “Intelligent Audio Production Strategies Informed by Best Practices,” presented at the AES 53rd International Conference: Semantic Audio (2014 Jan.), conference paper S2-2.Wilson, AD and Fazenda, BM 2016, 'Perception of audio quality in productions of popular music' , Journal of the Audio Engineering Society, 64 (1/2) , pp. 23-34.Wilson, AD and Fazenda, BM 2016, 'Variation in multitrack mixes : analysis of low-level audio signal features' , Journal of the Audio Engineering Society, 64 (7/8) , pp. 466-473.Wilson, AD and Fazenda, BM 2017, 'Populating the mix space : parametric methods for generating multitrack audio mixtures' , Applied Sciences, 7 (12) , p. 1329.De Man, B., Reiss, J., &amp; Stables, R. (2017). Ten years of automatic mixing. Proceedings of the 3rd Workshop on Intelligent Music Production, Salford, UK, 15 September 2017  ","version":"Next","tagName":"h3"},{"title":"Further Material​","type":1,"pageTitle":"Intelligent Sound Mixing","url":"/docs/learning_resources/Audio_mixing/edu_intelligent_mixing#further-material","content":" To know more about how to implement an intelligent sound mixing, please visit our dedicated link onLearning Resources. ","version":"Next","tagName":"h3"},{"title":"Abstract","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_aid_processing/edu_AP_acoustician_HA","content":"For the Clarity Workshop in 2021, Barry M. Gibbs gave a talk on an acoustician's experience of wearing a hearing aid. This talk and the rest of the workshop are freely available online. Abstract This is a personal account of the experiences of wearing a hearing aid to control Tinnitus. It is a description by a non-expert, who however, comes from a career in engineering acoustics, both as a researcher and teacher. The Tinnitus has been of long duration (45 years), is high-frequency and broad-band in character, and is confined to the right ear. With the onset of presbycusis the Tinnitus became progressively louder, again only in the right ear. On the recommendation of the NHS, a hearing aid was fitted three years ago, which, after some adjustments, suppressed the Tinnitus quite well. A year later, a purchased digital hearing aid provided more control of both the volume and frequency content. However, the use of these devices has compromised my binaural perception, which I might be able to explain, but also speech perception and classical music appreciation, which members of the audience might be able to explain. Bio Professor Barry M. Gibbs is Honorary Professor in the Acoustics Research Unit of the University of the Liverpool School of Architecture. His main research interest is structure-borne sound in buildings and other structures. He has been awarded about 20 major grants on this and other topics, which allowed over 25 postgraduate and postdoctoral appointments. He has authored and co-authored over 290 journal and conference papers and was founding Editor of the journal Building Acoustics, now into its third decade. He is a Fellow of the Institute of Acoustics, of the Acoustical Society of America, and of the International Institute of Acoustics and Vibration. He was President of the International Institute of Acoustics and Vibration in 2002-2004. In 2015, he received the Institute of Acoustics R W Stephens Medal. He was President of the Institute of Acoustics for the period 2018-20. As Past President, he will be Conference President of Internoise 2022, to be held in Glasgow on 21-24 August 2022.","keywords":"","version":"Next"},{"title":"Synopsis","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_aid_processing/edu_AP_HL_HA_processing","content":"For the Clarity Workshop in 2021, Karolina Smeds gave a talk on hearing loss and hearing-aid signal processing. This talk and the rest of the workshop are freely available online. Synopsis Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech in noisy backgrounds is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon.","keywords":"","version":"Next"},{"title":"Hearing Aids For Music","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_impairment/edu_HI_HAFM","content":"Hearing Aids For Music Our Cadenza bid was influenced by findings on the Hearing Aids for Music (HAfM) project which explored how hearing impairments and the use of hearing aid technology affects people's music experiences through a series of clinic surveys, an interview study and a national online survey. The interdisciplinary project was led by a small team and supported by an advisory board who were leaders in music psychology, clinical audiology, computer science, auditory perception, deaf education, and hearing therapy. The HAfM team obtained a large amount of original empirical data from &gt; 1,500 hearing aid users and &gt; 100 audiology practitioners across the UK and internationally. Results showed that whilst hearing aids facilitate music perception and appreciation, participants reported experiencing a range of problems such as pitch perception problems, distortion, difficulties hearing out instruments and hearing lyrics in songs, and difficulties in live performance contexts. Results also highlighted that the digital signal processing strategies applied to hearing aids to enhance speech perception can have a negative impact on music perception. For example, feedback manager can mistakenly identify musical stimuli (e.g. pure tones on the flute) as feedback and suppress it. Similarly, noise reduction strategies can mistake music for noise and suppress the music. Compression strategies can introduce changes to the sound level that can be too slow or too quick, particularly in music with sudden and wide dynamic changes (e.g. classical). Furthermore, there was mixed evidence for the efficacy of music programs implemented by manufacturers to help with music perception. Whilst a small number of participants reported being happy with their music program, a large percentage were not able to tell the difference between the music program and their speech program, or reported that the music program made things worse. Audiologists were asked to reflect on the strategies they used to enhance music perception. Many emphasised the importance of turning off adaptive signal processing features for speech, and they also reported using changes gain, compression speed and ratios. However, there were no clear patterns of what those changes should be, as some advocated increased gain at high frequencies whereas others increased gain at low frequencies, and whilst some audiologists chose slow-acting compression for music, others emphasised fast-acting compression to enhance music listening. Overall, the studies highlight that improvements to signal processing strategies for music for hearing aid users are needed, along with a better understanding of how novel processing strategies impact on listeners' experiences and enjoyment of music. This is one of the key aims of the Cadenza project. Website: https://musicandhearingaids.org/","keywords":"","version":"Next"},{"title":"Listener Panel","type":0,"sectionRef":"#","url":"/docs/learning_resources/Perceptual_testing/edu_listener_panel","content":"Listener Panel There are various approaches to assessing the audio quality of music or other audio signals such as speech. For example, numerous perceptual models of audio quality have been developed, such as the Perceptual Evaluation of Audio Quality (PEAQ), the Hearing-Aid Speech Quality Index (HASQI), and similarly the Hearing-Aid Audio Quality Index (HAAQI). These models can use software to simulate the human ear, auditory models, hearing loss, and hearing aid processing. Commonly, a degraded audio signal is compared with a reference signal in the perceptual models, and correlations between key audio features are performed to produce an evaluation metric of audio quality (e.g., ranging from 0 to 1). The HAAQI will be used to score your systems, but it is also important to collect data regarding the subjective perceptual experiences of listeners with hearing loss. Selected systems will also be rated in terms of perceived audio quality by a panel of listeners with hearing loss; the ratings used by these participants are developed through sensory evaluation studies. This will ultimately mean that signal outputs will be judged directly by those listeners that your system has processed for, and this subjective data can further be used to improve and develop on existing perceptual models of audio quality, with these refined models to be used moving forward in further Cadenza challenges. Further details of the listening tests to be used in the first Challenge can be found on the Listening tests page.","keywords":"","version":"Next"},{"title":"Introduction to Hearing Impairment","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_impairment/edu_HI_intro","content":"Introduction to Hearing Impairment These pages start with some general information about the various types of Hearing impairment that can occur. The Hearing aids for music project, provides useful background and context to the problem of music perception for those with a hearing impairment. Finally, the page on Measuring Hearing Impairment covers how to understand and interpret Audiograms.","keywords":"","version":"Next"},{"title":"Measuring the degree of hearing impairment ","type":0,"sectionRef":"#","url":"/docs/learning_resources/Hearing_impairment/edu_measuring_HI","content":"Measuring the degree of hearing impairment Audiologists carry out pure tone audiometry, recording the quietest pure tones, usually from 250-8000 Hz, a patient can hear to identify their hearing thresholds as the standard clinical measure of hearing impairment and use those thresholds to initially set how much amplification a hearing aid should provide for that patient. Hearing aids help people with hearing impairments by amplifying sounds to make them louder and clearer. Audiograms The audiometry results are represented by an audiogram: a plot of thresholds in dB HL (hearing level; dB SPL adjusted for our differing sensitivity across frequencies) as a function of the standard pure-tone frequencies. The audiogram provides a metric of basic hearing ability, helps visualise the impairment, and can also be an indicator for certain types of hearing disorders, such as noise-induced hearing loss. One of the primary uses of the audiogram is to set the amplification of hearing aids to provide audibility for that person based on a formula (i.e., applying gain that counters that person’s hearing loss at each frequency calculated from their hearing threshold at that frequency). Occasionally audiograms are equal across frequency, or show better hearing at higher frequencies, but by-far the typical audiogram for an older adult shows worse hearing at higher frequencies, sometimes by as much as 60 dB or more (see Figure 1 for examples of real audiograms). Figure 1, Examples of real Audiograms. It is usual to calculate an average across frequency to give a single-number value for hearing loss. Many rules for choice and weighting of frequencies have been proposed or used. We use the popular “four-frequency” average: the thresholds at four mid-frequencies important for speech - 0.5, 1, 2 and 4 kHz – are averaged out to give an overall hearing threshold score, usually for the ear with better (lower) thresholds. This better-ear pure-tone average (PTA), has been classified into broad categories, as follows: Hearing loss category\tBetter ear average (dB HL)\tPerformance in Quiet and NoiseNone\t&lt; 20\tNo or very slight hearing problems. Mild\t20 - 34\tNo problems in quiet but may have real difficulty following conversation in noise. Moderate\t35 - 49\tMay have difficulty in quiet hearing a normal voice and has difficulty with conversation in noise. Moderately severe\t50 - 64\tNeeds loud speech to hear in quiet and has great difficulty in noise. Severe\t65 - 79\tIn quiet, can hear loud speech directly in one's ear, and, in noise, has very great difficulty. Profound\t80 - 94\tUnable to hear and understand even a shouted voice whether in quiet or noise. (Table from Humes 2019) In the following audiogram example, the person has a bilateral symmetrical hearing loss which would be categorised as mild at lower frequencies and severe at higher frequencies. Based on their four-frequency (500, 1000, 2000 &amp; 4000 Hz) average in their better (left) ear, this person would be categorised as having a mild hearing loss (average 36 dB HL). In general, hearing loss tends to be greater (worse) with increasing frequency for older adults and worsens with age at a rate of roughly 1 dB/year. Figure 2, An example Audiogram. The threshold values of the audiogram defines how much gain the hearing aid needs to apply, with the calculation typically done by one of a group of &quot;prescription rules&quot;, e.g. CAMFIT,NAL-NL2 orDSL. Note that the scale of an audiogram is in &quot;dB HL&quot; = &quot;dB Hearing Level&quot;. This is not dB SPL; instead, it's relative to an international standard such that 0-dB is &quot;normal hearing&quot; at every frequency. For background see Why the Audiogram Is Upside-down | The Hearing Reviewand The Quest for Audiometric Zero | The Hearing Review As noted by British Society of Audiology guidelines, 'the ability to detect pure tones in a quiet environment is not in itself a reliable indicator of hearing disability and audiometric descriptors alone shall not be used as the measure of difficulties experienced' (p. 27). Nevertheless, the descriptors provide a useful summary of an individual's hearing ability, and research shows generally consistent patterns of difficulties experienced (e.g., difficulty hearing speech in noisy environments, difficulty hearing out melodies/harmonies in music) within each category. PTA provides a quantitative measurement of hearing impairment necessary for use in the challenges. For further reading on hearing-aid compression, the following papers may be of interest: Effects of Compression on Speech Acoustics, Intelligibility, and Sound Quality - Pamela E. Souza, 2002 – a review of basic perceptual aspectsPrinciples of Digital Dynamic-Range Compression - James M. Kates, 2005 – a basic tutorial of WDRC.","keywords":"","version":"Next"},{"title":"Learning Resources","type":0,"sectionRef":"#","url":"/docs/learning_resources/learning_intro","content":"Learning Resources Welcome to the Learning Resources section of the website! Here you will find a wide range of materials from different sources, designed so that interested researchers can fill in any gaps in their knowledge, to enable them to participate in the Cadenza Challenges.","keywords":"","version":"Next"},{"title":"Analysis","type":0,"sectionRef":"#","url":"/docs/learning_resources/Perceptual_testing/edu_PT_analysis","content":"Analysis Details to appear","keywords":"","version":"Next"},{"title":"Sensory Evaluation Study ","type":0,"sectionRef":"#","url":"/docs/learning_resources/Perceptual_testing/edu_PT_sensory_Evaluation","content":"","keywords":"","version":"Next"},{"title":"References​","type":1,"pageTitle":"Sensory Evaluation Study ","url":"/docs/learning_resources/Perceptual_testing/edu_PT_sensory_Evaluation#references","content":"   Perceptual characteristics of audio, https://forcetechnology.com/-/media/force-technology-media/pdf-files/unnumbered/senselab/tech-document-perceptual-characteristics-of-audio-uk.pdf?la=en. ","version":"Next","tagName":"h2"},{"title":"Getting started with Python","type":0,"sectionRef":"#","url":"/docs/learning_resources/Software/edu_SW_python","content":"Getting started with Python If you're new to Python, there are many excellent online resources that can help you get started. One of these is the &quot;Programming with Mosh&quot; series on youtube, which has a free full course for beginners.","keywords":"","version":"Next"}],"options":{"indexBaseUrl":true,"id":"default"}}
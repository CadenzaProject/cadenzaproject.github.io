---
id: cc1_data_overview
title: Task 1 Headphones
sidebar_label: Task1 headphones
sidebar_position: 5.1
---

import useBaseUrl from '@docusaurus/useBaseUrl';

Data and baseline code can be downloaded from the [download page](../Take%20part/cc1_download) following this [timeline](../Take%20part/cc1_key_dates).

## 1 Training/development

The main training/development database is the MUSDB18-HQ. MUSDB18-HQ has 86 training songs and 14 validation songs.

You can supplement the training and validation data from the following sources:

- Bach10
- FMA-small
- MedleydB version 1 and version 2

We leave it to you to decide how to use these as part of the training and validation sets.
Note, some songs from MedleydB are already part of the training set in MUSDB18-HQ. 
For more information on augmenting and supplementing the training data, please see the [rules](../Take%20part/cc1_rules#training-and-development). 

***

## 2 Evaluation

- We will use the MUSDB18-HQ's evaluation set which is made up of 50 songs.
- You must process all of these for the complete songs.
- All the music will be used for [HAAQI evaluation](../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech).
- We will then select a random 10-second sample from some of the pieces of music for listening panel evaluation.

[//]: # (### 2.1 HAAQI + RMS Score)

[//]: # ()
[//]: # (The HAAQI algorithm only computes the score from the non-silence segments of the signal. It first prunes any leading and trail silence.)

[//]: # (Then, it filters out any segments where the energy is below to certain threshold. )

[//]: # (This works well in a traditional music quality task where one can expect that always a component of the music will be active.)

[//]: # (However, in the demixing task, it is expected that a single stem has several silent segments during the song. )

[//]: # (For example, in a song, one can expect several segments where the `vocal` is silence, this, to favour a solo-instrument segment.)

[//]: # (This brings us the problem that HAAQI is not taking into consideration any distortion error from the silence segments. )

[//]: # ()
[//]: # (To solve this problem, we are computing a combined score for this task. The algorithm goes as follows:)

[//]: # ()
[//]: # (* First, the reference and processed signals are normalised to values between -1 and 1. )

[//]: # (* Using `reference signal`, detect all silence areas greater than two seconds. )

[//]: # (  * In average, the tempo of the training music is 120 bpm, which means 2 beats-per-second and `2 seconds` in a 4/4 bar.)

[//]: # (* Concatenate non-silence and silence segments from reference and processed signal, obtaining:)

[//]: # (  * `non_silence_reference`)

[//]: # (  * `non_silence_processed`)

[//]: # (  * `silence_processed`)

[//]: # (* Compute HAAQI using the `non_silence_reference` and `non_silence_processed` signals &#40;$HAAQI_{music}$&#41;)

[//]: # (* Compute RMS of the `silence_processed` signal &#40;$RMS_{silence}$&#41;)

[//]: # ()
[//]: # (Let $Samples_{music}$ the total number of samples of non-silence segments and $Samples_{silence}$ the total number of silence samples.)

[//]: # (The final score is computed as:)

[//]: # ($$)

[//]: # (Score = \frac{Samples_{music} * HAAQI_{music} - Samples_{silence} * RMS_{silence}}{Samples_{music} + Samples_{silence}})

[//]: # ($$)


***

## 3. Data file formats and naming conventions

### 3.1 Enhanced signals

There are nine output signals generated by the baseline enhancement algorithm:

* Eight enhanced output signal corresponding to the left and right channels of each stem (i.e., as submitted by the challenge entrants)

`<Listener ID>/<Song Name>/<Listener ID>_<Song Name>_<Channel>_<Stem>.wav`

* One enhanced output signal corresponding to the final remix

`<Listener ID>/<Song Name>/<Listener ID>_<Song Name>_remix.wav`

Where:
* `Listener ID` – ID of the listener panel member, e.g., L001 to L100 for initial `pseudo-listeners`, etc.
* `Song Name` - Track name from MUSDB18, e.g, One Minute Smile.
* `Channel` - left or right channel
* `Stem `- Vocal, Bass, Drums or Others


For example, for development listener ID `L5011` and development song name `One Minute Smile_left`,
the enhanced output is: 

```text
L5011
  └───One Minute Smile
      ├───L5011_Actions - One Minute Smile_left_bass.wav
      ├───L5011_Actions - One Minute Smile_right_bass.wav
      ├───L5011_Actions - One Minute Smile_left_drums.wav
      ├───L5011_Actions - One Minute Smile_right_drums.wav
      ├───L5011_Actions - One Minute Smile_left_other.wav
      ├───L5011_Actions - One Minute Smile_right_other.wav
      ├───L5011_Actions - One Minute Smile_left_vocals.wav
      ├───L5011_Actions - One Minute Smile_right_vocals.wav
      └───L5011_Actions - One Minute Smile_remix.wav
```

### 3.2 Music metadata

Music data is store in a single JSON per file dataset with the following format.

```json
[
  {
    "Track Name":"A Classic Education - NightOwl",
    "Genre":"Singer/Songwriter",
    "Source":"MedleyDB",
    "License":"CC BY-NC-SA",
    "Split":"train"
  },
  ...
]
```

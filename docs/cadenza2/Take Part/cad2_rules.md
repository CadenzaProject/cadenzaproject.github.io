---
id: rules
title: Rules
sidebar_label: Rules
sidebar_position: 4.5
---
import useBaseUrl from '@docusaurus/useBaseUrl';
import { TwitterTimelineEmbed } from "react-twitter-embed";

## 1. Teams

- Teams must [pre-register](registration) and nominate a contact person.
- Teams can be from one or more institution.

## 2. Transparency

- Anonymous entries are allowed.
- Teams must provide a technical document of up to 2 pages describing the system/model, what data and pre-existing tools, software and models used.
- We will publish all technical documents (anonymous or otherwise).
- Teams are encouraged to make their code open source.

## 3. What information can I use?

- `Official data`: refers to the training dataset generated by using the provided `scenes.tran.json` file.
- `Data augmentation`: refers to the use of techniques like, such as randomizing the stems, flipping right and left channels, applying SpecAugmentation, pitch shifting, etc.
- `Supplememented data`: refers to the use of additional music samples. In general, you may <b>not</b> use other datasets. If you think there is a public dataset that would be a good addition to the challenge then you can propose it to the Cadenza team. If we agree, we will then make it available to all teams.

### 3.1. Training and development

- There is no limit on the amount of training data that can be generated using our tools and the provided training data sets.
- You may <b>not</b> use other datasets.
- Pretrained models
  - These can only have been developed with public databases.
  - You must <b>not</b> use pretrained models developed on private datasets.
  - You must <b>not</b> use pretrained models that included any of the CAD2 evaluation data in their training.
- All the audio or metadata can be used during training and development.
- You must **not** use the evaluation data set for training or tuning the system.
- Systems trained using any other source for data supplementation not explicitly mentioned here will **not** enter the ranking.

### 3.2. Evaluation

The only data that can be used during evaluation are:

- The audiograms giving the listener characterisation for personalisation.
- The stereo music input signals.
- For rebalancing classical music the target gains.
- For lyric intelligibility the ùõº value that gives the balance between intelligibility and quality during evaluation.

## 4. Ranking

- Objective: entries will be ranked according to average score across all signals in the evaluation dataset.
- Listening panel: entries will be ranked according to average score across all evaluation signals auditioned in the listening tests.

We will also report whether the systems are causal or non-causal in the rank order table and model size.

## 5. Prizes

We will be awarding three prizes of $300 for the best performing systems:
- Best objective score for lyric intelligibility
- Best listening panel score for lyric intelligibility
- Best objective score for rebalancing classical music

Team prizes have been made available by the generosity of the IEEE SPS.

## 6. Computational restrictions

* Systems must either be:
  *  <b>causal and low latency</b> to allow them to work with live music, or
  *  <b>non-causal</b>, for use with recorded music.
* The latency restrictions for <b>causal entries</b> are that the output from the hearing aid at time <i>t</i> must not use any information from input samples more than 5 ms into the future i.e., no information from input samples <i>>t+5 ms</i>. See this [blog post](https://claritychallenge.org/blog/Latency,%20computation%20time%20and%20real-time%20operation) from our sister Clarity project for more.
* There is no limit on computational cost, but entrants must report model size.
* Teams must start with the baseline, with the blocks that can be changed labelled *Enhancement*
* While we have provided metrics for evaluation, other metrics and approaches can be used by the teams during training.

## 7. Submitting multiple entries for a task

This will be allowed in two circumstances:
- If very different approaches are used.
- For the lyric intelligibility, you could submit a system tuned for objective evaluation and another tuned for the listening panel.

## 8. Intellectual property

The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions. Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to these.

The **submission** constitutes the audio files submitted to the challenge and the accompanying technical report.

The Challenge Organiser is the Cadenza Project team.

As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the **submission**.

Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.
"use strict";(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[3455],{50533:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>o});var i=n(74848),s=n(28453);n(73560),n(85176),n(28770);const t={id:"lyric_data",title:"Lyric Intelligibility Data",sidebar_label:"Data",sidebar_position:2.3},l=void 0,r={id:"cadenza2/Lyric Intelligibility/lyric_data",title:"Lyric Intelligibility Data",description:"- To obtain the data and baseline code, please see the download page.",source:"@site/docs/cadenza2/Lyric Intelligibility/cc2_lyrics_data.md",sourceDirName:"cadenza2/Lyric Intelligibility",slug:"/cadenza2/Lyric Intelligibility/lyric_data",permalink:"/docs/cadenza2/Lyric Intelligibility/lyric_data",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2.3,frontMatter:{id:"lyric_data",title:"Lyric Intelligibility Data",sidebar_label:"Data",sidebar_position:2.3},sidebar:"tutorialSidebar_cad2",previous:{title:"Baseline",permalink:"/docs/cadenza2/Lyric Intelligibility/lyric_baseline"},next:{title:"Task2: Rebalancing Classical",permalink:"/docs/category/task2-rebalancing-classical"}},d={},o=[{value:"A. Training, validation and evaluation data",id:"a-training-validation-and-evaluation-data",level:2},{value:"A.1 Training and validation data",id:"a1-training-and-validation-data",level:3},{value:"A.2 Evaluation (test) set",id:"a2-evaluation-test-set",level:3},{value:"B. Metadata Information",id:"b-metadata-information",level:2},{value:"B.1 Listener characteristics",id:"b1-listener-characteristics",level:3},{value:"B.2 Alpha",id:"b2-alpha",level:3},{value:"B.3 Music",id:"b3-music",level:3},{value:"B.4. Scenes",id:"b4-scenes",level:3},{value:"B.5 Scene-Listeners",id:"b5-scene-listeners",level:3},{value:"References",id:"references",level:2}];function c(e){const a={a:"a",admonition:"admonition",annotation:"annotation",br:"br",code:"code",h2:"h2",h3:"h3",li:"li",math:"math",mi:"mi",mrow:"mrow",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:["To obtain the data and baseline code, please see the ",(0,i.jsx)(a.a,{href:"../Take%20Part/download",children:"download page"}),"."]}),"\n",(0,i.jsxs)(a.li,{children:["For instructions on what data and pretrained models can be used in the challenges, please see ",(0,i.jsx)(a.a,{href:"../Take%20Part/rules",children:"rules"}),"."]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"a-training-validation-and-evaluation-data",children:"A. Training, validation and evaluation data"}),"\n",(0,i.jsx)(a.p,{children:"The training and validation data are provided at challenge launch. The evaluation data is provided closer to the submission deadline."}),"\n",(0,i.jsx)(a.h3,{id:"a1-training-and-validation-data",children:"A.1 Training and validation data"}),"\n",(0,i.jsxs)(a.p,{children:["The dataset uses the transcription extension ",(0,i.jsx)(a.a,{href:"#refs",children:"[1]"})," of the training split of MUSDB18-HQ ",(0,i.jsx)(a.a,{href:"#refs",children:"[2]"}),". This extension comprises 96 manual transcriptions of English songs by\nnon-native English speakers, totalling 366 minutes of audio."]}),"\n",(0,i.jsxs)(a.p,{children:["We permit the use of the following additional datasets in training: ",(0,i.jsx)(a.a,{href:"https://github.com/mdeff/fma",children:"FMA"}),", ",(0,i.jsx)(a.a,{href:"https://medleydb.weebly.com/",children:"MedleydB version 1 and version 2"}),", and ",(0,i.jsx)(a.a,{href:"https://music.ai/blog/news/introducing-moisesdb-the-ultimate-multitrack-dataset-for-source-separation-beyond-4-stems/",children:"MousesDB"}),". We also permit the use of pre-trained models that might have been developed using these databases."]}),"\n",(0,i.jsx)(a.p,{children:"You should not use pre-trained models that were trained on our evaluation data."}),"\n",(0,i.jsx)(a.h3,{id:"a2-evaluation-test-set",children:"A.2 Evaluation (test) set"}),"\n",(0,i.jsxs)(a.p,{children:["The evaluation dataset combines the English subset of the JamendoLyrics dataset (20 songs) ",(0,i.jsx)(a.a,{href:"#refs",children:"[3]"})," with the 46 transcribed songs from the evaluation split of the\nMUSDB18-HQ dataset. We will tell you what part of the songs are required, the required value of ",(0,i.jsxs)(a.span,{className:"katex",children:[(0,i.jsx)(a.span,{className:"katex-mathml",children:(0,i.jsx)(a.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(a.semantics,{children:[(0,i.jsx)(a.mrow,{children:(0,i.jsx)(a.mi,{children:"\u03b1"})}),(0,i.jsx)(a.annotation,{encoding:"application/x-tex",children:"\\alpha"})]})})}),(0,i.jsx)(a.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(a.span,{className:"base",children:[(0,i.jsx)(a.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(a.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"})]})})]})," and the audiograms of the listeners."]}),"\n",(0,i.jsx)(a.admonition,{type:"danger",children:(0,i.jsx)(a.p,{children:"The evaluation set should not be used for refining the system."})}),"\n",(0,i.jsx)(a.h2,{id:"b-metadata-information",children:"B. Metadata Information"}),"\n",(0,i.jsx)(a.h3,{id:"b1-listener-characteristics",children:"B.1 Listener characteristics"}),"\n",(0,i.jsxs)(a.p,{children:["We provide metadata characterising the hearing abilities of listeners so the audio signals can be personalised. This is common for both tasks, so please see ",(0,i.jsx)(a.a,{href:"../data_listener",children:"Listener Metadata"})," for more details."]}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "L0001": {\n    "name": "L0001",\n    "audiogram_cfs": [250, 500, 1000, 2000, 3000, 4000, 6000, 8000],\n    "audiogram_levels_l": [45, 45, 35, 45, 60, 65, 70, 65],\n    "audiogram_levels_r": [40, 40, 45, 45, 60, 65, 80, 80]\n  },\n  "L0002": {\n    "name": "L0002",\n    ...\n  }\n'})}),"\n",(0,i.jsx)(a.h3,{id:"b2-alpha",children:"B.2 Alpha"}),"\n",(0,i.jsx)(a.p,{children:"This gives the balanced between intelligibility and quality. It will range from 0 to 1 in 0.1 steps."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "alpha_0": 0.0,\n  "alpha_1": 0.1,\n  ...\n}\n'})}),"\n",(0,i.jsx)(a.h3,{id:"b3-music",children:"B.3 Music"}),"\n",(0,i.jsx)(a.p,{children:"This provides the information of the audio segments with its transcriptions."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "A_Classic_Education_-_NightOwl_1": {\n    "track_name": "A Classic Education - NightOwl",\n    "path": "musdb18_hq/train/audios/A Classic Education - NightOwl",\n    "segment_id": 1,\n    "start_time": 0,\n    "end_time": 8.2,\n    "confidence": "a",\n    "text": "i think you\'re right i do"\n  },\n  "A_Classic_Education_-_NightOwl_2": {\n    ...\n\n'})}),"\n",(0,i.jsx)(a.h3,{id:"b4-scenes",children:"B.4. Scenes"}),"\n",(0,i.jsx)(a.p,{children:"This files provide the combination of segment ids and alpha to use for that segment.\nThis is a randomly generated combinations."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "S10001": {\n    "segment_id": "A_Classic_Education_-_NightOwl_1",\n    "alpha": "alpha_10"\n  },\n  "S10002": {\n    "segment_id": "A_Classic_Education_-_NightOwl_2",\n    "alpha": "alpha_5"\n  },\n  "S10003": {\n    ...\n'})}),"\n",(0,i.jsx)(a.h3,{id:"b5-scene-listeners",children:"B.5 Scene-Listeners"}),"\n",(0,i.jsx)(a.p,{children:"This provides the list of listeners for each scene."}),"\n",(0,i.jsx)(a.pre,{children:(0,i.jsx)(a.code,{className:"language-json",children:'{\n  "S10001": ["L0067", "L0044"],\n  "S10002": ["L0073", "L0054"],\n  ...\n\n'})}),"\n",(0,i.jsx)(a.h2,{id:"references",children:"References"}),"\n",(0,i.jsx)("a",{name:"refs"}),"\n",(0,i.jsxs)(a.p,{children:[(0,i.jsx)(a.strong,{children:"[1]"})," Schulze-Forster, K., Doire, C.S., Richard, G. and Badeau, R., 2021. Phoneme level lyrics alignment and text-informed singing voice separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, pp.2382-2395.",(0,i.jsx)(a.br,{}),"\n",(0,i.jsx)(a.strong,{children:"[2]"})," Rafii, Z., Liutkus, A., St\xf6ter, F.-R., Mimilakis, S. I., and Bittner, R. (2019). MUSDB18-HQ - an Uncompressed Version of MUSDB18. [Dataset]. doi:10.5281/zenodo.3338373",(0,i.jsx)(a.br,{}),"\n",(0,i.jsx)(a.strong,{children:"[3]"})," Durand, S., Stoller, D. and Ewert, S., 2023, June. Contrastive learning-based audio to lyrics alignment for multiple languages. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE."]})]})}function h(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);
"use strict";(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"CLIP Dataset","metadata":{"permalink":"/blog/CLIP Dataset","source":"@site/blog/2025-06-06-CLIP-dataset.md","title":"CLIP Challenge Dataset","description":"People with hearing loss can have difficulties to clearly and effortlessly hearing lyrics. In speech technology, having metrics to automatically evaluate intelligibility has driven improvements in speech enhancement through machine learning. We want to do the same for music lyrics. We are busy working on the infrastructure for the Cadenza Lyrics Intelligibility Prediction Challenge (CLIP) that will launch at the start of September.","date":"2025-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"cadenza","permalink":"/blog/tags/cadenza"},{"inline":true,"label":"dataset","permalink":"/blog/tags/dataset"},{"inline":true,"label":"CLIP","permalink":"/blog/tags/clip"}],"readingTime":3.105,"hasTruncateMarker":false,"authors":[{"name":"Gerardo Roa","title":"Clarity Team Member","url":"https://www.sheffield.ac.uk/cs/people/research-staff/gerardo-roa-dabike","socials":{"github":"https://github.com/groadabike"},"imageURL":"https://avatars.githubusercontent.com/groadabike","key":"gerardo","page":null},{"name":"Trevor Cox","title":"Cadenza Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox","key":"trevorcox","page":null}],"frontMatter":{"slug":"CLIP Dataset","title":"CLIP Challenge Dataset","authors":["gerardo","trevorcox"],"tags":["cadenza","dataset","CLIP"]},"unlisted":false,"nextItem":{"title":"Cadenza Lyric Intelligibility Prediction Challenge (CLIP)","permalink":"/blog/Preannouncing CLIP"}},"content":"import Image from \'@theme/IdealImage\';\\nimport useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nPeople with hearing loss can have difficulties to clearly and effortlessly hearing lyrics. In speech technology, having metrics to automatically evaluate intelligibility has driven improvements in speech enhancement through machine learning. We want to do the same for music lyrics. We are busy working on the infrastructure for the Cadenza Lyrics Intelligibility Prediction Challenge (CLIP) that will launch at the start of September.\\n\\nLyric intelligibility prediction is an understudied topic with only a couple of studies available. In speech, we\'re seeing advancements driven by large Language Models, but the equivalent is not available for music. One of the reasons for this is the lack of data where audio is paired with listener scores for intelligibility. This is what our new CLIP1 database will address.\\n\\n## Poppadom Peach by Madonna\\n\\nOne of the challenges were facing is that sung lyrics are often inherently less intelligibile than speech because of the way they\'re articulated. This is why Misheard lyrics are common and [websites gather funny examples](https://www.wearelyrical.com/misheard-lyrics-from-fatherly-advice-to-flavorful-delights-madonnas-spicy-tribute/). Because we\'re going to get word correct rates from listener transcriptions, we\'re going to have to avoid cases where the singer is unclear, but that does bias the dataset. What other challenges do you foresee us having?\\n\\n## CLIP Dataset Construction\\n\\nDifficulties in constructing a suitable dataset arise because:\\n\\n* The music must be unknown to listeners, so intelligibility scores are not overestimated due to previous knowledge of the words.\\n* The music must have the proper license for processing, modifying and sharing.\\n* The music must have ground truth transcriptions. This is challenging for unknown or non-commercial songs.\\n* The scale of the data must be large for machine learning.\\n\\n### The Audio Samples\\nThese will be taken from the [FMA-Full corpus](https://github.com/mdeff/fma) **[1]**. The process of constructing this dataset:\\n\\n1. From the FMA-Full dataset, we filter out all songs with NonDerivative license, and labelled Classical, Instrumental, Experimental and International because these don\'t have vocals or are not music.\\n2. Then, we discarded all songs that lack vocals (where the RMS of the separated vocals using [HTDemucs](https://github.com/facebookresearch/demucs) **[2]** were < 0.01 or no segments were detected with [SileroVAD](https://github.com/snakers4/silero-vad).\\n3. Next, we split the remaining songs into choruses and verses using the [All-in-one](https://github.com/mir-aidj/all-in-one) **[3]** model and randomly selected one verse or chorus per song.\\n\\nUnfortunately, FMA-full does not include transcriptions. Consequently, we are using [LabelStudio](https://labelstud.io) and human annotators to segment and transcribe coherent lyrics phrases within each verse or chorus. The goal is to get up to 5000 transcribed lyrics phrases that, after a cleaning process, will result in 3500 audio segments.\\nThese 3500 segments will be augmented by processing them using a hearing loss simulator, generating three versions: as-is and two processed signals (mild and moderate loss), resulting in about 10,000 audio segments.\\n\\n<div style={{textAlign:\'center\'}}>\\n<Image img={useBaseUrl(\'../img/blog_2025-06-06/labelstudio.png\')} />\\n**Screenshot of one annotation using LabelStudio**\\n</div>\\n\\n\\n### The Intelligibility Scores\\n\\nTo score the audio samples by their intelligibility, we will use Prolific and ask people with normal hearing to transcribe the audio segments\\nafter one or two listenings. This data will be then split into train and evaluation data for our challenge.\\n\\n## Stay informed\\n\\nTo stay informed about our lyric intelligibility prediction challenge, please sign up to our [Google Group](https://groups.google.com/g/cadenza-challenge/)\\n\\n## Reference\\n\\n**[1]** Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, & Xavier Bresson. (2017). FMA: A Dataset for Music Analysis. Proceedings of the 18th International Society for Music Information Retrieval Conference, 316\u2013323. https://doi.org/10.5281/zenodo.1414728  \\n**[2]** Rouard, Simon and Massa, Francisco and Defossez, Alexandre. (2023). Hybrid Transformers for Music Source Separation  \\n**[3]** Kim, Taejun and Nam, Juhan. (2023). All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)"},{"id":"Preannouncing CLIP","metadata":{"permalink":"/blog/Preannouncing CLIP","source":"@site/blog/2025-05-01-CLIP-preannounce.md","title":"Cadenza Lyric Intelligibility Prediction Challenge (CLIP)","description":"Dear colleague,","date":"2025-05-01T00:00:00.000Z","tags":[{"inline":true,"label":"cadenza","permalink":"/blog/tags/cadenza"},{"inline":true,"label":"launch","permalink":"/blog/tags/launch"},{"inline":true,"label":"CLIP","permalink":"/blog/tags/clip"}],"readingTime":2.015,"hasTruncateMarker":true,"authors":[{"name":"Gerardo Roa","title":"Clarity Team Member","url":"https://www.sheffield.ac.uk/cs/people/research-staff/gerardo-roa-dabike","socials":{"github":"https://github.com/groadabike"},"imageURL":"https://avatars.githubusercontent.com/groadabike","key":"gerardo","page":null},{"name":"Trevor Cox","title":"Cadenza Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox","key":"trevorcox","page":null}],"frontMatter":{"slug":"Preannouncing CLIP","title":"Cadenza Lyric Intelligibility Prediction Challenge (CLIP)","authors":["gerardo","trevorcox"],"tags":["cadenza","launch","CLIP"]},"unlisted":false,"prevItem":{"title":"CLIP Challenge Dataset","permalink":"/blog/CLIP Dataset"},"nextItem":{"title":"Listener panel study update","permalink":"/blog/Listener panel study update"}},"content":"Dear colleague,\\nIt gives us great pleasure to pre-announce the next Cadenza Challenge for music processing and hearing difference. \\nThis autumn we will be running the Cadenza Lyric Intelligibility Prediction Challenge (CLIP). \\nWe\'re hoping this will be accepted as an ICASSP 2026 Grand Challenge.\\n\\n## The Challenge\\n\\nTo develop better music processing through machine learning, we need reliable way to automatically evaluate the audio. \\nFor music with lyrics, then we need a metric to evaluate the intelligibility of the sung words. \\nThe metric would come from a predictive model that takes as input audio and estimates the lyric intelligibility score that someone would achieve in a listening test.\\n\\nWith the development of large language models and foundation models for speech and music, there is great potential to significantly improve the current state-of-the-art.\\nThe music will be genres like pop and rock. Some of this will be as-is, other will be passed through a hearing loss simulator to mimic listeners with hearing loss but not wearing hearing aids.\\n\\n{/*truncate*/}\\n\\n### What will be provided\\n\\n* Training, evaluation and test tests of music.\\n* Ground truth lyric intelligibility from listening tests.\\n* Software tools including a baseline system.\\n\\n### Important Dates\\n\\nAll dates are to be intended anywhere on earth time (AoE) and are provisional.\\n\\n* 1st September 2025: Launch of challenge, release of data.\\n* 1st November 2025: Release of evaluation data and opening of submission window.\\n* 1st December 2025: Submission deadline. All entrants must have submitted their predictions plus a draft of their technical report.\\n* If this is accepted as an ICASSP Grand Challenge\\n  * 7th December 2025. Invited papers for ICASSP session\\n  * 2-8 May 2026. Session at ICASSP 2026\\n\\nWe will know whether this is an ICASSP grand challenge in July 2025.\\n\\n### Stay informed\\n\\nTo stay informed please sign up to our [Google Group](https://groups.google.com/g/cadenza-challenge/)\\n\\n### Organisers\\n\\n* Michael A. Akeroyd, University of Nottingham\\n* Scott Bannister, University of Leeds\\n* Jon P Barker, University of Sheffield\\n* Trevor J. Cox, University of Salford\\n* Bruno Fazenda, University of Salford\\n* Jennifer Firth, University of Nottingham\\n* Simone Graetzer, University of Salford\\n* Alinka Greasley, University of Leeds\\n* Gerardo Roa-Dabike, University of Sheffield\\n* Rebecca R. Vos, University of Salford\\n* William M. Whitmer, University of Nottingham\\n\\n### Funded by\\nEngineering and Physical Sciences Research Council (EPSRC), UK\\n\\n### Partners\\nRNID, Google, Logitech, Sonova, BBC R&D, Oldenburg University."},{"id":"Listener panel study update","metadata":{"permalink":"/blog/Listener panel study update","source":"@site/blog/2023-10-05-sensory_panel_update.md","title":"Listener panel study update","description":"We have two major updates from the Cadenza project.","date":"2023-10-05T00:00:00.000Z","tags":[{"inline":true,"label":"sensory panel","permalink":"/blog/tags/sensory-panel"}],"readingTime":1.055,"hasTruncateMarker":true,"authors":[{"name":"Alinka Greasley","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/286/dr-alinka-greasley","imageURL":"/img/avatar/greasley.jpeg","key":"alinka","page":null},{"name":"Scott Bannister","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/3358/dr-scott-bannister","imageURL":"/img/avatar/bannister.jpeg","key":"scott","page":null}],"frontMatter":{"slug":"Listener panel study update","title":"Listener panel study update","authors":["alinka","scott"],"tags":["sensory panel"]},"unlisted":false,"prevItem":{"title":"Cadenza Lyric Intelligibility Prediction Challenge (CLIP)","permalink":"/blog/Preannouncing CLIP"},"nextItem":{"title":"Sensory evaluation study update","permalink":"/blog/Sensory evaluation study update"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nWe have two major updates from the Cadenza project.\\n\\nFirst, we completed the sensory panel study in which our participants with hearing loss developed audio quality scales for use in our listening experiments. \\nWe presented our study findings at the International Conference of Music Perception and Cognition (ICMPC) in Tokyo, Japan in August and at the Basic Auditory Science (BAS) conference in London in September.\\n\\n{/* truncate */}\\n\\n<div style={{textAlign:\'center\'}}>\\n<figure id=\\"fig1\\">\\n<img width=\\"400\\" src={useBaseUrl(\'../img/blog_2023-10-05/apsco.png\')} />\\n</figure>\\n</div>\\n\\nSecond, the entrants in the [First Cadenza Challenge](https://cadenzachallenge.org/docs/cadenza1/cc1_intro) have now submitted systems they devised to improve the audio quality of various music samples. \\nThese samples have been tailored for the hearing profiles of our listener panel participants and are currently being prepared for our online software.\\n\\n**We will be releasing the main online listening experiment over the coming days!**\\n\\nOn 20th September, we ran a webinar for our listener panel. In the first part, we gave a short talk summarising the aims and results of the sensory panel study.\\nYou can watch the recording of this event in the next video.\\n\\n<div style={{textAlign:\'center\'}}>\\n<iframe width=\\"750\\" height=\\"500\\" src=\\"https://www.youtube.com/embed/-DW3S5yjU90?si=UMc1FjMIGovgBXsO\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n</div>\\n\\n**Our thanks to the challenge entrants for their submissions!**"},{"id":"Sensory evaluation study update","metadata":{"permalink":"/blog/Sensory evaluation study update","source":"@site/blog/2023-05-02-sensory_panel_update.md","title":"Sensory evaluation study update","description":"We are now reaching the end of our sensory evaluation study, where a panel of twelve listeners who use hearing aids","date":"2023-05-02T00:00:00.000Z","tags":[{"inline":true,"label":"sensory panel","permalink":"/blog/tags/sensory-panel"}],"readingTime":2.545,"hasTruncateMarker":true,"authors":[{"name":"Alinka Greasley","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/286/dr-alinka-greasley","imageURL":"/img/avatar/greasley.jpeg","key":"alinka","page":null},{"name":"Scott Bannister","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/3358/dr-scott-bannister","imageURL":"/img/avatar/bannister.jpeg","key":"scott","page":null},{"name":"Bruno Fazenda","title":"Cadenza Team Member","url":"https://www.salford.ac.uk/our-staff/bruno-fazenda","imageURL":"/img/avatar/fazenda.jpeg","key":"bruno","page":null},{"name":"Michael Akeroyd","title":"Cadenza Team Member","url":"https://www.nottingham.ac.uk/medicine/people/michael.akeroyd","imageURL":"/img/avatar/akeroyd.jpeg","key":"michael","page":null},{"name":"William Whitmer","title":"Cadenza Team Member","url":"https://www.nottingham.ac.uk/medicine/people/bill.whitmer","imageURL":"/img/avatar/whitmer.jpeg","key":"bill","page":null}],"frontMatter":{"slug":"Sensory evaluation study update","title":"Sensory evaluation study update","authors":["alinka","scott","bruno","michael","bill"],"tags":["sensory panel"]},"unlisted":false,"prevItem":{"title":"Listener panel study update","permalink":"/blog/Listener panel study update"},"nextItem":{"title":"Sensory evaluation study","permalink":"/blog/Sensory evaluation study"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nWe are now reaching the end of our sensory evaluation study, where a panel of twelve listeners who use hearing aids\\nhave worked across online music listening tasks and three focus groups, to reach a consensus on the important\\nperceptual attributes of music audio quality.\\n\\n{/* truncate */}\\n\\nStarting from 373 unique terms used by participants to describe music audio quality, a discussion process \\nwas completed as outlined in the image below:\\n\\n<figure id=\\"fig1\\">\\n<img width=\\"1000\\" src={useBaseUrl(\'../img/Sensory_Eval_Flowchart.jpg\')} />\\n\\n<figcaption>Figure 1, Objectives of each Focus Group session.</figcaption>\\n</figure>\\n  \\n  \\nAt this stage in the work, we wanted to share the current state of these perceptual attributes and short definitions:\\n\\n| Attribute              | Short Definition                                                                                                                                                                                                                                                                    | \\n|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  \\n| Overall Audio Quality  | Perceived audio quality results from judgments of the sound of the music, in relation to a person\u2019s expectations of how the music should ideally sound to them.                                                                                                                     |  \\n| Clarity                | Clarity refers to how well you can hear and distinguish between the different instruments and elements within the music.                                                                                                                                                            |  \\n| Harshness              | Harshness refers to an uncomfortable overemphasis of certain parts of the sound. It is most often heard in the treble resulting in a piercing, screechy or sharp sounds.                                                                                                            |\\n| Distortion             | Distortion can be caused by artefacts that shouldn\u2019t be present e.g., noise, hiss, pops or crackles. It can also be caused by the pitches sounding wrong. Music with No distortion sounds like an authentic version of what was performed.                                          |\\n| Spaciousness           | Spaciousness refers to how much you feel the music is \u2018coloured\u2019 by the performance space, and how much you can hear the reverberations and sense of space.                                                                                                                         |\\n| Treble Strength        | Treble strength refers to the perceived strength or prominence of sound qualities that are characterised by higher frequencies in the treble range, or similarly, sounds, instruments or voices with higher pitches.                                                                |\\n| Middle Strength        | Middle strength refers to the perceived strength or prominence of sound qualities that are characterised by middle frequencies found between bass and treble ranges, or similarly, sounds, instruments, or voices that pitches perceived as being between lower and higher pitches. |\\n| Bass Strength          | Bass strength refers to the perceived strength or prominence of sound qualities that are characterised by lower frequencies in the bass range, or similarly, sounds, instruments or voices with lower pitches.                                                                      |\\n| Frequency Balance      | Frequency balance refers to the perceived balance between treble (or higher pitch) and bass (or lower pitch) sounds.                                                                                                                                                                |\\n\\nOur next steps in the perceptual research will involve further data collection and testing of these attributes, \\nto understand which are the strongest predictors of overall audio quality. This is an important process as enhanced\\nmusic signals submitted by challenge entrants will be scored on these attributes by a listening panel, \\nand so the necessity of these attributes requires some initial testing.\\n\\nAs always, we would like to express our sincere gratitude and thanks to our sensory panel group for their \\nincredible commitment, motivation, and contribution to this research!"},{"id":"Sensory evaluation study","metadata":{"permalink":"/blog/Sensory evaluation study","source":"@site/blog/2023-01-30-sensory_panel.md","title":"Sensory evaluation study","description":"Here at Cadenza our sensory evaluation work to define audio quality for hearing impaired listeners is underway. We want to understand better how hearing-impaired listeners perceive audio quality in music and develop quality metrics that will subsequently be used by our listener panel to rate the systems submitted by challenge entrants. Through careful listening tasks and group discussion, the sensory panel will arrive at a consensus about important sound quality attributes and how these should be measured. You can find out more about this process on the Sensory Evaluation page.","date":"2023-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"sensory panel","permalink":"/blog/tags/sensory-panel"}],"readingTime":1.855,"hasTruncateMarker":true,"authors":[{"name":"Alinka Greasley","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/286/dr-alinka-greasley","imageURL":"/img/avatar/greasley.jpeg","key":"alinka","page":null},{"name":"Scott Bannister","title":"Cadenza Team Member","url":"https://ahc.leeds.ac.uk/music/staff/3358/dr-scott-bannister","imageURL":"/img/avatar/bannister.jpeg","key":"scott","page":null},{"name":"Bruno Fazenda","title":"Cadenza Team Member","url":"https://www.salford.ac.uk/our-staff/bruno-fazenda","imageURL":"/img/avatar/fazenda.jpeg","key":"bruno","page":null},{"name":"Michael Akeroyd","title":"Cadenza Team Member","url":"https://www.nottingham.ac.uk/medicine/people/michael.akeroyd","imageURL":"/img/avatar/akeroyd.jpeg","key":"michael","page":null},{"name":"William Whitmer","title":"Cadenza Team Member","url":"https://www.nottingham.ac.uk/medicine/people/bill.whitmer","imageURL":"/img/avatar/whitmer.jpeg","key":"bill","page":null}],"frontMatter":{"slug":"Sensory evaluation study","title":"Sensory evaluation study","authors":["alinka","scott","bruno","michael","bill"],"tags":["sensory panel"]},"unlisted":false,"prevItem":{"title":"Sensory evaluation study update","permalink":"/blog/Sensory evaluation study update"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nHere at Cadenza our sensory evaluation work to define audio quality for hearing impaired listeners is underway. We want to understand better how hearing-impaired listeners perceive audio quality in music and develop quality metrics that will subsequently be used by our listener panel to rate the systems submitted by challenge entrants. Through careful listening tasks and group discussion, the sensory panel will arrive at a consensus about important sound quality attributes and how these should be measured. You can find out more about this process on the [Sensory Evaluation page](/docs/learning_resources/Perceptual_testing/edu_PT_sensory_Evaluation).\\n\\n{/* truncate */}\\n\\nThe first task was an individual elicitation task in which we asked twelve listeners with hearing impairment to provide single-word perceptual terms to describe various music excerpts provided to them. This resulted in hundreds of unique attributes used to describe sound quality, of which 89 were used more than four times (see  [Fig. 1](#fig1)) and 87 were used two or three times (see [Fig. 2](#fig2)). This provided the starting point for the first Focus group which sought to identify the most important attributes and to identify ways of grouping the attributes into perceptual dimensions that could be captured in the quality metric.\\n\\nThe outcome of Focus group one was a spatial mapping of the ways in which the panel grouped attributes together which was used as a starting point for Focus group two (see [Fig. 3](#fig3)). The panel then discussed further the meaning and grouping of different terms to reach consensus about important dimensions. In February, we will carry out a third Focus group to arrive at final dimensions, how these can be defined, and how they can be rated or scored in the challenges.\\n\\nWe would like to thank our Sensory panel group for their ongoing motivation and commitment with this challenging task!\\n\\n\\n<figure id=\\"fig1\\">\\n<img width=\\"500\\" src={useBaseUrl(\'../img/sensory_blog_1.png\')} />\\n<figcaption>Figure 1, 89 terms that were used four or more times to describe the musical excerpts.</figcaption>\\n</figure>\\n\\n<figure id=\\"fig2\\">\\n<img width=\\"500\\" src={useBaseUrl(\'../img/sensory_blog_2.png\')} />\\n<figcaption>Figure 2, 87 terms that were used twice or three times to describe the musical excerpts.</figcaption>\\n</figure>\\n\\n<figure id=\\"fig3\\">\\n<img width=\\"500\\" src={useBaseUrl(\'../img/sensory_blog_3.jpg\')} />\\n<figcaption>Figure 3, starting point for focus group two, discussing the further meaning and grouping of different terms.</figcaption>\\n</figure>"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2023-01-03-welcome.md","title":"Welcome","description":"Welcome to the new Cadenza webpage. We will be using this page to post the latest news about our forthcoming machine learning challenges and workshops, as well as posts discussing the tools and techniques that we are using in our baseline systems.","date":"2023-01-03T00:00:00.000Z","tags":[{"inline":true,"label":"cadenza","permalink":"/blog/tags/cadenza"},{"inline":true,"label":"hello","permalink":"/blog/tags/hello"}],"readingTime":0.225,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Cadenza Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox","key":"trevorcox","page":null},{"name":"Jon Barker","title":"Clarity Team Member","url":"https://www.sheffield.ac.uk/cs/people/academic/jon-barker","email":"clarity-group@sheffield.ac.uk","socials":{"github":"https://github.com/jonbarker68"},"imageURL":"https://avatars.githubusercontent.com/jonbarker68","key":"jonbarker","page":null}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["trevorcox","jonbarker"],"tags":["cadenza","hello"]},"unlisted":false,"prevItem":{"title":"Sensory evaluation study","permalink":"/blog/Sensory evaluation study"}},"content":"Welcome to the new Cadenza webpage. We will be using this page to post the latest news about our forthcoming machine learning challenges and workshops, as well as posts discussing the tools and techniques that we are using in our baseline systems.\\n\\n{/* truncate */}"}]}}')}}]);
(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[255],{94677:(e,a,s)=>{e.exports={src:{srcSet:s.p+"assets/ideal-img/34507.378e619.640.jpg 640w,"+s.p+"assets/ideal-img/34507.0531d24.940.jpg 940w",images:[{path:s.p+"assets/ideal-img/34507.378e619.640.jpg",width:640,height:247},{path:s.p+"assets/ideal-img/34507.0531d24.940.jpg",width:940,height:363}],src:s.p+"assets/ideal-img/34507.378e619.640.jpg",toString:function(){return s.p+"assets/ideal-img/34507.378e619.640.jpg"},placeholder:void 0,width:640,height:247},preSrc:"data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAEAAoDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIH/8QAHRAAAQQDAQEAAAAAAAAAAAAAAgABAwQREiEiQf/EABUBAQEAAAAAAAAAAAAAAAAAAAAD/8QAGBEBAQADAAAAAAAAAAAAAAAAAQACITH/2gAMAwEAAhEDEQA/AMOsRBUqVXhHBS6bFnvpuq5bRxSnGLBqBOLZb4yImIJupy//2Q=="}},18634:(e,a,s)=>{"use strict";s.r(a),s.d(a,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"cadenza2/evaluation","title":"Evaluation Set","description":"1. Evaluation package","source":"@site/docs/cadenza2/cc2_evaluation.md","sourceDirName":"cadenza2","slug":"/cadenza2/evaluation","permalink":"/docs/cadenza2/evaluation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4.7,"frontMatter":{"id":"evaluation","title":"Evaluation Set","sidebar_label":"Evaluation","sidebar_position":4.7}}');var n=s(74848),t=s(28453),r=s(89341);s(24763);const l={id:"evaluation",title:"Evaluation Set",sidebar_label:"Evaluation",sidebar_position:4.7},o=void 0,d={},c=[{value:"1. Evaluation package",id:"1-evaluation-package",level:2},{value:"2. Listeners Metadata",id:"2-listeners-metadata",level:2},{value:"3. Lyrics Intelligibility",id:"3-lyrics-intelligibility",level:2},{value:"3.1 Music samples",id:"31-music-samples",level:3},{value:"3.2 Metadata",id:"32-metadata",level:3},{value:"4. Rebalancing Classical",id:"4-rebalancing-classical",level:2},{value:"4.1 Music samples",id:"41-music-samples",level:3},{value:"4.2 Metadata",id:"42-metadata",level:3},{value:"5. Submission",id:"5-submission",level:2},{value:"References",id:"references",level:2}];function h(e){const a={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.A,{img:s(94677)}),"\n",(0,n.jsx)(a.h2,{id:"1-evaluation-package",children:"1. Evaluation package"}),"\n",(0,n.jsx)(a.p,{children:"The evaluation packages contains all the audios and metadata necessary to run the evaluations.\nTo get access to the data, please download the package from the corresponding Zenodo repository."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Lyrics Intelligibility"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"cadenza_cad2_task1_eval.v1_0.tar.gz"})," ",(0,n.jsx)(a.strong,{children:"[1.5 GB]"})," - audio and metadata."]}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://zenodo.org/records/12685820",children:"Zenodo"})}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.strong,{children:"Rebalancing Classical Music"})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:"cadenza_cad2_task2_eval.v1_0.tar.gz"})," ",(0,n.jsx)(a.strong,{children:"[350 MB]"})," - audio and metadata."]}),"\n",(0,n.jsx)(a.li,{children:(0,n.jsx)(a.a,{href:"https://zenodo.org/records/14041297",children:"Zenodo"})}),"\n"]}),"\n",(0,n.jsx)(a.h2,{id:"2-listeners-metadata",children:"2. Listeners Metadata"}),"\n",(0,n.jsx)(a.p,{children:"The evaluation listeners correspond to 50 real audiograms of hearing aid users recruited by the University of Leeds, plus a no-loss audiogram with 0 dB across all frequencies to evaluate systems under no-loss conditions.\nThe distribution of these audiograms by hearing loss category is as follows: 18 mild, 16 moderate, 16 moderately severe, and 1 with no loss.\nThe same listeners will be used in both the Lyrics Intelligibility and Rebalancing Classical Music tasks."}),"\n",(0,n.jsx)(a.h2,{id:"3-lyrics-intelligibility",children:"3. Lyrics Intelligibility"}),"\n",(0,n.jsx)(a.h3,{id:"31-music-samples",children:"3.1 Music samples"}),"\n",(0,n.jsx)(a.p,{children:"The evaluation set is derived from the MUSDB18 test split and the English subset of the JamendoLyrics datasets.\nFor MUSDB18, the test split contains 50 tracks, but only 39 were included in our evaluation.\nThe remaining 11 tracks were excluded for the following reasons: five are not in English, three have vocals but no lyrics (vocal harmony or just repeating one word), one contains potentially offensive lyrics, and two feature styles that might disturb our listener panel.\nFrom the JamendoLyrics dataset, 18 of the 20 English tracks were included. One track was excluded due to offensive lyrics, and the other could not be properly segmented."}),"\n",(0,n.jsx)(a.p,{children:"The evaluation set focuses exclusively on verse sections of songs.\nChoruses were excluded because they are generally easier to understand, as they are designed to be simpler and more memorable."}),"\n",(0,n.jsxs)(a.p,{children:["To construct the evaluation set, we first employed the ",(0,n.jsx)(a.a,{href:"https://github.com/mir-aidj/all-in-one",children:"all-in-one"})," ",(0,n.jsx)(a.a,{href:"#references",children:"[1]"})," model to perform an initial verse/chorus segmentation.\nNext, we used these segments with the community edition of ",(0,n.jsx)(a.a,{href:"https://labelstud.io/",children:"Label Studio"})," to align the audio sections with their corresponding lyrics.\nThe objective was to annotate the verses while keeping the segments in a range of 10-20 seconds long.\nFor the MUSDB18 dataset, lyrics were available online for 16 tracks.\nFor the remaining tracks, we used the ",(0,n.jsx)(a.a,{href:"https://zenodo.org/records/3989267",children:"lyrics extension dataset"})," ",(0,n.jsx)(a.a,{href:"#references",children:"[2]"})," as a starting point and manually corrected the lyrics when necessary.\nFor JamendoLyrics, all 20 tracks had lyrics available online together with the ",(0,n.jsx)(a.a,{href:"https://audioshake.github.io/jam-alt/",children:"Jam-Alt dataset"})," ",(0,n.jsx)(a.a,{href:"#references",children:"[3]"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Finally, audio segments were manually reviewed and verified by five members of the Cadenza project team to ensure accuracy and consistency.\nThe final evaluation set contains 242 segments, each ranging between 10 and 20 seconds."}),"\n",(0,n.jsxs)(a.p,{children:["For the quality score (HAAQI), the reference signal was set as a remix of the original track with the vocals boosted by +2 dB.\nTo achieve this, we generated a Musdb18-like version of the JamendoLyrics dataset using the\n",(0,n.jsx)(a.a,{href:"https://pytorch.org/audio/main/generated/torchaudio.pipelines.HDEMUCS_HIGH_MUSDB_PLUS.html",children:"HDEMUCS_HIGH_MUSDB_PLUS"})," pretrained model from torchaudio."]}),"\n",(0,n.jsx)(a.h3,{id:"32-metadata",children:"3.2 Metadata"}),"\n",(0,n.jsxs)(a.p,{children:["The evaluation metadata has the same structure as the training and validation sets described ",(0,n.jsx)(a.a,{href:"Lyric%20Intelligibility/lyric_data",children:"here"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["There are 242 ",(0,n.jsx)(a.strong,{children:"scenes"}),", each corresponding to an audio segment paired with one of the 11 ",(0,n.jsx)(a.strong,{children:"alphas"}),".\nThe ",(0,n.jsx)(a.strong,{children:"scene-listener"})," file pairs each scene with all 51 evaluation listeners, resulting in a total 12,342 processed signals to submit."]}),"\n",(0,n.jsx)(a.h2,{id:"4-rebalancing-classical",children:"4. Rebalancing Classical"}),"\n",(0,n.jsx)(a.h3,{id:"41-music-samples",children:"4.1 Music samples"}),"\n",(0,n.jsxs)(a.p,{children:["The evaluation set is based on 8 tracks from the BACH10 and 13 tracks from the URMP datasets.\nCompositions that intersect with the training sets (EnsembleSet and CadenzaWoodwind) were excluded from the evaluation.\nNote that the ",(0,n.jsx)(a.strong,{children:"Real Data for Tuning"})," tracks are also part of the BACH10 and URMP datasets, but they are not included in the evaluation.\nFor more details on the evaluation music, please refer to ",(0,n.jsx)(a.a,{href:"Rebalancing%20Classical/rebalancing_data#a3-evaluation-test-set",children:"here"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"Each track in the evaluation set were split into consecutive 15-second segments with no overlap, resulting in 87 audio segments.\nThe segments were then paired with 4 randomly selected gains and each segment-gain pair will be processed for 21 (out of 51) listeners.\nThe 21 listeners were randomly selected keeping the same distribution of listeners with moderate, moderately severe, severe and no loss."}),"\n",(0,n.jsx)(a.h3,{id:"42-metadata",children:"4.2 Metadata"}),"\n",(0,n.jsxs)(a.p,{children:["The metadata has the same structure as the training and validation sets described ",(0,n.jsx)(a.a,{href:"http://localhost:3000/docs/cadenza2/Rebalancing%20Classical/rebalancing_data",children:"here"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["The 87 audio segments are pair with 4 randomly selected ",(0,n.jsx)(a.strong,{children:"gains"}),", resulting in 348 ",(0,n.jsx)(a.strong,{children:"scenes"}),".\nEach scene was paired with 21 listeners, randomly selected to maintain the same distributed by loss category: 8 mild, 6 moderately severe, 6 severe and 1 no loss, resulting in 3,708 ",(0,n.jsx)(a.strong,{children:"scene-listeners"})," pairs."]}),"\n",(0,n.jsx)(a.h2,{id:"5-submission",children:"5. Submission"}),"\n",(0,n.jsx)(a.p,{children:"The submission for the challenge must include all the processed audio signals, along with a technical report summarising the approach."}),"\n",(0,n.jsx)(a.p,{children:"The processed signals should be compressed into a single ZIP package and submitted to a repository, which will be shared with participants in due time.\nIf participant have issues packaging or submitting a large file, they may divide the data into\nseveral smaller packages, each no smaller than 2GB."}),"\n",(0,n.jsx)(a.p,{children:"The total size of the submission package for:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Lyrics intelligibility: ",(0,n.jsx)(a.strong,{children:"XX"})," GB."]}),"\n",(0,n.jsxs)(a.li,{children:["Rebalancing Classical Music: ",(0,n.jsx)(a.strong,{children:"XX"})," GB"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Participants are allowed to submit several systems. More information on how to submit several systems will be provided along with the Team ID."}),"\n",(0,n.jsx)(a.h2,{id:"references",children:"References"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"[1]"})," Kim, Taejun and Nam, Juhan (2023), All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio, IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\n",(0,n.jsx)(a.strong,{children:"[2]"})," Schulze-Forster, K., Doire, C.S., Richard, G. and Badeau, R., 2021. Phoneme level lyrics alignment and text-informed singing voice separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, pp.2382-2395.",(0,n.jsx)(a.br,{}),"\n",(0,n.jsx)(a.strong,{children:"[3]"})," C\xedfka, O., Dimitriou, C., Wang, C. I., Schreiber, H., Miner, L., & St\xf6ter, F. R. (2023). Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark. arXiv preprint arXiv:2311.13987."]})]})}function u(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}}}]);
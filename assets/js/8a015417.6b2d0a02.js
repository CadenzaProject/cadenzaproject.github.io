"use strict";(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[944],{53347:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>h,contentTitle:()=>c,default:()=>g,frontMatter:()=>o,metadata:()=>n,toc:()=>d});var n=i(74359),a=i(74848),s=i(28453),r=i(66754),l=i(41306);const o={slug:"CLIP Dataset",title:"CLIP Challenge Dataset",authors:["gerardo","trevorcox"],tags:["cadenza","dataset","CLIP"]},c=void 0,h={authorsImageUrls:[void 0,void 0]},d=[{value:"Poppadom Peach by Madonna",id:"poppadom-peach-by-madonna",level:2},{value:"CLIP Dataset Construction",id:"clip-dataset-construction",level:2},{value:"The Audio Samples",id:"the-audio-samples",level:3},{value:"The Intelligibility Scores",id:"the-intelligibility-scores",level:3},{value:"Stay informed",id:"stay-informed",level:2},{value:"Reference",id:"reference",level:2}];function u(e){const t={a:"a",br:"br",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"People with hearing loss can have difficulties to clearly and effortlessly hearing lyrics. In speech technology, having metrics to automatically evaluate intelligibility has driven improvements in speech enhancement through machine learning. We want to do the same for music lyrics. We are busy working on the infrastructure for the Cadenza Lyrics Intelligibility Prediction Challenge (CLIP) that will launch at the start of September."}),"\n",(0,a.jsx)(t.p,{children:"Lyric intelligibility prediction is an understudied topic with only a couple of studies available. In speech, we're seeing advancements driven by large Language Models, but the equivalent is not available for music. One of the reasons for this is the lack of data where audio is paired with listener scores for intelligibility. This is what our new CLIP1 database will address."}),"\n",(0,a.jsx)(t.h2,{id:"poppadom-peach-by-madonna",children:"Poppadom Peach by Madonna"}),"\n",(0,a.jsxs)(t.p,{children:["One of the challenges were facing is that sung lyrics are often inherently less intelligibile than speech because of the way they're articulated. This is why Misheard lyrics are common and ",(0,a.jsx)(t.a,{href:"https://www.wearelyrical.com/misheard-lyrics-from-fatherly-advice-to-flavorful-delights-madonnas-spicy-tribute/",children:"websites gather funny examples"}),". Because we're going to get word correct rates from listener transcriptions, we're going to have to avoid cases where the singer is unclear, but that does bias the dataset. What other challenges do you foresee us having?"]}),"\n",(0,a.jsx)(t.h2,{id:"clip-dataset-construction",children:"CLIP Dataset Construction"}),"\n",(0,a.jsx)(t.p,{children:"Difficulties in constructing a suitable dataset arise because:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"The music must be unknown to listeners, so intelligibility scores are not overestimated due to previous knowledge of the words."}),"\n",(0,a.jsx)(t.li,{children:"The music must have the proper license for processing, modifying and sharing."}),"\n",(0,a.jsx)(t.li,{children:"The music must have ground truth transcriptions. This is challenging for unknown or non-commercial songs."}),"\n",(0,a.jsx)(t.li,{children:"The scale of the data must be large for machine learning."}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"the-audio-samples",children:"The Audio Samples"}),"\n",(0,a.jsxs)(t.p,{children:["These will be taken from the ",(0,a.jsx)(t.a,{href:"https://github.com/mdeff/fma",children:"FMA-Full corpus"})," ",(0,a.jsx)(t.strong,{children:"[1]"}),". The process of constructing this dataset:"]}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsx)(t.li,{children:"From the FMA-Full dataset, we filter out all songs with NonDerivative license, and labelled Classical, Instrumental, Experimental and International because these don't have vocals or are not music."}),"\n",(0,a.jsxs)(t.li,{children:["Then, we discarded all songs that lack vocals (where the RMS of the separated vocals using ",(0,a.jsx)(t.a,{href:"https://github.com/facebookresearch/demucs",children:"HTDemucs"})," ",(0,a.jsx)(t.strong,{children:"[2]"})," were < 0.01 or no segments were detected with ",(0,a.jsx)(t.a,{href:"https://github.com/snakers4/silero-vad",children:"SileroVAD"}),"."]}),"\n",(0,a.jsxs)(t.li,{children:["Next, we split the remaining songs into choruses and verses using the ",(0,a.jsx)(t.a,{href:"https://github.com/mir-aidj/all-in-one",children:"All-in-one"})," ",(0,a.jsx)(t.strong,{children:"[3]"})," model and randomly selected one verse or chorus per song."]}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["Unfortunately, FMA-full does not include transcriptions. Consequently, we are using ",(0,a.jsx)(t.a,{href:"https://labelstud.io",children:"LabelStudio"})," and human annotators to segment and transcribe coherent lyrics phrases within each verse or chorus. The goal is to get up to 5000 transcribed lyrics phrases that, after a cleaning process, will result in 3500 audio segments.\nThese 3500 segments will be augmented by processing them using a hearing loss simulator, generating three versions: as-is and two processed signals (mild and moderate loss), resulting in about 10,000 audio segments."]}),"\n",(0,a.jsxs)("div",{style:{textAlign:"center"},children:[(0,a.jsx)(r.A,{img:(0,l.Ay)("../img/blog_2025-06-06/labelstudio.png")}),(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"Screenshot of one annotation using LabelStudio"})})]}),"\n",(0,a.jsx)(t.h3,{id:"the-intelligibility-scores",children:"The Intelligibility Scores"}),"\n",(0,a.jsx)(t.p,{children:"To score the audio samples by their intelligibility, we will use Prolific and ask people with normal hearing to transcribe the audio segments\nafter one or two listenings. This data will be then split into train and evaluation data for our challenge."}),"\n",(0,a.jsx)(t.h2,{id:"stay-informed",children:"Stay informed"}),"\n",(0,a.jsxs)(t.p,{children:["To stay informed about our lyric intelligibility prediction challenge, please sign up to our ",(0,a.jsx)(t.a,{href:"https://groups.google.com/g/cadenza-challenge/",children:"Google Group"})]}),"\n",(0,a.jsx)(t.h2,{id:"reference",children:"Reference"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"[1]"})," Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, & Xavier Bresson. (2017). FMA: A Dataset for Music Analysis. Proceedings of the 18th International Society for Music Information Retrieval Conference, 316\u2013323. ",(0,a.jsx)(t.a,{href:"https://doi.org/10.5281/zenodo.1414728",children:"https://doi.org/10.5281/zenodo.1414728"}),(0,a.jsx)(t.br,{}),"\n",(0,a.jsx)(t.strong,{children:"[2]"})," Rouard, Simon and Massa, Francisco and Defossez, Alexandre. (2023). Hybrid Transformers for Music Source Separation",(0,a.jsx)(t.br,{}),"\n",(0,a.jsx)(t.strong,{children:"[3]"})," Kim, Taejun and Nam, Juhan. (2023). All-In-One Metrical And Functional Structure Analysis With Neighborhood Attentions on Demixed Audio. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)"]})]})}function g(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},74359:e=>{e.exports=JSON.parse('{"permalink":"/blog/CLIP Dataset","source":"@site/blog/2025-06-06-CLIP-dataset.md","title":"CLIP Challenge Dataset","description":"People with hearing loss can have difficulties to clearly and effortlessly hearing lyrics. In speech technology, having metrics to automatically evaluate intelligibility has driven improvements in speech enhancement through machine learning. We want to do the same for music lyrics. We are busy working on the infrastructure for the Cadenza Lyrics Intelligibility Prediction Challenge (CLIP) that will launch at the start of September.","date":"2025-06-06T00:00:00.000Z","tags":[{"inline":true,"label":"cadenza","permalink":"/blog/tags/cadenza"},{"inline":true,"label":"dataset","permalink":"/blog/tags/dataset"},{"inline":true,"label":"CLIP","permalink":"/blog/tags/clip"}],"readingTime":3.105,"hasTruncateMarker":false,"authors":[{"name":"Gerardo Roa","title":"Clarity Team Member","url":"https://www.sheffield.ac.uk/cs/people/research-staff/gerardo-roa-dabike","socials":{"github":"https://github.com/groadabike"},"imageURL":"https://avatars.githubusercontent.com/groadabike","key":"gerardo","page":null},{"name":"Trevor Cox","title":"Cadenza Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox","key":"trevorcox","page":null}],"frontMatter":{"slug":"CLIP Dataset","title":"CLIP Challenge Dataset","authors":["gerardo","trevorcox"],"tags":["cadenza","dataset","CLIP"]},"unlisted":false,"nextItem":{"title":"Cadenza Lyric Intelligibility Prediction Challenge (CLIP)","permalink":"/blog/Preannouncing CLIP"}}')}}]);
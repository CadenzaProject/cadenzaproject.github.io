"use strict";(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[3886],{53088:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var s=i(85893),a=i(11151),t=i(79524);const r={id:"cc1_baseline",title:"Baseline",sidebar_label:"Baseline",sidebar_position:6.1},o=void 0,l={id:"cadenza1/Software/cc1_baseline",title:"Baseline",description:"Challenge entrants have a fully functioning baseline system to build from.",source:"@site/docs/cadenza1/Software/cc1_baseline.mdx",sourceDirName:"cadenza1/Software",slug:"/cadenza1/Software/cc1_baseline",permalink:"/docs/cadenza1/Software/cc1_baseline",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6.1,frontMatter:{id:"cc1_baseline",title:"Baseline",sidebar_label:"Baseline",sidebar_position:6.1},sidebar:"tutorialSidebar_cad1",previous:{title:"Software",permalink:"/docs/category/software"},next:{title:"Core Software",permalink:"/docs/cadenza1/Software/cc1_core_software"}},h={},d=[{value:"1. Task 1: Headphones",id:"1-task-1-headphones",level:2},{value:"1.1 Baseline system",id:"11-baseline-system",level:3},{value:"2. Task 2: Car",id:"2-task-2-car",level:2},{value:"2.1 Baseline system",id:"21-baseline-system",level:3},{value:"2.1.1 Loudness consideration in enhancement",id:"211-loudness-consideration-in-enhancement",level:4},{value:"2.1.1 Loudness consideration in evaluation",id:"211-loudness-consideration-in-evaluation",level:4},{value:"3. References",id:"3-references",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Challenge entrants have a fully functioning baseline system to build from."}),"\n",(0,s.jsx)(n.h2,{id:"1-task-1-headphones",children:"1. Task 1: Headphones"}),"\n",(0,s.jsxs)("figure",{id:"fig1",children:[(0,s.jsx)("img",{width:"800",src:(0,t.Z)("../img/headphone_simple_v2.png")}),(0,s.jsx)("figcaption",{children:"Figure 1, The baseline for the car listening scenario (Task 2), not all connections shown."})]}),"\n",(0,s.jsxs)(n.p,{children:["The music databases (blue box) are available in object based format, allowing us to create both stereo input to the\ndemixer (grey line) and music in VDBO (vocal, drums, bass, other) format (red lines).\nThese later signals form the reference VDBO signals that are needed for the objective evaluation using ",(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"})," (Hearing Aid Audio Quality Index) [1].\nThe demixing part is therefore a variant on a standard demixing challenge, except the quality of\nthe separation is evaluated using HAAQI rather than a measure like SDR (Signal to Distortion Ratio)."]}),"\n",(0,s.jsx)(n.p,{children:"The audiogram metadata allows the music enhancement (e.g. demixing/remixing) to be individualised to the hearing\nability of the listener (dash grey lines)."}),"\n",(0,s.jsx)(n.p,{children:"The VDBO signals are remixed to give the stereo output from the headphones. It would be possible to use a simple\nremixer that uses the levels stated in the original music's metadata. But there is freedom here to experiment with\nchanging the remixing to improve the audio quality for the listeners."}),"\n",(0,s.jsx)(n.h3,{id:"11-baseline-system",children:"1.1 Baseline system"}),"\n",(0,s.jsx)(n.p,{children:"We are presenting two baseline systems using two out-of-the-box systems trained exclusively on the MUSDB18 training data."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["The time-domain system ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2111.03600",children:"Hybrid Demucs music source separation model"}),". This model is publicly available in the\n",(0,s.jsx)(n.a,{href:"https://pytorch.org/audio/main/generated/torchaudio.pipelines.HDEMUCS_HIGH_MUSDB.html#torchaudio.pipelines.HDEMUCS_HIGH_MUSDB",children:"TorchAudio library"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The spectrogram-based system ",(0,s.jsx)(n.a,{href:"https://github.com/sigsep/open-unmix-pytorch",children:"Open-Unmix model"}),". This model is publicly available through ",(0,s.jsx)(n.code,{children:"torch.hub"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The output of the demixing is 8 stems corresponding to the left and right channel for each source,\ni.e., left vocal, right vocal, left bass, right bass, and so on."}),"\n",(0,s.jsx)(n.p,{children:"Then, a NAL-R amplification and compression is applied to each stem using the personalized audiogram of each listener.\nNAL-R is a prescription formulation for fitting simple hearing aids. This then creates the output stems."}),"\n",(0,s.jsxs)(n.p,{children:["These output stems are then evaluated by computing the mean ",(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"})," score of each stem."]}),"\n",(0,s.jsx)(n.p,{children:"For the remixing procedure, the baseline simply does a linear addition of the output stems."}),"\n",(0,s.jsx)(n.p,{children:'Your challenge is to improve the demixing and remixing blocks in the "enhancement" box. The rest of the baseline is fixed and should not be changed.'}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-task-2-car",children:"2. Task 2: Car"}),"\n",(0,s.jsxs)("figure",{id:"fig1",children:[(0,s.jsx)("img",{width:"800",src:(0,t.Z)("/img/car_simple_v2.png")}),(0,s.jsx)("figcaption",{children:"Figure 1, The baseline for the headphone listening scenario (Task 1)."})]}),"\n",(0,s.jsxs)(n.p,{children:["The music database (blue box) provide samples as input to the car stereo and also reference left and right\nstereo signals for evaluating using ",(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"}),". Your task is to process the music taking into account the listener\naudiogram and also the car noise. You have access to the car metadata (e.g. speed), which will determine\nthe power spectrum of the car noise."]}),"\n",(0,s.jsxs)(n.p,{children:["The evaluation starts by predicting the signals at the microphones of the hearing aids.\nThe effect of the 'room' acoustics is simulated by applying Binaural Room Impulse Responses\n(taken from the ",(0,s.jsx)(n.a,{href:"https://www.phon.ucl.ac.uk/resource/ebrird/",children:"eBrIRD"})," database).\nThe car noise comes from a simulator we provide."]}),"\n",(0,s.jsxs)(n.p,{children:["After the car noise and acoustic simulation, the signals are then processed by a simple hearing aid.\nThis then provides left and right signals that can be used for evaluation either by ",(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"})," or the listening panel."]}),"\n",(0,s.jsx)(n.p,{children:"Your challenge is to improve the car stereo ('Enhancement'). The 'Evaluation' is fixed and should not be changed."}),"\n",(0,s.jsx)(n.h3,{id:"21-baseline-system",children:"2.1 Baseline system"}),"\n",(0,s.jsx)(n.p,{children:"In enhancement, the baseline system simply normalises the signal accordingly to the average audiogram of the ear with\ngreater hearing loss. The system attenuates the signal and saves it at 16 bit resolution."}),"\n",(0,s.jsx)(n.p,{children:"In the evaluation stage, the car noise and car acoustics characteristics (HRTFs) are added to the enhanced signal.\nThis signal is then passed to a simple hearing aid (HA) composed of a NAL-R amplification."}),"\n",(0,s.jsx)(n.p,{children:"The output of the HA is use for HAAQI and listening panel evaluation."}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"An important issue to be aware of is the possibility of the hearing aid producing signals with an amplitude that causes clipping.\nWhen this happens, the system prints warning messages in the log file, including the track path and the number of clipped samples."})}),"\n",(0,s.jsxs)(n.p,{children:["More details can be found in the ",(0,s.jsx)(n.a,{href:"cc1_core_software",children:"core software"})," page."]}),"\n",(0,s.jsx)(n.h4,{id:"211-loudness-consideration-in-enhancement",children:"2.1.1 Loudness consideration in enhancement"}),"\n",(0,s.jsx)(n.p,{children:"The baseline enhancement system simply sets an appropriate level for the original music based on the music and audiogram of the listener. It isn't perfect and some samples still clip!"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Computes the level in dB LUFS (loudness units relative to full scale) of the original song, i.e., the song from the dataset."}),"\n",(0,s.jsxs)(n.li,{children:["Computes the average hearing loss (HL) for each ear.  Refer to ",(0,s.jsx)(n.a,{href:"docs/learning_resources/Hearing_impairment/edu_measuring_HI",children:"measuring hearing impairment"}),"\nresource to understand how this average is computed."]}),"\n",(0,s.jsxs)(n.li,{children:["Set a temporal target level according maximum average HL.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"If average is equal or greater than 50, set the temporal target level to -19 dB LUFS. Then, for every 5dB LUFS over 50 dB,\nreduce the level in an additional 1 dB LUFS."}),"\n",(0,s.jsx)(n.li,{children:"Otherwise, set the temporal target level at -14 dB LUFS."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"The final target level for normalisation is set as the minimum between the temporal target and the original song levels."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Example:\nAll levels are in dB LUFS"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Song"}),(0,s.jsx)(n.th,{children:"Max average HL"}),(0,s.jsx)(n.th,{children:"Original Level"}),(0,s.jsx)(n.th,{children:"Temp Target Level"}),(0,s.jsx)(n.th,{children:"Final Level"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"35"}),(0,s.jsx)(n.td,{children:"-13"}),(0,s.jsx)(n.td,{children:"-14"}),(0,s.jsx)(n.td,{children:"-14"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2"}),(0,s.jsx)(n.td,{children:"42"}),(0,s.jsx)(n.td,{children:"-15"}),(0,s.jsx)(n.td,{children:"-14"}),(0,s.jsx)(n.td,{children:"-15"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"3"}),(0,s.jsx)(n.td,{children:"50"}),(0,s.jsx)(n.td,{children:"-15"}),(0,s.jsx)(n.td,{children:"-19"}),(0,s.jsx)(n.td,{children:"-19"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"4"}),(0,s.jsx)(n.td,{children:"60"}),(0,s.jsx)(n.td,{children:"-13"}),(0,s.jsx)(n.td,{children:"-19"}),(0,s.jsx)(n.td,{children:"-21"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["The levels of -14 and -19 dB LUFS were set according to ",(0,s.jsx)(n.a,{href:"https://artists.spotify.com/en/help/article/loudness-normalization",children:"Spotify standard"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"211-loudness-consideration-in-evaluation",children:"2.1.1 Loudness consideration in evaluation"}),"\n",(0,s.jsxs)(n.p,{children:['For the signal passing through the hearing aid. The enhanced signal ("processed music in above diagram") has the car acoustic characteristics and car noise added (See ',(0,s.jsx)(n.a,{href:"cc1_core_software#21-car-acoustics-model",children:"here"}),").\nThis then goes into the hearing aid, which is a simple linear amplifier. If any sample values exceed +1 (or are below -1) on the output of the hearing aid, they are then set to +1 (or -1). Consequently, setting the levels of the enhancement signals to prevent clipping at this point is vital."]}),"\n",(0,s.jsxs)(n.p,{children:['For the reference signal ("clean music") passing to ',(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"}),", a simple stereo set up in a dead room is simulated:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Anechoic HRTFs are applied to the reference signal to simulate the stereo set-up. This then gives the music at the listener ear canal."}),"\n",(0,s.jsxs)(n.li,{children:["The reference signal is then normalise to the lower loudness between the enhanced signal after adding the car acoustics and -14 dB LUFS.\nThis becomes the reference signal for ",(0,s.jsx)(n.a,{href:"../../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"HAAQI"})," evaluation."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-references",children:"3. References"}),"\n",(0,s.jsx)("a",{name:"refs"}),"\n",(0,s.jsx)(n.p,{children:"[1] Kates, J.M.  and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858."})]})}function u(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>o,a:()=>r});var s=i(67294);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);
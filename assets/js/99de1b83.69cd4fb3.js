"use strict";(self.webpackChunkclarity_cec_1=self.webpackChunkclarity_cec_1||[]).push([[8690],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=p(a),d=r,h=u["".concat(l,".").concat(d)]||u[d]||m[d]||i;return a?n.createElement(h,s(s({ref:t},c),{},{components:a})):n.createElement(h,s({ref:t},c))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,s=new Array(i);s[0]=d;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[u]="string"==typeof e?e:r,s[1]=o;for(var p=2;p<i;p++)s[p]=a[p];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6270:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var n=a(7462),r=(a(7294),a(3905)),i=(a(8209),a(4996));const s={id:"baseline",title:"Baseline",sidebar_label:"Baseline",sidebar_position:7.1},o=void 0,l={unversionedId:"icassp_2024/software/baseline",id:"icassp_2024/software/baseline",title:"Baseline",description:"Challenge entrants are supplied with a fully functioning baseline system.",source:"@site/docs/icassp_2024/software/icassp2024_baseline.mdx",sourceDirName:"icassp_2024/software",slug:"/icassp_2024/software/baseline",permalink:"/docs/icassp_2024/software/baseline",draft:!1,tags:[],version:"current",sidebarPosition:7.1,frontMatter:{id:"baseline",title:"Baseline",sidebar_label:"Baseline",sidebar_position:7.1},sidebar:"tutorialSidebar_icassp2024",previous:{title:"Software",permalink:"/docs/category/software-1"},next:{title:"Core Software",permalink:"/docs/icassp_2024/software/core_software"}},p={},c=[{value:"The <strong>Pre-Process</strong> blocks",id:"the-pre-process-blocks",level:3},{value:"The <strong>Enhancement</strong> block",id:"the-enhancement-block",level:3},{value:"The <strong>Evaluation</strong> block",id:"the-evaluation-block",level:3},{value:"Baseline Scores",id:"baseline-scores",level:3},{value:"References",id:"references",level:2}],u={toc:c},m="wrapper";function d(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Challenge entrants are supplied with a fully functioning baseline system."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"#fig2"},"Figure 2")," shows a detailed schematic of the baseline system:"),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)("figure",{id:"fig2"},(0,r.kt)("img",{width:"900",src:(0,i.Z)("/img/icassp_2024/task_diagram_hrtf.png")}),(0,r.kt)("figcaption",null,"Figure 2, Detailed schematic of the baseline system."))),(0,r.kt)("p",null,"where:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Green boxes represent audio signals."),(0,r.kt)("li",{parentName:"ul"},"Blue boxes represent operations applied to the audio signals."),(0,r.kt)("li",{parentName:"ul"},"Blue database box is the anechoic HRTF dataset (audio signals)."),(0,r.kt)("li",{parentName:"ul"},"Red database box is the listener characteristics dataset (metadata information)."),(0,r.kt)("li",{parentName:"ul"},"Yellow database box is the gains dataset (metadata information)."),(0,r.kt)("li",{parentName:"ul"},"White 'Weight and sum' box is the downmix operation."),(0,r.kt)("li",{parentName:"ul"},"Solid lines are signals transferred from one state of process to the next."),(0,r.kt)("li",{parentName:"ul"},"Dashed lines are metadata information.")),(0,r.kt)("h3",{id:"the-pre-process-blocks"},"The ",(0,r.kt)("strong",{parentName:"h3"},"Pre-Process")," blocks"),(0,r.kt)("p",null,'The system starts by applying HRTFs to the music of MUSDB18-HQ dataset, simulating the music as it is picked up by the hearing aids microphones.\nThis stage is illustrated by the "pre-process enhancement" and "pre-process evaluation" boxes. However, in practice both boxes\ncorrespond to the output of the ',(0,r.kt)("inlineCode",{parentName:"p"},"generate_at_mic_musdb18.py")," script."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"First, it takes the ",(0,r.kt)("inlineCode",{parentName:"li"},"Scene")," details:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"MUSDB18-HQ music (mixture, vocal, drums, bass, other)."),(0,r.kt)("li",{parentName:"ul"},"Subject head and loudspeaker-position (HRTFs)."))),(0,r.kt)("li",{parentName:"ul"},"It applies the HRTFs to the left and right side of all signals (mixture and VDBO components) [",(0,r.kt)("a",{parentName:"li",href:"#fig2"},"Figure 2"),"]"),(0,r.kt)("li",{parentName:"ul"},'The mixture with HRTF applied corresponds to the output of the "pre-process enhancement" block.'),(0,r.kt)("li",{parentName:"ul"},'The VDBO signals with HRTF applied correspond to the output of the "pre-process evaluation" block.')),(0,r.kt)("div",{style:{textAlign:"center"}},(0,r.kt)("figure",{id:"fig2"},(0,r.kt)("img",{width:"500",src:(0,i.Z)("/img/icassp_2024/cross-talk-hrtf.png")}),(0,r.kt)("figcaption",null,"Figure 3, The scenario."))),(0,r.kt)("h3",{id:"the-enhancement-block"},"The ",(0,r.kt)("strong",{parentName:"h3"},"Enhancement")," block"),(0,r.kt)("p",null,"The enhancement takes a mixture signal as it is picked up by the hearing aids microphones and attempts to output a personalized rebalanced stereo rendition of the music."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},'First, it takes stereo tracks ("mixture at the hearing aid mics") and demixes them into their VDBO (vocal, drums, bass and other) representation. This is done by using an out-of-the-box audio source separation system.'),(0,r.kt)("li",{parentName:"ul"},"Then, using the gains provided, the music is downmixed to stereo after changing the level of the different elements of the music."),(0,r.kt)("li",{parentName:"ul"},"Next, the downmixed signal is normalised to match the LUFS level of the input mixture."),(0,r.kt)("li",{parentName:"ul"},"NAL-R amplification is applied to the normalised downmixed signal, allowing for a personalised amplification for the listener using a standard hearing aid algorithm."),(0,r.kt)("li",{parentName:"ul"},"This amplified signal is the output of the system: ",(0,r.kt)("inlineCode",{parentName:"li"},"Processed signal"))),(0,r.kt)("h3",{id:"the-evaluation-block"},"The ",(0,r.kt)("strong",{parentName:"h3"},"Evaluation")," block"),(0,r.kt)("p",null,"The evaluation generates the reference and processed signals and computes the HAAQI score."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"First, it takes the VDBO signals at the hearing aid microphones (these are the VDBO components provided by MUSDB18-HQ with the HRTF applied to them) and remixes the signals using the same gains as applied in the enhancement."),(0,r.kt)("li",{parentName:"ul"},'Then, it normalises the remix to the same LUFS level as the "mixture at the hearing aid mics".'),(0,r.kt)("li",{parentName:"ul"},"Next, it applies the NAL-R amplification."),(0,r.kt)("li",{parentName:"ul"},"This process results in the ",(0,r.kt)("inlineCode",{parentName:"li"},"Reference signal"),' for HAAQI, which simulates a "listener preferred mixture". The reference signal corresponds to an ideal rebalanced signal when we have access to the clean VDBO components.'),(0,r.kt)("li",{parentName:"ul"},"As HAAQI is an intrusive metric, the score is computed by comparing the ",(0,r.kt)("inlineCode",{parentName:"li"},"Processed signal")," (downmixed music) with the ",(0,r.kt)("inlineCode",{parentName:"li"},"Reference signal"),", focussing on changes to time-frequency envelope modulation, temporal fine structure and long-term spectrum.")),(0,r.kt)("admonition",{title:"Note",type:"info"},(0,r.kt)("ul",{parentName:"admonition"},(0,r.kt)("li",{parentName:"ul"},"In the Enhancement and Evaluation blocks, we apply a loudness normalisation (in ",(0,r.kt)("a",{parentName:"li",href:"https://www.izotope.com/en/learn/what-are-lufs.html"},"LUFS"),") after applying the gains.\nThis is to keep the loudness of the remix at the same levels as the mixture at the hearing aid mics."),(0,r.kt)("li",{parentName:"ul"},"As required by HAAQI, we resample both the reference and processed signal before computing the score."))),(0,r.kt)("h3",{id:"baseline-scores"},"Baseline Scores"),(0,r.kt)("p",null,"Two baseline systems are proposed by employing two out-of-the-box audio source separation systems in the ",(0,r.kt)("inlineCode",{parentName:"p"},"enhancement")," block."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Hybrid Demucs [",(0,r.kt)("a",{parentName:"li",href:"#refs"},"2"),"] distributed on TorchAudio"),(0,r.kt)("li",{parentName:"ul"},"Open-Unmix [",(0,r.kt)("a",{parentName:"li",href:"#refs"},"3"),"]  distributed through Pytorch hub.")),(0,r.kt)("p",null,"The average HAAQI scores are:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"System"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Left HAAQI"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Right HAAQI"),(0,r.kt)("th",{parentName:"tr",align:"center"},"Overall"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Demucs"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.6690"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.6665"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.6677")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"OpenUnmix"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.5986"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.5940"),(0,r.kt)("td",{parentName:"tr",align:"center"},"0.5963")))),(0,r.kt)("h2",{id:"references"},"References"),(0,r.kt)("a",{name:"refs"}),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"[1]")," Kates, J.M.  and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"[2]"),' D\xe9fossez, A. "Hybrid Spectrogram and Waveform Source Separation". Proceedings of the ISMIR 2021 Workshop on Music Source Separation. doi:10.48550/arXiv.2111.03600'),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"[3]"),' St\xf6ter, F. R., Liutkus, A., Ito, N., Nakashika, T., Ono, N., & Mitsufuji, Y. (2019). "Open-Unmix: A Reference Implementation for Music Source Separation". Journal of Open Source Software, 4(41), 1667. doi:10.21105/joss.01667'))}d.isMDXComponent=!0},8209:(e,t,a)=>{a(7294)}}]);
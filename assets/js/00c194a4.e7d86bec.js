/*! For license information please see 00c194a4.e7d86bec.js.LICENSE.txt */
(self.webpackChunkcadenza=self.webpackChunkcadenza||[]).push([[4404,8627],{52570:(e,n,i)=>{"use strict";i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"icassp_2024/intro","title":"The ICASSP 2024 Cadenza Grand Challenge","description":"The ICASSP 2024 Grand Challenge concluded in January 2024.","source":"@site/docs/icassp_2024/icassp_2024_intro.md","sourceDirName":"icassp_2024","slug":"/icassp_2024/intro","permalink":"/docs/icassp_2024/intro","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"intro","title":"The ICASSP 2024 Cadenza Grand Challenge","sidebar_label":"ICASSP 2024 Introduction","sidebar_position":1},"sidebar":"tutorialSidebar_icassp2024","next":{"title":"The Challenge Data","permalink":"/docs/category/the-challenge-data-1"}}');var a=i(74848),s=i(28453),r=i(24763);i(28770);const o={id:"intro",title:"The ICASSP 2024 Cadenza Grand Challenge",sidebar_label:"ICASSP 2024 Introduction",sidebar_position:1},l=void 0,d={},c=[{value:"Overview",id:"overview",level:2},{value:"What makes the demix different to previous demix challenges?",id:"what-makes-the-demix-different-to-previous-demix-challenges",level:3},{value:"Do I have to demix and then downmix to stereo?",id:"do-i-have-to-demix-and-then-downmix-to-stereo",level:3},{value:"Do I need to know about hearing loss and hearing aids?",id:"do-i-need-to-know-about-hearing-loss-and-hearing-aids",level:3},{value:"The task",id:"the-task",level:3},{value:"What is being provided",id:"what-is-being-provided",level:3},{value:"The systems&#39; output",id:"the-systems-output",level:3},{value:"Evaluation Stage",id:"evaluation-stage",level:3},{value:"Cite as",id:"cite-as",level:3},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.admonition,{title:"Challenge Closed",type:"tip",children:[(0,a.jsx)(n.p,{children:"The ICASSP 2024 Grand Challenge concluded in January 2024."}),(0,a.jsxs)(n.p,{children:["All signals and corresponding HAAQI scores submitted by participants are anonymously and openly accessible on Zenodo at ",(0,a.jsx)(n.a,{href:"https://zenodo.org/records/13285031",children:"https://zenodo.org/records/13285031"})]}),(0,a.jsx)(n.p,{children:"If you are interested in exploring this challenge further or using the submission signals, please cite the following paper:"}),(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:'G. Roa Dabike, M. A. Akeroyd, S. Bannister, J. P. Barker, T. J. Cox, B. Fazenda, J. Firth, S. Graetzer, A. Greasley, R. R. Vos and W. M. Whitmer, "The First Cadenza Challenges: Using Machine Learning Competitions to Improve Music for Listeners With a Hearing Loss," in IEEE Open Journal of Signal Processing, under review.'}),"\n"]})]}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"There is a global challenge of an ageing population. According to The World Health Organization (WHO), over 1.5 billion people worldwide have hearing loss. and is projected to increase to 1 in 10 people having disabling hearing loss by 2050. Hearing loss causes problems when listening to music. It can make picking out lyrics more difficult, with music becoming duller as high frequencies disappear. This reduces the enjoyment of music and can lead to disengagement from listening and music-making, reducing the health and well-being effects people otherwise get from music. We want to get more of the ICASSP community to consider diverse hearing and so allow those with a hearing loss to benefit from the latest signal processing advances."}),"\n",(0,a.jsx)(n.p,{children:"In this challenge, someone with a hearing loss is listening to music via their hearing aids. The challenge is to develop a signal processing system that allows a personalised rebalancing of the music to improve the listening experience, for example by amplifying the vocals relative to the sound of the band. One approach would be to a demix the music and then apply gains to the separated tracks to change the balance when the music is downmixed to stereo."}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsx)("iframe",{width:"750",height:"500",src:"https://www.youtube.com/embed/EniPRT-GgNk?si=FaTT9cXvLUMvtSoz",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",allowfullscreen:!0})}),"\n",(0,a.jsx)(n.h3,{id:"what-makes-the-demix-different-to-previous-demix-challenges",children:"What makes the demix different to previous demix challenges?"}),"\n",(0,a.jsxs)(n.p,{children:["The left and right signals you are working with are those picked up by a microphone at each ear when the person is listening to a pair of stereo loudspeakers. This means the signals at the ear that you have for demix is a combination of both the right and left stereo signals because of cross-talk (see ",(0,a.jsx)(n.a,{href:"#fig1",children:"Figure 1"}),"). This cross-talk will be strongest at low frequency when the wavelength is largest. This means that the spatial distribution of an instrument will be different in the microphone signals at the ear compared to the original left-right music signals. Stereo demix algorithms will need to be revised to allow for this frequency-dependent change. We will model the cross-talk using HRTFs (Head Related Transfer Functions), assuming the music comes from a pair of stereo loudspeakers in a dead room."]}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsxs)("figure",{id:"fig1",children:[(0,a.jsx)("img",{width:"500",src:(0,r.Ay)("/img/icassp_2024/cross-talk-hrtf.png")}),(0,a.jsx)("figcaption",{children:"Figure 1, The scenario."})]})}),"\n",(0,a.jsxs)(n.p,{children:["Although in the long term demixing on hearing aids would need to be causal and low latency, for ICASSP 2024 we are allowing causal ",(0,a.jsx)("u",{children:"and"})," non-causal approaches. If you produce a low latency solution that will be great and very novel. There are increasing number of DNN approaches for causal signal processing from other areas such as speech that could be adapted for this."]}),"\n",(0,a.jsx)(n.h3,{id:"do-i-have-to-demix-and-then-downmix-to-stereo",children:"Do I have to demix and then downmix to stereo?"}),"\n",(0,a.jsx)(n.p,{children:"Our baseline does demixing, but you don't have to. You could create an end-to-end system without an explicit demixing stage if you want."}),"\n",(0,a.jsx)(n.h3,{id:"do-i-need-to-know-about-hearing-loss-and-hearing-aids",children:"Do I need to know about hearing loss and hearing aids?"}),"\n",(0,a.jsxs)(n.p,{children:["Not really. We provide code for a standard amplification that is done by simple hearing aids. The challenge is mostly about rebalancing the music. We use a metric developed for hearing aids, but you could use another quality metric like Signal to Distortion Ratio (SDR) to develop your systems if you prefer. If you want to learn more about hearing loss and aids, however, there is lots of information in our ",(0,a.jsx)(n.a,{href:"../learning_resources/learning_intro",children:"learning resources"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"the-task",children:"The task"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"#fig2",children:"Figure 2"})," shows a simplified schematic of the baseline system:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["A scene generator (blue box) creates the scene characteristics:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Music signal at the hearing aids' microphones."}),"\n",(0,a.jsx)(n.li,{children:"The required gains for the output signal for the vocals, drums, bass and other (VDBO)."}),"\n",(0,a.jsx)(n.li,{children:"The reference rebalanced signal for scoring."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["The music enhancement stage (pink box) takes the music at the microphones as inputs and attempts to generate the rebalanced output:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"It estimates the VDBO components from the mixture."}),"\n",(0,a.jsx)(n.li,{children:"Then, it remixes the signal using the gains."}),"\n",(0,a.jsx)(n.li,{children:"Lastly, it applies listener-specific amplification following a standard hearing aid fitting."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Listener characteristics (green oval) are audiograms and are supplied to both the enhancement and evaluation."}),"\n",(0,a.jsxs)(n.li,{children:["The enhancement outputs are evaluated for audio quality via the ",(0,a.jsx)(n.a,{href:"../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech#haaqi-hearing-aid-audio-quality-index",children:"Hearing-Aid Audio Quality Index (HAAQI)"})," [",(0,a.jsx)(n.a,{href:"#refs",children:"1"}),"] (orange box). Note, HAAQI is an intrusive measure that requires a reference signal."]}),"\n"]}),"\n",(0,a.jsx)("div",{style:{textAlign:"center"},children:(0,a.jsxs)("figure",{id:"fig2",children:[(0,a.jsx)("img",{width:"900",src:(0,r.Ay)("/img/icassp_2024/baseline_simplify.png")}),(0,a.jsx)("figcaption",{children:"Figure 2, The simplified baseline schematics. For simplicity, not all signal paths are shown."})]})}),"\n",(0,a.jsx)(n.p,{children:"Your challenge is to improve what happens in the pink music enhancement box. The rest of the baseline is fixed and should not be changed."}),"\n",(0,a.jsx)(n.p,{children:"We are interested in systems that are either: (i) causal and low latency for live music, and (ii) non-causal for recorded music."}),"\n",(0,a.jsx)(n.h3,{id:"what-is-being-provided",children:"What is being provided"}),"\n",(0,a.jsx)(n.p,{children:"You will have access to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Full length songs from MUSDB18-HQ dataset."}),"\n",(0,a.jsx)(n.li,{children:"HRTFs to model the propagation of sound from the loudspeakers to the hearing aid microphones."}),"\n",(0,a.jsx)(n.li,{children:"Scripts to pre-process the music and construct the music signals at the hearing aid microphones with HRTF applied."}),"\n",(0,a.jsx)(n.li,{children:"Music data for augmentation, if needed."}),"\n",(0,a.jsxs)(n.li,{children:["Listeners characteristics (audiograms) - see ",(0,a.jsx)(n.a,{href:"data/data_listener",children:"Listener Data"})]}),"\n",(0,a.jsx)(n.li,{children:"Target gains for the VDBO stems used to mix the target stereo"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Please refer to ",(0,a.jsx)(n.a,{href:"data/data_overview",children:"data page"})," and the ",(0,a.jsx)(n.a,{href:"https://github.com/claritychallenge/clarity/blob/main/recipes/cad_icassp_2024/baseline/README.md",children:"baseline readme"})," in GitHub for details.\nTo download the datasets, please visit ",(0,a.jsx)(n.a,{href:"take_part/download",children:"download data and software"})]}),"\n",(0,a.jsx)(n.h3,{id:"the-systems-output",children:"The systems' output"}),"\n",(0,a.jsx)(n.p,{children:"Stereo downmixed signals"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Sample rate: 32,000 Hz"}),"\n",(0,a.jsx)(n.li,{children:"Precision: 16-bit integer"}),"\n",(0,a.jsx)(n.li,{children:"Compression: FLAC"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["For more details about the format of the submission, please refer to the ",(0,a.jsx)(n.a,{href:"take_part/submission",children:"submission"})," webpage."]}),"\n",(0,a.jsx)(n.p,{children:":::caution Note\nThe responsibility for the final remixed signal level is yours.\nIt\u2019s worth bearing in mind there may be clipping in the evaluation block in some tasks\nif the processed signals are too large.\n:::"}),"\n",(0,a.jsx)(n.h3,{id:"evaluation-stage",children:"Evaluation Stage"}),"\n",(0,a.jsx)(n.admonition,{title:"Warning",type:"danger",children:(0,a.jsxs)(n.p,{children:["You are not allowed to change the ",(0,a.jsx)(n.strong,{children:"evaluation"})," script provided in the baseline.\nYour output signals with be scored using this script."]})}),"\n",(0,a.jsxs)(n.p,{children:["The evaluation stage computes ",(0,a.jsx)(n.a,{href:"../learning_resources/Hearing_aid_processing/edu_HAP_HA_processed_speech",children:"HAAQI"}),"\nscores for the remixed stereo - see ",(0,a.jsx)(n.a,{href:"https://github.com/claritychallenge/clarity/blob/main/clarity/evaluator/haaqi/haaqi.py",children:"Python HAAQI implementation"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The output of the evaluation stage is a CSV file with all the HAAQI scores."}),"\n",(0,a.jsx)(n.h3,{id:"cite-as",children:"Cite as"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"@misc{roa2023cadenza,\n    title={The Cadenza ICASSP 2024 Grand Challenge},\n    author={Gerardo Roa Dabike and Michael A. Akeroyd and Scott Bannister and Jon Barker and \n            Trevor J. Cox and Bruno Fazenda and Jennifer Firth and Simone Graetzer and \n            Alinka Greasley and Rebecca Vos and William Whitmer},\n    year={2023},\n    eprint={2310.03480},\n    archivePrefix={arXiv},\n    primaryClass={eess.AS},\n    note={to be submitted in ICASSP 2024}\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsx)("a",{name:"refs"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"[1]"})," Kates, J.M.  and Arehart, K.H., 2016. The Hearing-Aid Audio Quality Index (HAAQI), in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 2, pp. 354-365, doi: 10.1109/TASLP.2015.2507858."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},28770:(e,n,i)=>{"use strict";i.d(n,{CU:()=>r});var t=i(96540),a="https://platform.twitter.com/widgets.js",s="createTimeline",r=function(e){var n=t.useRef(null),r=t.useState(!0),o=r[0],l=r[1];return t.useEffect((function(){var t=!0;return i(62491)(a,"twitter-embed",(function(){if(window.twttr){if(t){if(!window.twttr.widgets[s])return void console.error("Method "+s+" is not present anymore in twttr.widget api");var i=function(){var i,t,a=Object.assign({},e.options);return null!=e&&e.autoHeight&&(a.height=null===(i=n.current)||void 0===i||null===(t=i.parentNode)||void 0===t?void 0:t.offsetHeight),a=Object.assign({},a,{theme:null==e?void 0:e.theme,linkColor:null==e?void 0:e.linkColor,borderColor:null==e?void 0:e.borderColor,lang:null==e?void 0:e.lang,tweetLimit:null==e?void 0:e.tweetLimit,ariaPolite:null==e?void 0:e.ariaPolite})}();i=function(n){return n.chrome="",e.noHeader&&(n.chrome=n.chrome+" noheader"),e.noFooter&&(n.chrome=n.chrome+" nofooter"),e.noBorders&&(n.chrome=n.chrome+" noborders"),e.noScrollbar&&(n.chrome=n.chrome+" noscrollbar"),e.transparent&&(n.chrome=n.chrome+" transparent"),n}(i),window.twttr.widgets[s]({sourceType:e.sourceType,screenName:e.screenName,userId:e.userId,ownerScreenName:e.ownerScreenName,slug:e.slug,id:e.id||e.widgetId,url:e.url},null==n?void 0:n.current,i).then((function(n){l(!1),e.onLoad&&e.onLoad(n)}))}}else console.error("Failure to load window.twttr, aborting load")})),function(){t=!1}}),[]),t.createElement(t.Fragment,null,o&&t.createElement(t.Fragment,null,e.placeholder),t.createElement("div",{ref:n}))}},62491:(e,n,i)=>{var t,a,s;s=function(){var e,n,i=document,t=i.getElementsByTagName("head")[0],a="push",s="readyState",r="onreadystatechange",o={},l={},d={},c={};function h(e,n){for(var i=0,t=e.length;i<t;++i)if(!n(e[i]))return!1;return 1}function u(e,n){h(e,(function(e){return n(e),1}))}function g(n,i,t){n=n[a]?n:[n];var s=i&&i.call,r=s?i:t,m=s?n.join(""):i,f=n.length;function x(e){return e.call?e():o[e]}function v(){if(! --f)for(var e in o[m]=1,r&&r(),d)h(e.split("|"),x)&&!u(d[e],x)&&(d[e]=[])}return setTimeout((function(){u(n,(function n(i,t){return null===i?v():(t||/^https?:\/\//.test(i)||!e||(i=-1===i.indexOf(".js")?e+i+".js":e+i),c[i]?(m&&(l[m]=1),2==c[i]?v():setTimeout((function(){n(i,!0)}),0)):(c[i]=1,m&&(l[m]=1),void p(i,v)))}))}),0),g}function p(e,a){var o,l=i.createElement("script");l.onload=l.onerror=l[r]=function(){l[s]&&!/^c|loade/.test(l[s])||o||(l.onload=l[r]=null,o=1,c[e]=2,a())},l.async=1,l.src=n?e+(-1===e.indexOf("?")?"?":"&")+n:e,t.insertBefore(l,t.lastChild)}return g.get=p,g.order=function(e,n,i){!function t(a){a=e.shift(),e.length?g(a,t):g(a,n,i)}()},g.path=function(n){e=n},g.urlArgs=function(e){n=e},g.ready=function(e,n,i){e=e[a]?e:[e];var t,s=[];return!u(e,(function(e){o[e]||s[a](e)}))&&h(e,(function(e){return o[e]}))?n():(t=e.join("|"),d[t]=d[t]||[],d[t][a](n),i&&i(s)),g},g.done=function(e){g([null],e)},g},e.exports?e.exports=s():void 0===(a="function"==typeof(t=s)?t.call(n,i,n,e):t)||(e.exports=a)},28453:(e,n,i)=>{"use strict";i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);